{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('Scattered path loss of 48 sensors for 2443 positions.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103.267633</td>\n",
       "      <td>131.597806</td>\n",
       "      <td>81.831203</td>\n",
       "      <td>108.480113</td>\n",
       "      <td>126.401200</td>\n",
       "      <td>153.464898</td>\n",
       "      <td>112.979443</td>\n",
       "      <td>138.850280</td>\n",
       "      <td>95.879983</td>\n",
       "      <td>120.538335</td>\n",
       "      <td>...</td>\n",
       "      <td>109.733925</td>\n",
       "      <td>122.659778</td>\n",
       "      <td>95.395578</td>\n",
       "      <td>130.490611</td>\n",
       "      <td>73.119448</td>\n",
       "      <td>121.722946</td>\n",
       "      <td>104.559876</td>\n",
       "      <td>137.225660</td>\n",
       "      <td>91.041703</td>\n",
       "      <td>127.652224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>91.665371</td>\n",
       "      <td>134.314718</td>\n",
       "      <td>83.926980</td>\n",
       "      <td>126.167508</td>\n",
       "      <td>127.048010</td>\n",
       "      <td>147.329908</td>\n",
       "      <td>113.967891</td>\n",
       "      <td>144.590411</td>\n",
       "      <td>97.747726</td>\n",
       "      <td>127.941590</td>\n",
       "      <td>...</td>\n",
       "      <td>113.915353</td>\n",
       "      <td>123.190051</td>\n",
       "      <td>95.157075</td>\n",
       "      <td>127.762477</td>\n",
       "      <td>66.005689</td>\n",
       "      <td>115.881671</td>\n",
       "      <td>107.128477</td>\n",
       "      <td>140.352729</td>\n",
       "      <td>86.947794</td>\n",
       "      <td>119.210396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>108.052816</td>\n",
       "      <td>132.300121</td>\n",
       "      <td>76.009308</td>\n",
       "      <td>116.964033</td>\n",
       "      <td>132.173743</td>\n",
       "      <td>143.051777</td>\n",
       "      <td>115.162722</td>\n",
       "      <td>137.676604</td>\n",
       "      <td>102.738553</td>\n",
       "      <td>122.522861</td>\n",
       "      <td>...</td>\n",
       "      <td>112.052375</td>\n",
       "      <td>117.562515</td>\n",
       "      <td>99.313642</td>\n",
       "      <td>128.192048</td>\n",
       "      <td>75.164417</td>\n",
       "      <td>116.706805</td>\n",
       "      <td>112.276377</td>\n",
       "      <td>129.772121</td>\n",
       "      <td>88.276306</td>\n",
       "      <td>117.740004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101.053301</td>\n",
       "      <td>127.103974</td>\n",
       "      <td>84.735175</td>\n",
       "      <td>115.929306</td>\n",
       "      <td>118.353146</td>\n",
       "      <td>149.874228</td>\n",
       "      <td>106.017232</td>\n",
       "      <td>134.139507</td>\n",
       "      <td>98.226181</td>\n",
       "      <td>122.213773</td>\n",
       "      <td>...</td>\n",
       "      <td>108.861984</td>\n",
       "      <td>129.042510</td>\n",
       "      <td>98.180222</td>\n",
       "      <td>129.344039</td>\n",
       "      <td>66.522773</td>\n",
       "      <td>117.580686</td>\n",
       "      <td>107.813133</td>\n",
       "      <td>134.786828</td>\n",
       "      <td>91.081817</td>\n",
       "      <td>118.908161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>108.955215</td>\n",
       "      <td>128.747209</td>\n",
       "      <td>82.213517</td>\n",
       "      <td>116.666273</td>\n",
       "      <td>125.771908</td>\n",
       "      <td>153.445316</td>\n",
       "      <td>111.663266</td>\n",
       "      <td>140.358840</td>\n",
       "      <td>96.138273</td>\n",
       "      <td>128.801601</td>\n",
       "      <td>...</td>\n",
       "      <td>108.706859</td>\n",
       "      <td>119.970422</td>\n",
       "      <td>100.514277</td>\n",
       "      <td>126.900903</td>\n",
       "      <td>71.984768</td>\n",
       "      <td>110.931963</td>\n",
       "      <td>113.277748</td>\n",
       "      <td>139.563546</td>\n",
       "      <td>92.386548</td>\n",
       "      <td>129.117372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.179344</td>\n",
       "      <td>114.641972</td>\n",
       "      <td>105.710144</td>\n",
       "      <td>88.287079</td>\n",
       "      <td>149.165563</td>\n",
       "      <td>137.537974</td>\n",
       "      <td>133.750673</td>\n",
       "      <td>112.015302</td>\n",
       "      <td>120.576864</td>\n",
       "      <td>112.474493</td>\n",
       "      <td>...</td>\n",
       "      <td>119.309378</td>\n",
       "      <td>112.129281</td>\n",
       "      <td>133.167988</td>\n",
       "      <td>114.383990</td>\n",
       "      <td>102.902100</td>\n",
       "      <td>74.953746</td>\n",
       "      <td>136.775728</td>\n",
       "      <td>135.511864</td>\n",
       "      <td>113.860294</td>\n",
       "      <td>97.289917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>126.365931</td>\n",
       "      <td>110.972446</td>\n",
       "      <td>103.198207</td>\n",
       "      <td>73.414657</td>\n",
       "      <td>146.420909</td>\n",
       "      <td>140.802299</td>\n",
       "      <td>136.087553</td>\n",
       "      <td>119.668856</td>\n",
       "      <td>117.611405</td>\n",
       "      <td>97.729138</td>\n",
       "      <td>...</td>\n",
       "      <td>119.776043</td>\n",
       "      <td>109.341785</td>\n",
       "      <td>134.167797</td>\n",
       "      <td>108.758001</td>\n",
       "      <td>97.232046</td>\n",
       "      <td>76.553890</td>\n",
       "      <td>131.527636</td>\n",
       "      <td>120.620501</td>\n",
       "      <td>114.793561</td>\n",
       "      <td>86.316080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>124.285145</td>\n",
       "      <td>103.360797</td>\n",
       "      <td>109.290753</td>\n",
       "      <td>79.003261</td>\n",
       "      <td>151.453751</td>\n",
       "      <td>138.577223</td>\n",
       "      <td>133.219135</td>\n",
       "      <td>111.313822</td>\n",
       "      <td>121.482318</td>\n",
       "      <td>115.272484</td>\n",
       "      <td>...</td>\n",
       "      <td>124.257271</td>\n",
       "      <td>108.008908</td>\n",
       "      <td>124.793498</td>\n",
       "      <td>110.650887</td>\n",
       "      <td>110.008857</td>\n",
       "      <td>71.785029</td>\n",
       "      <td>141.782071</td>\n",
       "      <td>125.802032</td>\n",
       "      <td>113.221141</td>\n",
       "      <td>91.163433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>128.455467</td>\n",
       "      <td>113.305564</td>\n",
       "      <td>120.534195</td>\n",
       "      <td>70.567774</td>\n",
       "      <td>150.202237</td>\n",
       "      <td>147.104822</td>\n",
       "      <td>128.145483</td>\n",
       "      <td>115.972568</td>\n",
       "      <td>118.499662</td>\n",
       "      <td>104.549275</td>\n",
       "      <td>...</td>\n",
       "      <td>125.544587</td>\n",
       "      <td>114.946916</td>\n",
       "      <td>128.766695</td>\n",
       "      <td>114.402412</td>\n",
       "      <td>100.797834</td>\n",
       "      <td>77.919668</td>\n",
       "      <td>139.414337</td>\n",
       "      <td>117.483786</td>\n",
       "      <td>123.049769</td>\n",
       "      <td>91.451389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>122.108331</td>\n",
       "      <td>117.023437</td>\n",
       "      <td>102.527909</td>\n",
       "      <td>87.494457</td>\n",
       "      <td>151.737028</td>\n",
       "      <td>140.752720</td>\n",
       "      <td>139.551852</td>\n",
       "      <td>113.287240</td>\n",
       "      <td>119.649711</td>\n",
       "      <td>114.469436</td>\n",
       "      <td>...</td>\n",
       "      <td>120.001152</td>\n",
       "      <td>109.766753</td>\n",
       "      <td>125.244051</td>\n",
       "      <td>114.664490</td>\n",
       "      <td>110.627432</td>\n",
       "      <td>76.722995</td>\n",
       "      <td>138.251458</td>\n",
       "      <td>128.808403</td>\n",
       "      <td>123.226069</td>\n",
       "      <td>96.901824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5   \\\n",
       "0     103.267633  131.597806   81.831203  108.480113  126.401200  153.464898   \n",
       "1      91.665371  134.314718   83.926980  126.167508  127.048010  147.329908   \n",
       "2     108.052816  132.300121   76.009308  116.964033  132.173743  143.051777   \n",
       "3     101.053301  127.103974   84.735175  115.929306  118.353146  149.874228   \n",
       "4     108.955215  128.747209   82.213517  116.666273  125.771908  153.445316   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.179344  114.641972  105.710144   88.287079  149.165563  137.537974   \n",
       "2439  126.365931  110.972446  103.198207   73.414657  146.420909  140.802299   \n",
       "2440  124.285145  103.360797  109.290753   79.003261  151.453751  138.577223   \n",
       "2441  128.455467  113.305564  120.534195   70.567774  150.202237  147.104822   \n",
       "2442  122.108331  117.023437  102.527909   87.494457  151.737028  140.752720   \n",
       "\n",
       "              6           7           8           9   ...          38  \\\n",
       "0     112.979443  138.850280   95.879983  120.538335  ...  109.733925   \n",
       "1     113.967891  144.590411   97.747726  127.941590  ...  113.915353   \n",
       "2     115.162722  137.676604  102.738553  122.522861  ...  112.052375   \n",
       "3     106.017232  134.139507   98.226181  122.213773  ...  108.861984   \n",
       "4     111.663266  140.358840   96.138273  128.801601  ...  108.706859   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "2438  133.750673  112.015302  120.576864  112.474493  ...  119.309378   \n",
       "2439  136.087553  119.668856  117.611405   97.729138  ...  119.776043   \n",
       "2440  133.219135  111.313822  121.482318  115.272484  ...  124.257271   \n",
       "2441  128.145483  115.972568  118.499662  104.549275  ...  125.544587   \n",
       "2442  139.551852  113.287240  119.649711  114.469436  ...  120.001152   \n",
       "\n",
       "              39          40          41          42          43          44  \\\n",
       "0     122.659778   95.395578  130.490611   73.119448  121.722946  104.559876   \n",
       "1     123.190051   95.157075  127.762477   66.005689  115.881671  107.128477   \n",
       "2     117.562515   99.313642  128.192048   75.164417  116.706805  112.276377   \n",
       "3     129.042510   98.180222  129.344039   66.522773  117.580686  107.813133   \n",
       "4     119.970422  100.514277  126.900903   71.984768  110.931963  113.277748   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  112.129281  133.167988  114.383990  102.902100   74.953746  136.775728   \n",
       "2439  109.341785  134.167797  108.758001   97.232046   76.553890  131.527636   \n",
       "2440  108.008908  124.793498  110.650887  110.008857   71.785029  141.782071   \n",
       "2441  114.946916  128.766695  114.402412  100.797834   77.919668  139.414337   \n",
       "2442  109.766753  125.244051  114.664490  110.627432   76.722995  138.251458   \n",
       "\n",
       "              45          46          47  \n",
       "0     137.225660   91.041703  127.652224  \n",
       "1     140.352729   86.947794  119.210396  \n",
       "2     129.772121   88.276306  117.740004  \n",
       "3     134.786828   91.081817  118.908161  \n",
       "4     139.563546   92.386548  129.117372  \n",
       "...          ...         ...         ...  \n",
       "2438  135.511864  113.860294   97.289917  \n",
       "2439  120.620501  114.793561   86.316080  \n",
       "2440  125.802032  113.221141   91.163433  \n",
       "2441  117.483786  123.049769   91.451389  \n",
       "2442  128.808403  123.226069   96.901824  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real x-y-z positions of 2443 locations.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103.267633</td>\n",
       "      <td>131.597806</td>\n",
       "      <td>81.831203</td>\n",
       "      <td>108.480113</td>\n",
       "      <td>126.401200</td>\n",
       "      <td>153.464898</td>\n",
       "      <td>112.979443</td>\n",
       "      <td>138.850280</td>\n",
       "      <td>95.879983</td>\n",
       "      <td>120.538335</td>\n",
       "      <td>...</td>\n",
       "      <td>109.733925</td>\n",
       "      <td>122.659778</td>\n",
       "      <td>95.395578</td>\n",
       "      <td>130.490611</td>\n",
       "      <td>73.119448</td>\n",
       "      <td>121.722946</td>\n",
       "      <td>104.559876</td>\n",
       "      <td>137.225660</td>\n",
       "      <td>91.041703</td>\n",
       "      <td>127.652224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>91.665371</td>\n",
       "      <td>134.314718</td>\n",
       "      <td>83.926980</td>\n",
       "      <td>126.167508</td>\n",
       "      <td>127.048010</td>\n",
       "      <td>147.329908</td>\n",
       "      <td>113.967891</td>\n",
       "      <td>144.590411</td>\n",
       "      <td>97.747726</td>\n",
       "      <td>127.941590</td>\n",
       "      <td>...</td>\n",
       "      <td>113.915353</td>\n",
       "      <td>123.190051</td>\n",
       "      <td>95.157075</td>\n",
       "      <td>127.762477</td>\n",
       "      <td>66.005689</td>\n",
       "      <td>115.881671</td>\n",
       "      <td>107.128477</td>\n",
       "      <td>140.352729</td>\n",
       "      <td>86.947794</td>\n",
       "      <td>119.210396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>108.052816</td>\n",
       "      <td>132.300121</td>\n",
       "      <td>76.009308</td>\n",
       "      <td>116.964033</td>\n",
       "      <td>132.173743</td>\n",
       "      <td>143.051777</td>\n",
       "      <td>115.162722</td>\n",
       "      <td>137.676604</td>\n",
       "      <td>102.738553</td>\n",
       "      <td>122.522861</td>\n",
       "      <td>...</td>\n",
       "      <td>112.052375</td>\n",
       "      <td>117.562515</td>\n",
       "      <td>99.313642</td>\n",
       "      <td>128.192048</td>\n",
       "      <td>75.164417</td>\n",
       "      <td>116.706805</td>\n",
       "      <td>112.276377</td>\n",
       "      <td>129.772121</td>\n",
       "      <td>88.276306</td>\n",
       "      <td>117.740004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101.053301</td>\n",
       "      <td>127.103974</td>\n",
       "      <td>84.735175</td>\n",
       "      <td>115.929306</td>\n",
       "      <td>118.353146</td>\n",
       "      <td>149.874228</td>\n",
       "      <td>106.017232</td>\n",
       "      <td>134.139507</td>\n",
       "      <td>98.226181</td>\n",
       "      <td>122.213773</td>\n",
       "      <td>...</td>\n",
       "      <td>108.861984</td>\n",
       "      <td>129.042510</td>\n",
       "      <td>98.180222</td>\n",
       "      <td>129.344039</td>\n",
       "      <td>66.522773</td>\n",
       "      <td>117.580686</td>\n",
       "      <td>107.813133</td>\n",
       "      <td>134.786828</td>\n",
       "      <td>91.081817</td>\n",
       "      <td>118.908161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>108.955215</td>\n",
       "      <td>128.747209</td>\n",
       "      <td>82.213517</td>\n",
       "      <td>116.666273</td>\n",
       "      <td>125.771908</td>\n",
       "      <td>153.445316</td>\n",
       "      <td>111.663266</td>\n",
       "      <td>140.358840</td>\n",
       "      <td>96.138273</td>\n",
       "      <td>128.801601</td>\n",
       "      <td>...</td>\n",
       "      <td>108.706859</td>\n",
       "      <td>119.970422</td>\n",
       "      <td>100.514277</td>\n",
       "      <td>126.900903</td>\n",
       "      <td>71.984768</td>\n",
       "      <td>110.931963</td>\n",
       "      <td>113.277748</td>\n",
       "      <td>139.563546</td>\n",
       "      <td>92.386548</td>\n",
       "      <td>129.117372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.179344</td>\n",
       "      <td>114.641972</td>\n",
       "      <td>105.710144</td>\n",
       "      <td>88.287079</td>\n",
       "      <td>149.165563</td>\n",
       "      <td>137.537974</td>\n",
       "      <td>133.750673</td>\n",
       "      <td>112.015302</td>\n",
       "      <td>120.576864</td>\n",
       "      <td>112.474493</td>\n",
       "      <td>...</td>\n",
       "      <td>119.309378</td>\n",
       "      <td>112.129281</td>\n",
       "      <td>133.167988</td>\n",
       "      <td>114.383990</td>\n",
       "      <td>102.902100</td>\n",
       "      <td>74.953746</td>\n",
       "      <td>136.775728</td>\n",
       "      <td>135.511864</td>\n",
       "      <td>113.860294</td>\n",
       "      <td>97.289917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>126.365931</td>\n",
       "      <td>110.972446</td>\n",
       "      <td>103.198207</td>\n",
       "      <td>73.414657</td>\n",
       "      <td>146.420909</td>\n",
       "      <td>140.802299</td>\n",
       "      <td>136.087553</td>\n",
       "      <td>119.668856</td>\n",
       "      <td>117.611405</td>\n",
       "      <td>97.729138</td>\n",
       "      <td>...</td>\n",
       "      <td>119.776043</td>\n",
       "      <td>109.341785</td>\n",
       "      <td>134.167797</td>\n",
       "      <td>108.758001</td>\n",
       "      <td>97.232046</td>\n",
       "      <td>76.553890</td>\n",
       "      <td>131.527636</td>\n",
       "      <td>120.620501</td>\n",
       "      <td>114.793561</td>\n",
       "      <td>86.316080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>124.285145</td>\n",
       "      <td>103.360797</td>\n",
       "      <td>109.290753</td>\n",
       "      <td>79.003261</td>\n",
       "      <td>151.453751</td>\n",
       "      <td>138.577223</td>\n",
       "      <td>133.219135</td>\n",
       "      <td>111.313822</td>\n",
       "      <td>121.482318</td>\n",
       "      <td>115.272484</td>\n",
       "      <td>...</td>\n",
       "      <td>124.257271</td>\n",
       "      <td>108.008908</td>\n",
       "      <td>124.793498</td>\n",
       "      <td>110.650887</td>\n",
       "      <td>110.008857</td>\n",
       "      <td>71.785029</td>\n",
       "      <td>141.782071</td>\n",
       "      <td>125.802032</td>\n",
       "      <td>113.221141</td>\n",
       "      <td>91.163433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>128.455467</td>\n",
       "      <td>113.305564</td>\n",
       "      <td>120.534195</td>\n",
       "      <td>70.567774</td>\n",
       "      <td>150.202237</td>\n",
       "      <td>147.104822</td>\n",
       "      <td>128.145483</td>\n",
       "      <td>115.972568</td>\n",
       "      <td>118.499662</td>\n",
       "      <td>104.549275</td>\n",
       "      <td>...</td>\n",
       "      <td>125.544587</td>\n",
       "      <td>114.946916</td>\n",
       "      <td>128.766695</td>\n",
       "      <td>114.402412</td>\n",
       "      <td>100.797834</td>\n",
       "      <td>77.919668</td>\n",
       "      <td>139.414337</td>\n",
       "      <td>117.483786</td>\n",
       "      <td>123.049769</td>\n",
       "      <td>91.451389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>122.108331</td>\n",
       "      <td>117.023437</td>\n",
       "      <td>102.527909</td>\n",
       "      <td>87.494457</td>\n",
       "      <td>151.737028</td>\n",
       "      <td>140.752720</td>\n",
       "      <td>139.551852</td>\n",
       "      <td>113.287240</td>\n",
       "      <td>119.649711</td>\n",
       "      <td>114.469436</td>\n",
       "      <td>...</td>\n",
       "      <td>120.001152</td>\n",
       "      <td>109.766753</td>\n",
       "      <td>125.244051</td>\n",
       "      <td>114.664490</td>\n",
       "      <td>110.627432</td>\n",
       "      <td>76.722995</td>\n",
       "      <td>138.251458</td>\n",
       "      <td>128.808403</td>\n",
       "      <td>123.226069</td>\n",
       "      <td>96.901824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     103.267633  131.597806   81.831203  108.480113  126.401200  153.464898   \n",
       "1      91.665371  134.314718   83.926980  126.167508  127.048010  147.329908   \n",
       "2     108.052816  132.300121   76.009308  116.964033  132.173743  143.051777   \n",
       "3     101.053301  127.103974   84.735175  115.929306  118.353146  149.874228   \n",
       "4     108.955215  128.747209   82.213517  116.666273  125.771908  153.445316   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.179344  114.641972  105.710144   88.287079  149.165563  137.537974   \n",
       "2439  126.365931  110.972446  103.198207   73.414657  146.420909  140.802299   \n",
       "2440  124.285145  103.360797  109.290753   79.003261  151.453751  138.577223   \n",
       "2441  128.455467  113.305564  120.534195   70.567774  150.202237  147.104822   \n",
       "2442  122.108331  117.023437  102.527909   87.494457  151.737028  140.752720   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10  ...    sensor39  \\\n",
       "0     112.979443  138.850280   95.879983  120.538335  ...  109.733925   \n",
       "1     113.967891  144.590411   97.747726  127.941590  ...  113.915353   \n",
       "2     115.162722  137.676604  102.738553  122.522861  ...  112.052375   \n",
       "3     106.017232  134.139507   98.226181  122.213773  ...  108.861984   \n",
       "4     111.663266  140.358840   96.138273  128.801601  ...  108.706859   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "2438  133.750673  112.015302  120.576864  112.474493  ...  119.309378   \n",
       "2439  136.087553  119.668856  117.611405   97.729138  ...  119.776043   \n",
       "2440  133.219135  111.313822  121.482318  115.272484  ...  124.257271   \n",
       "2441  128.145483  115.972568  118.499662  104.549275  ...  125.544587   \n",
       "2442  139.551852  113.287240  119.649711  114.469436  ...  120.001152   \n",
       "\n",
       "        sensor40    sensor41    sensor42    sensor43    sensor44    sensor45  \\\n",
       "0     122.659778   95.395578  130.490611   73.119448  121.722946  104.559876   \n",
       "1     123.190051   95.157075  127.762477   66.005689  115.881671  107.128477   \n",
       "2     117.562515   99.313642  128.192048   75.164417  116.706805  112.276377   \n",
       "3     129.042510   98.180222  129.344039   66.522773  117.580686  107.813133   \n",
       "4     119.970422  100.514277  126.900903   71.984768  110.931963  113.277748   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  112.129281  133.167988  114.383990  102.902100   74.953746  136.775728   \n",
       "2439  109.341785  134.167797  108.758001   97.232046   76.553890  131.527636   \n",
       "2440  108.008908  124.793498  110.650887  110.008857   71.785029  141.782071   \n",
       "2441  114.946916  128.766695  114.402412  100.797834   77.919668  139.414337   \n",
       "2442  109.766753  125.244051  114.664490  110.627432   76.722995  138.251458   \n",
       "\n",
       "        sensor46    sensor47    sensor48  \n",
       "0     137.225660   91.041703  127.652224  \n",
       "1     140.352729   86.947794  119.210396  \n",
       "2     129.772121   88.276306  117.740004  \n",
       "3     134.786828   91.081817  118.908161  \n",
       "4     139.563546   92.386548  129.117372  \n",
       "...          ...         ...         ...  \n",
       "2438  135.511864  113.860294   97.289917  \n",
       "2439  120.620501  114.793561   86.316080  \n",
       "2440  125.802032  113.221141   91.163433  \n",
       "2441  117.483786  123.049769   91.451389  \n",
       "2442  128.808403  123.226069   96.901824  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e000bcc",
   "metadata": {},
   "source": [
    "### Concatenating Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "abe21859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103.267633</td>\n",
       "      <td>131.597806</td>\n",
       "      <td>81.831203</td>\n",
       "      <td>108.480113</td>\n",
       "      <td>126.401200</td>\n",
       "      <td>153.464898</td>\n",
       "      <td>112.979443</td>\n",
       "      <td>138.850280</td>\n",
       "      <td>95.879983</td>\n",
       "      <td>120.538335</td>\n",
       "      <td>...</td>\n",
       "      <td>130.490611</td>\n",
       "      <td>73.119448</td>\n",
       "      <td>121.722946</td>\n",
       "      <td>104.559876</td>\n",
       "      <td>137.225660</td>\n",
       "      <td>91.041703</td>\n",
       "      <td>127.652224</td>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>91.665371</td>\n",
       "      <td>134.314718</td>\n",
       "      <td>83.926980</td>\n",
       "      <td>126.167508</td>\n",
       "      <td>127.048010</td>\n",
       "      <td>147.329908</td>\n",
       "      <td>113.967891</td>\n",
       "      <td>144.590411</td>\n",
       "      <td>97.747726</td>\n",
       "      <td>127.941590</td>\n",
       "      <td>...</td>\n",
       "      <td>127.762477</td>\n",
       "      <td>66.005689</td>\n",
       "      <td>115.881671</td>\n",
       "      <td>107.128477</td>\n",
       "      <td>140.352729</td>\n",
       "      <td>86.947794</td>\n",
       "      <td>119.210396</td>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>108.052816</td>\n",
       "      <td>132.300121</td>\n",
       "      <td>76.009308</td>\n",
       "      <td>116.964033</td>\n",
       "      <td>132.173743</td>\n",
       "      <td>143.051777</td>\n",
       "      <td>115.162722</td>\n",
       "      <td>137.676604</td>\n",
       "      <td>102.738553</td>\n",
       "      <td>122.522861</td>\n",
       "      <td>...</td>\n",
       "      <td>128.192048</td>\n",
       "      <td>75.164417</td>\n",
       "      <td>116.706805</td>\n",
       "      <td>112.276377</td>\n",
       "      <td>129.772121</td>\n",
       "      <td>88.276306</td>\n",
       "      <td>117.740004</td>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101.053301</td>\n",
       "      <td>127.103974</td>\n",
       "      <td>84.735175</td>\n",
       "      <td>115.929306</td>\n",
       "      <td>118.353146</td>\n",
       "      <td>149.874228</td>\n",
       "      <td>106.017232</td>\n",
       "      <td>134.139507</td>\n",
       "      <td>98.226181</td>\n",
       "      <td>122.213773</td>\n",
       "      <td>...</td>\n",
       "      <td>129.344039</td>\n",
       "      <td>66.522773</td>\n",
       "      <td>117.580686</td>\n",
       "      <td>107.813133</td>\n",
       "      <td>134.786828</td>\n",
       "      <td>91.081817</td>\n",
       "      <td>118.908161</td>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>108.955215</td>\n",
       "      <td>128.747209</td>\n",
       "      <td>82.213517</td>\n",
       "      <td>116.666273</td>\n",
       "      <td>125.771908</td>\n",
       "      <td>153.445316</td>\n",
       "      <td>111.663266</td>\n",
       "      <td>140.358840</td>\n",
       "      <td>96.138273</td>\n",
       "      <td>128.801601</td>\n",
       "      <td>...</td>\n",
       "      <td>126.900903</td>\n",
       "      <td>71.984768</td>\n",
       "      <td>110.931963</td>\n",
       "      <td>113.277748</td>\n",
       "      <td>139.563546</td>\n",
       "      <td>92.386548</td>\n",
       "      <td>129.117372</td>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.179344</td>\n",
       "      <td>114.641972</td>\n",
       "      <td>105.710144</td>\n",
       "      <td>88.287079</td>\n",
       "      <td>149.165563</td>\n",
       "      <td>137.537974</td>\n",
       "      <td>133.750673</td>\n",
       "      <td>112.015302</td>\n",
       "      <td>120.576864</td>\n",
       "      <td>112.474493</td>\n",
       "      <td>...</td>\n",
       "      <td>114.383990</td>\n",
       "      <td>102.902100</td>\n",
       "      <td>74.953746</td>\n",
       "      <td>136.775728</td>\n",
       "      <td>135.511864</td>\n",
       "      <td>113.860294</td>\n",
       "      <td>97.289917</td>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>126.365931</td>\n",
       "      <td>110.972446</td>\n",
       "      <td>103.198207</td>\n",
       "      <td>73.414657</td>\n",
       "      <td>146.420909</td>\n",
       "      <td>140.802299</td>\n",
       "      <td>136.087553</td>\n",
       "      <td>119.668856</td>\n",
       "      <td>117.611405</td>\n",
       "      <td>97.729138</td>\n",
       "      <td>...</td>\n",
       "      <td>108.758001</td>\n",
       "      <td>97.232046</td>\n",
       "      <td>76.553890</td>\n",
       "      <td>131.527636</td>\n",
       "      <td>120.620501</td>\n",
       "      <td>114.793561</td>\n",
       "      <td>86.316080</td>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>124.285145</td>\n",
       "      <td>103.360797</td>\n",
       "      <td>109.290753</td>\n",
       "      <td>79.003261</td>\n",
       "      <td>151.453751</td>\n",
       "      <td>138.577223</td>\n",
       "      <td>133.219135</td>\n",
       "      <td>111.313822</td>\n",
       "      <td>121.482318</td>\n",
       "      <td>115.272484</td>\n",
       "      <td>...</td>\n",
       "      <td>110.650887</td>\n",
       "      <td>110.008857</td>\n",
       "      <td>71.785029</td>\n",
       "      <td>141.782071</td>\n",
       "      <td>125.802032</td>\n",
       "      <td>113.221141</td>\n",
       "      <td>91.163433</td>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>128.455467</td>\n",
       "      <td>113.305564</td>\n",
       "      <td>120.534195</td>\n",
       "      <td>70.567774</td>\n",
       "      <td>150.202237</td>\n",
       "      <td>147.104822</td>\n",
       "      <td>128.145483</td>\n",
       "      <td>115.972568</td>\n",
       "      <td>118.499662</td>\n",
       "      <td>104.549275</td>\n",
       "      <td>...</td>\n",
       "      <td>114.402412</td>\n",
       "      <td>100.797834</td>\n",
       "      <td>77.919668</td>\n",
       "      <td>139.414337</td>\n",
       "      <td>117.483786</td>\n",
       "      <td>123.049769</td>\n",
       "      <td>91.451389</td>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>122.108331</td>\n",
       "      <td>117.023437</td>\n",
       "      <td>102.527909</td>\n",
       "      <td>87.494457</td>\n",
       "      <td>151.737028</td>\n",
       "      <td>140.752720</td>\n",
       "      <td>139.551852</td>\n",
       "      <td>113.287240</td>\n",
       "      <td>119.649711</td>\n",
       "      <td>114.469436</td>\n",
       "      <td>...</td>\n",
       "      <td>114.664490</td>\n",
       "      <td>110.627432</td>\n",
       "      <td>76.722995</td>\n",
       "      <td>138.251458</td>\n",
       "      <td>128.808403</td>\n",
       "      <td>123.226069</td>\n",
       "      <td>96.901824</td>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     103.267633  131.597806   81.831203  108.480113  126.401200  153.464898   \n",
       "1      91.665371  134.314718   83.926980  126.167508  127.048010  147.329908   \n",
       "2     108.052816  132.300121   76.009308  116.964033  132.173743  143.051777   \n",
       "3     101.053301  127.103974   84.735175  115.929306  118.353146  149.874228   \n",
       "4     108.955215  128.747209   82.213517  116.666273  125.771908  153.445316   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.179344  114.641972  105.710144   88.287079  149.165563  137.537974   \n",
       "2439  126.365931  110.972446  103.198207   73.414657  146.420909  140.802299   \n",
       "2440  124.285145  103.360797  109.290753   79.003261  151.453751  138.577223   \n",
       "2441  128.455467  113.305564  120.534195   70.567774  150.202237  147.104822   \n",
       "2442  122.108331  117.023437  102.527909   87.494457  151.737028  140.752720   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10  ...    sensor42  \\\n",
       "0     112.979443  138.850280   95.879983  120.538335  ...  130.490611   \n",
       "1     113.967891  144.590411   97.747726  127.941590  ...  127.762477   \n",
       "2     115.162722  137.676604  102.738553  122.522861  ...  128.192048   \n",
       "3     106.017232  134.139507   98.226181  122.213773  ...  129.344039   \n",
       "4     111.663266  140.358840   96.138273  128.801601  ...  126.900903   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "2438  133.750673  112.015302  120.576864  112.474493  ...  114.383990   \n",
       "2439  136.087553  119.668856  117.611405   97.729138  ...  108.758001   \n",
       "2440  133.219135  111.313822  121.482318  115.272484  ...  110.650887   \n",
       "2441  128.145483  115.972568  118.499662  104.549275  ...  114.402412   \n",
       "2442  139.551852  113.287240  119.649711  114.469436  ...  114.664490   \n",
       "\n",
       "        sensor43    sensor44    sensor45    sensor46    sensor47    sensor48  \\\n",
       "0      73.119448  121.722946  104.559876  137.225660   91.041703  127.652224   \n",
       "1      66.005689  115.881671  107.128477  140.352729   86.947794  119.210396   \n",
       "2      75.164417  116.706805  112.276377  129.772121   88.276306  117.740004   \n",
       "3      66.522773  117.580686  107.813133  134.786828   91.081817  118.908161   \n",
       "4      71.984768  110.931963  113.277748  139.563546   92.386548  129.117372   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  102.902100   74.953746  136.775728  135.511864  113.860294   97.289917   \n",
       "2439   97.232046   76.553890  131.527636  120.620501  114.793561   86.316080   \n",
       "2440  110.008857   71.785029  141.782071  125.802032  113.221141   91.163433   \n",
       "2441  100.797834   77.919668  139.414337  117.483786  123.049769   91.451389   \n",
       "2442  110.627432   76.722995  138.251458  128.808403  123.226069   96.901824   \n",
       "\n",
       "          Pos X      Pos Y  Pos Z  \n",
       "0    -45.581275  30.239368 -60.00  \n",
       "1    -45.188830  30.181623 -59.96  \n",
       "2    -44.791865  30.131806 -59.92  \n",
       "3    -44.390422  30.089935 -59.88  \n",
       "4    -43.984540  30.056029 -59.84  \n",
       "...         ...        ...    ...  \n",
       "2438 -59.939858  51.788725  37.52  \n",
       "2439 -59.963718  51.389997  37.56  \n",
       "2440 -59.981583  50.990713  37.60  \n",
       "2441 -59.993448  50.591032  37.64  \n",
       "2442 -59.999315  50.191116  37.68  \n",
       "\n",
       "[2443 rows x 51 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.concat([sensors_data, position_data], axis = 1)\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420340aa",
   "metadata": {},
   "source": [
    "# Split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6ce133ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(sensors_data, position_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b13c5df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>136.310276</td>\n",
       "      <td>150.384808</td>\n",
       "      <td>115.241974</td>\n",
       "      <td>137.637955</td>\n",
       "      <td>117.291541</td>\n",
       "      <td>146.146202</td>\n",
       "      <td>78.341112</td>\n",
       "      <td>97.943548</td>\n",
       "      <td>134.871243</td>\n",
       "      <td>152.990180</td>\n",
       "      <td>...</td>\n",
       "      <td>63.914573</td>\n",
       "      <td>87.619839</td>\n",
       "      <td>124.152237</td>\n",
       "      <td>144.832337</td>\n",
       "      <td>88.736673</td>\n",
       "      <td>121.186405</td>\n",
       "      <td>108.173911</td>\n",
       "      <td>130.995238</td>\n",
       "      <td>65.964310</td>\n",
       "      <td>105.349250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>117.216199</td>\n",
       "      <td>143.936832</td>\n",
       "      <td>85.941792</td>\n",
       "      <td>113.718086</td>\n",
       "      <td>129.516070</td>\n",
       "      <td>143.437491</td>\n",
       "      <td>89.314080</td>\n",
       "      <td>120.160527</td>\n",
       "      <td>119.588970</td>\n",
       "      <td>133.328797</td>\n",
       "      <td>...</td>\n",
       "      <td>96.977724</td>\n",
       "      <td>94.758331</td>\n",
       "      <td>115.035795</td>\n",
       "      <td>137.374467</td>\n",
       "      <td>69.654533</td>\n",
       "      <td>99.454581</td>\n",
       "      <td>127.165148</td>\n",
       "      <td>133.664254</td>\n",
       "      <td>80.935308</td>\n",
       "      <td>112.044007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>146.817294</td>\n",
       "      <td>150.086441</td>\n",
       "      <td>121.703266</td>\n",
       "      <td>124.232207</td>\n",
       "      <td>123.061674</td>\n",
       "      <td>124.593387</td>\n",
       "      <td>81.544715</td>\n",
       "      <td>90.485171</td>\n",
       "      <td>139.849309</td>\n",
       "      <td>134.497568</td>\n",
       "      <td>...</td>\n",
       "      <td>77.396928</td>\n",
       "      <td>76.561616</td>\n",
       "      <td>135.041172</td>\n",
       "      <td>127.084607</td>\n",
       "      <td>93.700129</td>\n",
       "      <td>103.835466</td>\n",
       "      <td>120.820151</td>\n",
       "      <td>128.786508</td>\n",
       "      <td>94.515361</td>\n",
       "      <td>80.176141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>125.850460</td>\n",
       "      <td>140.721249</td>\n",
       "      <td>119.845559</td>\n",
       "      <td>134.493230</td>\n",
       "      <td>102.363626</td>\n",
       "      <td>129.254969</td>\n",
       "      <td>79.529738</td>\n",
       "      <td>117.701210</td>\n",
       "      <td>134.691676</td>\n",
       "      <td>143.238576</td>\n",
       "      <td>...</td>\n",
       "      <td>72.204929</td>\n",
       "      <td>90.847143</td>\n",
       "      <td>114.974615</td>\n",
       "      <td>130.542340</td>\n",
       "      <td>95.172722</td>\n",
       "      <td>122.963722</td>\n",
       "      <td>98.014574</td>\n",
       "      <td>126.879814</td>\n",
       "      <td>78.669088</td>\n",
       "      <td>109.928746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2197</th>\n",
       "      <td>139.014239</td>\n",
       "      <td>128.180114</td>\n",
       "      <td>132.661534</td>\n",
       "      <td>120.944083</td>\n",
       "      <td>119.856020</td>\n",
       "      <td>100.881685</td>\n",
       "      <td>109.353318</td>\n",
       "      <td>87.625060</td>\n",
       "      <td>142.014413</td>\n",
       "      <td>125.605938</td>\n",
       "      <td>...</td>\n",
       "      <td>86.506994</td>\n",
       "      <td>73.733019</td>\n",
       "      <td>130.082710</td>\n",
       "      <td>114.724938</td>\n",
       "      <td>122.117498</td>\n",
       "      <td>93.762063</td>\n",
       "      <td>112.250302</td>\n",
       "      <td>99.193848</td>\n",
       "      <td>96.431511</td>\n",
       "      <td>84.568286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638</th>\n",
       "      <td>130.751581</td>\n",
       "      <td>128.682978</td>\n",
       "      <td>105.120199</td>\n",
       "      <td>100.689118</td>\n",
       "      <td>129.783160</td>\n",
       "      <td>129.376056</td>\n",
       "      <td>105.830734</td>\n",
       "      <td>109.113527</td>\n",
       "      <td>132.046751</td>\n",
       "      <td>121.223918</td>\n",
       "      <td>...</td>\n",
       "      <td>91.460819</td>\n",
       "      <td>78.253837</td>\n",
       "      <td>122.091667</td>\n",
       "      <td>111.124861</td>\n",
       "      <td>103.913664</td>\n",
       "      <td>99.922609</td>\n",
       "      <td>122.707208</td>\n",
       "      <td>126.397366</td>\n",
       "      <td>99.969125</td>\n",
       "      <td>88.134085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>123.885788</td>\n",
       "      <td>139.526533</td>\n",
       "      <td>84.737342</td>\n",
       "      <td>96.710272</td>\n",
       "      <td>137.029086</td>\n",
       "      <td>149.761441</td>\n",
       "      <td>114.098792</td>\n",
       "      <td>116.684965</td>\n",
       "      <td>123.098980</td>\n",
       "      <td>131.044254</td>\n",
       "      <td>...</td>\n",
       "      <td>102.542809</td>\n",
       "      <td>111.914253</td>\n",
       "      <td>117.520099</td>\n",
       "      <td>127.223376</td>\n",
       "      <td>75.560846</td>\n",
       "      <td>92.103865</td>\n",
       "      <td>121.316796</td>\n",
       "      <td>133.848540</td>\n",
       "      <td>97.383919</td>\n",
       "      <td>102.297738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>117.705787</td>\n",
       "      <td>127.908987</td>\n",
       "      <td>86.522797</td>\n",
       "      <td>106.445743</td>\n",
       "      <td>143.134803</td>\n",
       "      <td>148.555034</td>\n",
       "      <td>113.875577</td>\n",
       "      <td>122.116334</td>\n",
       "      <td>115.590718</td>\n",
       "      <td>122.626193</td>\n",
       "      <td>...</td>\n",
       "      <td>107.494969</td>\n",
       "      <td>107.256587</td>\n",
       "      <td>128.427037</td>\n",
       "      <td>129.324954</td>\n",
       "      <td>77.388964</td>\n",
       "      <td>100.409632</td>\n",
       "      <td>134.609795</td>\n",
       "      <td>138.051854</td>\n",
       "      <td>97.544267</td>\n",
       "      <td>94.832262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>120.007510</td>\n",
       "      <td>126.895272</td>\n",
       "      <td>107.170011</td>\n",
       "      <td>102.032684</td>\n",
       "      <td>125.874924</td>\n",
       "      <td>127.836227</td>\n",
       "      <td>116.742468</td>\n",
       "      <td>115.698891</td>\n",
       "      <td>108.807729</td>\n",
       "      <td>107.730241</td>\n",
       "      <td>...</td>\n",
       "      <td>112.265567</td>\n",
       "      <td>99.373877</td>\n",
       "      <td>107.203389</td>\n",
       "      <td>119.762184</td>\n",
       "      <td>87.913304</td>\n",
       "      <td>95.052745</td>\n",
       "      <td>114.163863</td>\n",
       "      <td>120.113389</td>\n",
       "      <td>101.505071</td>\n",
       "      <td>107.527440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>142.091967</td>\n",
       "      <td>144.929032</td>\n",
       "      <td>117.727627</td>\n",
       "      <td>131.181538</td>\n",
       "      <td>121.255144</td>\n",
       "      <td>126.722988</td>\n",
       "      <td>87.921926</td>\n",
       "      <td>93.251797</td>\n",
       "      <td>140.945402</td>\n",
       "      <td>155.922155</td>\n",
       "      <td>...</td>\n",
       "      <td>73.086046</td>\n",
       "      <td>71.315165</td>\n",
       "      <td>121.956253</td>\n",
       "      <td>134.890009</td>\n",
       "      <td>91.081224</td>\n",
       "      <td>116.285614</td>\n",
       "      <td>107.783692</td>\n",
       "      <td>120.950343</td>\n",
       "      <td>78.687291</td>\n",
       "      <td>94.793262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1954 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "233   136.310276  150.384808  115.241974  137.637955  117.291541  146.146202   \n",
       "433   117.216199  143.936832   85.941792  113.718086  129.516070  143.437491   \n",
       "1430  146.817294  150.086441  121.703266  124.232207  123.061674  124.593387   \n",
       "307   125.850460  140.721249  119.845559  134.493230  102.363626  129.254969   \n",
       "2197  139.014239  128.180114  132.661534  120.944083  119.856020  100.881685   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "1638  130.751581  128.682978  105.120199  100.689118  129.783160  129.376056   \n",
       "1095  123.885788  139.526533   84.737342   96.710272  137.029086  149.761441   \n",
       "1130  117.705787  127.908987   86.522797  106.445743  143.134803  148.555034   \n",
       "1294  120.007510  126.895272  107.170011  102.032684  125.874924  127.836227   \n",
       "860   142.091967  144.929032  117.727627  131.181538  121.255144  126.722988   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10  ...    sensor39  \\\n",
       "233    78.341112   97.943548  134.871243  152.990180  ...   63.914573   \n",
       "433    89.314080  120.160527  119.588970  133.328797  ...   96.977724   \n",
       "1430   81.544715   90.485171  139.849309  134.497568  ...   77.396928   \n",
       "307    79.529738  117.701210  134.691676  143.238576  ...   72.204929   \n",
       "2197  109.353318   87.625060  142.014413  125.605938  ...   86.506994   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "1638  105.830734  109.113527  132.046751  121.223918  ...   91.460819   \n",
       "1095  114.098792  116.684965  123.098980  131.044254  ...  102.542809   \n",
       "1130  113.875577  122.116334  115.590718  122.626193  ...  107.494969   \n",
       "1294  116.742468  115.698891  108.807729  107.730241  ...  112.265567   \n",
       "860    87.921926   93.251797  140.945402  155.922155  ...   73.086046   \n",
       "\n",
       "        sensor40    sensor41    sensor42    sensor43    sensor44    sensor45  \\\n",
       "233    87.619839  124.152237  144.832337   88.736673  121.186405  108.173911   \n",
       "433    94.758331  115.035795  137.374467   69.654533   99.454581  127.165148   \n",
       "1430   76.561616  135.041172  127.084607   93.700129  103.835466  120.820151   \n",
       "307    90.847143  114.974615  130.542340   95.172722  122.963722   98.014574   \n",
       "2197   73.733019  130.082710  114.724938  122.117498   93.762063  112.250302   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "1638   78.253837  122.091667  111.124861  103.913664   99.922609  122.707208   \n",
       "1095  111.914253  117.520099  127.223376   75.560846   92.103865  121.316796   \n",
       "1130  107.256587  128.427037  129.324954   77.388964  100.409632  134.609795   \n",
       "1294   99.373877  107.203389  119.762184   87.913304   95.052745  114.163863   \n",
       "860    71.315165  121.956253  134.890009   91.081224  116.285614  107.783692   \n",
       "\n",
       "        sensor46    sensor47    sensor48  \n",
       "233   130.995238   65.964310  105.349250  \n",
       "433   133.664254   80.935308  112.044007  \n",
       "1430  128.786508   94.515361   80.176141  \n",
       "307   126.879814   78.669088  109.928746  \n",
       "2197   99.193848   96.431511   84.568286  \n",
       "...          ...         ...         ...  \n",
       "1638  126.397366   99.969125   88.134085  \n",
       "1095  133.848540   97.383919  102.297738  \n",
       "1130  138.051854   97.544267   94.832262  \n",
       "1294  120.113389  101.505071  107.527440  \n",
       "860   120.950343   78.687291   94.793262  \n",
       "\n",
       "[1954 rows x 48 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ba8fa0af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>59.678117</td>\n",
       "      <td>54.115562</td>\n",
       "      <td>-50.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>-19.191177</td>\n",
       "      <td>62.122007</td>\n",
       "      <td>-42.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>45.890531</td>\n",
       "      <td>69.708995</td>\n",
       "      <td>-2.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>48.255475</td>\n",
       "      <td>30.881811</td>\n",
       "      <td>-47.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2197</th>\n",
       "      <td>46.396180</td>\n",
       "      <td>30.387490</td>\n",
       "      <td>27.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638</th>\n",
       "      <td>11.378450</td>\n",
       "      <td>42.552019</td>\n",
       "      <td>5.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>-36.900143</td>\n",
       "      <td>69.397796</td>\n",
       "      <td>-16.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>-50.886002</td>\n",
       "      <td>67.974162</td>\n",
       "      <td>-14.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>-28.195387</td>\n",
       "      <td>33.407817</td>\n",
       "      <td>-8.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>59.591095</td>\n",
       "      <td>54.630197</td>\n",
       "      <td>-25.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1954 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "233   59.678117  54.115562 -50.68\n",
       "433  -19.191177  62.122007 -42.68\n",
       "1430  45.890531  69.708995  -2.80\n",
       "307   48.255475  30.881811 -47.72\n",
       "2197  46.396180  30.387490  27.88\n",
       "...         ...        ...    ...\n",
       "1638  11.378450  42.552019   5.52\n",
       "1095 -36.900143  69.397796 -16.20\n",
       "1130 -50.886002  67.974162 -14.80\n",
       "1294 -28.195387  33.407817  -8.24\n",
       "860   59.591095  54.630197 -25.60\n",
       "\n",
       "[1954 rows x 3 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377eab42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2279475b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bbc101",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4677b10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e427d50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abe89b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6657cac1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460ebdaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8deda6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9d2ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b011814e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ad4c52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc852324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e19035c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72da024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f224f20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12950c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa25b040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9304be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e1bae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8a5230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8261235a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98877a01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15704808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d068365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d3cdd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff4fc04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e9b1a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ad64e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe83856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5761f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddd95403",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fc14e018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a3f5e530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to numpy array\n",
    "X_train = X_train.to_numpy()\n",
    "y_train = y_train.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460745e8",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d5391ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "62/62 [==============================] - 9s 9ms/step - loss: 1731.9226\n",
      "Epoch 2/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1384.7493\n",
      "Epoch 3/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 967.3941\n",
      "Epoch 4/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 915.0701\n",
      "Epoch 5/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 906.9567\n",
      "Epoch 6/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 888.7769\n",
      "Epoch 7/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 848.6707\n",
      "Epoch 8/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 774.0081\n",
      "Epoch 9/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 714.3827\n",
      "Epoch 10/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 682.0687\n",
      "Epoch 11/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 669.4859\n",
      "Epoch 12/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 664.2800\n",
      "Epoch 13/500\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 657.2150\n",
      "Epoch 14/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 654.0415\n",
      "Epoch 15/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 657.8191\n",
      "Epoch 16/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 647.5029\n",
      "Epoch 17/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 647.2701\n",
      "Epoch 18/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 639.2938\n",
      "Epoch 19/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 632.7399\n",
      "Epoch 20/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 622.6584\n",
      "Epoch 21/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 610.3101\n",
      "Epoch 22/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 581.6552\n",
      "Epoch 23/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 537.0967\n",
      "Epoch 24/500\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 452.2956\n",
      "Epoch 25/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 318.0017\n",
      "Epoch 26/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 221.9963\n",
      "Epoch 27/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 168.6774\n",
      "Epoch 28/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 150.1761\n",
      "Epoch 29/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 130.3762\n",
      "Epoch 30/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 114.8153\n",
      "Epoch 31/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 104.6292\n",
      "Epoch 32/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 98.2980\n",
      "Epoch 33/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 93.7851\n",
      "Epoch 34/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 89.1482\n",
      "Epoch 35/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 88.7640\n",
      "Epoch 36/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 85.6248\n",
      "Epoch 37/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 84.6629\n",
      "Epoch 38/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 81.7038\n",
      "Epoch 39/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 78.4308\n",
      "Epoch 40/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 80.6828\n",
      "Epoch 41/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 76.6879\n",
      "Epoch 42/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 80.1489\n",
      "Epoch 43/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 75.5747\n",
      "Epoch 44/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 73.2271\n",
      "Epoch 45/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 72.1436\n",
      "Epoch 46/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 70.4251\n",
      "Epoch 47/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 66.5961\n",
      "Epoch 48/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 66.0976\n",
      "Epoch 49/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 65.4928\n",
      "Epoch 50/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 62.5965\n",
      "Epoch 51/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 62.5863\n",
      "Epoch 52/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 59.9995\n",
      "Epoch 53/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 57.1548\n",
      "Epoch 54/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 55.8991\n",
      "Epoch 55/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 66.2205\n",
      "Epoch 56/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 53.0650\n",
      "Epoch 57/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 51.7461\n",
      "Epoch 58/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 45.4101\n",
      "Epoch 59/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 62.1339\n",
      "Epoch 60/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 51.0969\n",
      "Epoch 61/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 47.3860\n",
      "Epoch 62/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 63.2121\n",
      "Epoch 63/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 53.7074\n",
      "Epoch 64/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 52.8420\n",
      "Epoch 65/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 48.6785\n",
      "Epoch 66/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 46.7009\n",
      "Epoch 67/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 44.8095\n",
      "Epoch 68/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 42.5865\n",
      "Epoch 69/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 39.8284\n",
      "Epoch 70/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 38.4299\n",
      "Epoch 71/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 36.0609\n",
      "Epoch 72/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 35.6430\n",
      "Epoch 73/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 33.1967\n",
      "Epoch 74/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 33.9175\n",
      "Epoch 75/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 33.6725\n",
      "Epoch 76/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 29.0636\n",
      "Epoch 77/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 28.9614\n",
      "Epoch 78/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 30.4369\n",
      "Epoch 79/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 28.0511\n",
      "Epoch 80/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 28.7270\n",
      "Epoch 81/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 27.0403\n",
      "Epoch 82/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 26.5162\n",
      "Epoch 83/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 27.4104\n",
      "Epoch 84/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 25.6825\n",
      "Epoch 85/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 25.2871\n",
      "Epoch 86/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 24.7825\n",
      "Epoch 87/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 24.6493\n",
      "Epoch 88/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 24.0852\n",
      "Epoch 89/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 24.0079\n",
      "Epoch 90/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 23.2650\n",
      "Epoch 91/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 23.8962\n",
      "Epoch 92/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 22.3942\n",
      "Epoch 93/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 22.9902\n",
      "Epoch 94/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 22.2901\n",
      "Epoch 95/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 22.8577\n",
      "Epoch 96/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 21.9081\n",
      "Epoch 97/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 21.9006\n",
      "Epoch 98/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 23.8000\n",
      "Epoch 99/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 21.7467\n",
      "Epoch 100/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 0s 3ms/step - loss: 21.5977\n",
      "Epoch 101/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 23.1522\n",
      "Epoch 102/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 21.3285\n",
      "Epoch 103/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 22.9323\n",
      "Epoch 104/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 20.3334\n",
      "Epoch 105/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 20.2644\n",
      "Epoch 106/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 20.4320\n",
      "Epoch 107/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 20.4838\n",
      "Epoch 108/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 19.1121\n",
      "Epoch 109/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 18.9082\n",
      "Epoch 110/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 18.9239\n",
      "Epoch 111/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 20.2335\n",
      "Epoch 112/500\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 19.5475\n",
      "Epoch 113/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 18.6538\n",
      "Epoch 114/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 18.9200\n",
      "Epoch 115/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 20.3729\n",
      "Epoch 116/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 18.7675\n",
      "Epoch 117/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 18.2982\n",
      "Epoch 118/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 17.3992\n",
      "Epoch 119/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 17.4918\n",
      "Epoch 120/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 18.0647\n",
      "Epoch 121/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 18.1553\n",
      "Epoch 122/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 17.3285\n",
      "Epoch 123/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 18.4723\n",
      "Epoch 124/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 17.3101\n",
      "Epoch 125/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 17.2267\n",
      "Epoch 126/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 19.1819\n",
      "Epoch 127/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 16.3161\n",
      "Epoch 128/500\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 16.5476\n",
      "Epoch 129/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 19.8487\n",
      "Epoch 130/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 16.1812\n",
      "Epoch 131/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 20.3956\n",
      "Epoch 132/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 17.4772\n",
      "Epoch 133/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 18.4745\n",
      "Epoch 134/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 16.1719\n",
      "Epoch 135/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 16.5569\n",
      "Epoch 136/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 17.3706\n",
      "Epoch 137/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 16.7307\n",
      "Epoch 138/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 15.2701\n",
      "Epoch 139/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 15.5852\n",
      "Epoch 140/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 16.4626\n",
      "Epoch 141/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 14.9854\n",
      "Epoch 142/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 15.3603\n",
      "Epoch 143/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 15.2069\n",
      "Epoch 144/500\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 14.9714\n",
      "Epoch 145/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 14.0045\n",
      "Epoch 146/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 14.5619\n",
      "Epoch 147/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 14.5220\n",
      "Epoch 148/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 14.7133\n",
      "Epoch 149/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 14.9775\n",
      "Epoch 150/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 14.7308\n",
      "Epoch 151/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 14.6305\n",
      "Epoch 152/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 14.5201\n",
      "Epoch 153/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 14.5151\n",
      "Epoch 154/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 17.4476\n",
      "Epoch 155/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 15.5355\n",
      "Epoch 156/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 14.0209\n",
      "Epoch 157/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 15.1691\n",
      "Epoch 158/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 14.0160\n",
      "Epoch 159/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 14.1529\n",
      "Epoch 160/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 13.6868\n",
      "Epoch 161/500\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 15.9146\n",
      "Epoch 162/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 13.7445\n",
      "Epoch 163/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 13.6381\n",
      "Epoch 164/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 13.3946\n",
      "Epoch 165/500\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 13.1943\n",
      "Epoch 166/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 14.2073\n",
      "Epoch 167/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 14.0534\n",
      "Epoch 168/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 14.4194\n",
      "Epoch 169/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 14.0708\n",
      "Epoch 170/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 13.0189\n",
      "Epoch 171/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 14.9181\n",
      "Epoch 172/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 13.3559\n",
      "Epoch 173/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 14.0832\n",
      "Epoch 174/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 12.5810\n",
      "Epoch 175/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 13.4426\n",
      "Epoch 176/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 12.0589\n",
      "Epoch 177/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 11.9951\n",
      "Epoch 178/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 13.0024\n",
      "Epoch 179/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 13.3482\n",
      "Epoch 180/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 14.3504\n",
      "Epoch 181/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 12.7955\n",
      "Epoch 182/500\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 12.8200\n",
      "Epoch 183/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 14.5841\n",
      "Epoch 184/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 13.8075\n",
      "Epoch 185/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 12.9024\n",
      "Epoch 186/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 12.7551\n",
      "Epoch 187/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 12.9381\n",
      "Epoch 188/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 12.2108\n",
      "Epoch 189/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 11.9001\n",
      "Epoch 190/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 11.9509\n",
      "Epoch 191/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 12.8541\n",
      "Epoch 192/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 11.8370\n",
      "Epoch 193/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 12.3961\n",
      "Epoch 194/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 11.8243\n",
      "Epoch 195/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 13.5776\n",
      "Epoch 196/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 12.3403\n",
      "Epoch 197/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 12.4911\n",
      "Epoch 198/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 0s 5ms/step - loss: 11.9641\n",
      "Epoch 199/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 12.1367\n",
      "Epoch 200/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 12.1529\n",
      "Epoch 201/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 11.9317\n",
      "Epoch 202/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 11.8566\n",
      "Epoch 203/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 11.8933\n",
      "Epoch 204/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 12.8305\n",
      "Epoch 205/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 11.6357\n",
      "Epoch 206/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 11.5022\n",
      "Epoch 207/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 13.2986\n",
      "Epoch 208/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 11.5130\n",
      "Epoch 209/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 10.8637\n",
      "Epoch 210/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 11.0912\n",
      "Epoch 211/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 13.2401\n",
      "Epoch 212/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 12.5659\n",
      "Epoch 213/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 12.7996\n",
      "Epoch 214/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 12.0079\n",
      "Epoch 215/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 10.8172\n",
      "Epoch 216/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 11.2871\n",
      "Epoch 217/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 14.1773\n",
      "Epoch 218/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 11.1540\n",
      "Epoch 219/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 11.3586\n",
      "Epoch 220/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 11.0637\n",
      "Epoch 221/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 13.5284\n",
      "Epoch 222/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 12.2591\n",
      "Epoch 223/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 12.2037\n",
      "Epoch 224/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 10.6893\n",
      "Epoch 225/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 10.7165\n",
      "Epoch 226/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 11.4948\n",
      "Epoch 227/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 10.5933\n",
      "Epoch 228/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 11.4029\n",
      "Epoch 229/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 10.9509\n",
      "Epoch 230/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 10.3302\n",
      "Epoch 231/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 11.6141\n",
      "Epoch 232/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 10.9635\n",
      "Epoch 233/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 11.2250\n",
      "Epoch 234/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 10.9134\n",
      "Epoch 235/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 13.2395\n",
      "Epoch 236/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 12.2822\n",
      "Epoch 237/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 11.0497\n",
      "Epoch 238/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 12.8378\n",
      "Epoch 239/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 11.6537\n",
      "Epoch 240/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 11.2208\n",
      "Epoch 241/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 10.2538\n",
      "Epoch 242/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 11.4393\n",
      "Epoch 243/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 9.9499\n",
      "Epoch 244/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 10.2639\n",
      "Epoch 245/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 10.6767\n",
      "Epoch 246/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 13.7982\n",
      "Epoch 247/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 11.6761\n",
      "Epoch 248/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 11.2169\n",
      "Epoch 249/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 10.6201\n",
      "Epoch 250/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 13.2166\n",
      "Epoch 251/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 10.9556\n",
      "Epoch 252/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 10.0995\n",
      "Epoch 253/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 10.1404\n",
      "Epoch 254/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 11.6962\n",
      "Epoch 255/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 10.2928\n",
      "Epoch 256/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 10.7041\n",
      "Epoch 257/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 10.7007\n",
      "Epoch 258/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 9.3758\n",
      "Epoch 259/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 9.6527\n",
      "Epoch 260/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 10.2224\n",
      "Epoch 261/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 10.4474\n",
      "Epoch 262/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 9.6650\n",
      "Epoch 263/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 10.0603\n",
      "Epoch 264/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 9.8788\n",
      "Epoch 265/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 11.5058\n",
      "Epoch 266/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 11.2084\n",
      "Epoch 267/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 11.9925\n",
      "Epoch 268/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 9.9098\n",
      "Epoch 269/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 10.3399\n",
      "Epoch 270/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 10.2113\n",
      "Epoch 271/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 11.0648\n",
      "Epoch 272/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 10.7979\n",
      "Epoch 273/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 10.3685\n",
      "Epoch 274/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 9.9092\n",
      "Epoch 275/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 10.4605\n",
      "Epoch 276/500\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 9.8023\n",
      "Epoch 277/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 9.7637\n",
      "Epoch 278/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 9.6331\n",
      "Epoch 279/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 10.2859\n",
      "Epoch 280/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 9.0038\n",
      "Epoch 281/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 10.2140\n",
      "Epoch 282/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 9.4878\n",
      "Epoch 283/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 10.8715\n",
      "Epoch 284/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 12.5553\n",
      "Epoch 285/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 10.2982\n",
      "Epoch 286/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 9.5094\n",
      "Epoch 287/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 9.6144\n",
      "Epoch 288/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 10.3265\n",
      "Epoch 289/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 10.8383\n",
      "Epoch 290/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 9.3281\n",
      "Epoch 291/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 9.9707\n",
      "Epoch 292/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 10.0340\n",
      "Epoch 293/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 10.4963\n",
      "Epoch 294/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 10.6161\n",
      "Epoch 295/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 9.5857\n",
      "Epoch 296/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 9.7782\n",
      "Epoch 297/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 0s 5ms/step - loss: 10.9808\n",
      "Epoch 298/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 10.1638\n",
      "Epoch 299/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 9.9146\n",
      "Epoch 300/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 9.2141\n",
      "Epoch 301/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 10.2263\n",
      "Epoch 302/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 9.0103\n",
      "Epoch 303/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 9.3385\n",
      "Epoch 304/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 10.5545\n",
      "Epoch 305/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 9.4810\n",
      "Epoch 306/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 9.2198\n",
      "Epoch 307/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 9.6703\n",
      "Epoch 308/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 9.9733\n",
      "Epoch 309/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 10.4278\n",
      "Epoch 310/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 10.1406\n",
      "Epoch 311/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 9.0070\n",
      "Epoch 312/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 9.1385\n",
      "Epoch 313/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 10.5312\n",
      "Epoch 314/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 11.0062\n",
      "Epoch 315/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 9.0655\n",
      "Epoch 316/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 8.8286\n",
      "Epoch 317/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 9.0516\n",
      "Epoch 318/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 8.9895\n",
      "Epoch 319/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 9.0972\n",
      "Epoch 320/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 10.4371\n",
      "Epoch 321/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 9.4675\n",
      "Epoch 322/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 8.7354\n",
      "Epoch 323/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 8.6655\n",
      "Epoch 324/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 9.8146\n",
      "Epoch 325/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 9.6372\n",
      "Epoch 326/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 10.2615\n",
      "Epoch 327/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 9.4183\n",
      "Epoch 328/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 12.5217\n",
      "Epoch 329/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 9.4241\n",
      "Epoch 330/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 9.1702\n",
      "Epoch 331/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 9.0245\n",
      "Epoch 332/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 9.4847\n",
      "Epoch 333/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 8.0543\n",
      "Epoch 334/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 9.0741\n",
      "Epoch 335/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 10.3411\n",
      "Epoch 336/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 9.1322\n",
      "Epoch 337/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 8.6092\n",
      "Epoch 338/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 8.5181\n",
      "Epoch 339/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 8.6184\n",
      "Epoch 340/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 10.0594\n",
      "Epoch 341/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 8.9841\n",
      "Epoch 342/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 11.0310\n",
      "Epoch 343/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 8.9954\n",
      "Epoch 344/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 9.0383\n",
      "Epoch 345/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 8.8162\n",
      "Epoch 346/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 9.4195\n",
      "Epoch 347/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 9.5984\n",
      "Epoch 348/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 9.9750\n",
      "Epoch 349/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 9.0539\n",
      "Epoch 350/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 10.0275\n",
      "Epoch 351/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 10.7456\n",
      "Epoch 352/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 9.1818\n",
      "Epoch 353/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 8.8904\n",
      "Epoch 354/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 9.5083\n",
      "Epoch 355/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 8.5448\n",
      "Epoch 356/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 8.7510\n",
      "Epoch 357/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 8.5516\n",
      "Epoch 358/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 8.2903\n",
      "Epoch 359/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 8.6486\n",
      "Epoch 360/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 10.5039\n",
      "Epoch 361/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 8.7073\n",
      "Epoch 362/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 8.6094\n",
      "Epoch 363/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 9.3916\n",
      "Epoch 364/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 8.9033\n",
      "Epoch 365/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 8.4691\n",
      "Epoch 366/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 9.0665\n",
      "Epoch 367/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 8.8701\n",
      "Epoch 368/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 8.5297\n",
      "Epoch 369/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 8.4823\n",
      "Epoch 370/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 10.2485\n",
      "Epoch 371/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 8.1596\n",
      "Epoch 372/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 9.0134\n",
      "Epoch 373/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 9.5456\n",
      "Epoch 374/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 7.8759\n",
      "Epoch 375/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 9.0108\n",
      "Epoch 376/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 8.3131\n",
      "Epoch 377/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 8.7114\n",
      "Epoch 378/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 8.3624\n",
      "Epoch 379/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 10.3717\n",
      "Epoch 380/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 8.0258\n",
      "Epoch 381/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 9.3873\n",
      "Epoch 382/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 8.5807\n",
      "Epoch 383/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 8.1440\n",
      "Epoch 384/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 8.9169\n",
      "Epoch 385/500\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 8.2749\n",
      "Epoch 386/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 8.7520\n",
      "Epoch 387/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 8.7037\n",
      "Epoch 388/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 8.2069\n",
      "Epoch 389/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 7.7937\n",
      "Epoch 390/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 8.3062\n",
      "Epoch 391/500\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 8.2491\n",
      "Epoch 392/500\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 8.1117\n",
      "Epoch 393/500\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 8.0582\n",
      "Epoch 394/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 8.0712\n",
      "Epoch 395/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 7.9882\n",
      "Epoch 396/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 0s 6ms/step - loss: 8.9059\n",
      "Epoch 397/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 8.3105\n",
      "Epoch 398/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 8.9835\n",
      "Epoch 399/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 8.8404\n",
      "Epoch 400/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 9.7123\n",
      "Epoch 401/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 8.1664\n",
      "Epoch 402/500\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 9.1215\n",
      "Epoch 403/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 8.2584\n",
      "Epoch 404/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 8.3509\n",
      "Epoch 405/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 9.2938\n",
      "Epoch 406/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 8.7609\n",
      "Epoch 407/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 8.1034\n",
      "Epoch 408/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 8.1444\n",
      "Epoch 409/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 8.6608\n",
      "Epoch 410/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 8.6455\n",
      "Epoch 411/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 8.1304\n",
      "Epoch 412/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 10.0197\n",
      "Epoch 413/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 9.6413\n",
      "Epoch 414/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 9.9544\n",
      "Epoch 415/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 8.1379\n",
      "Epoch 416/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 8.6518\n",
      "Epoch 417/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 7.8434\n",
      "Epoch 418/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 8.5955\n",
      "Epoch 419/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 8.5380\n",
      "Epoch 420/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 8.4079\n",
      "Epoch 421/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 7.6959\n",
      "Epoch 422/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 8.2192\n",
      "Epoch 423/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 8.9453\n",
      "Epoch 424/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 8.6848\n",
      "Epoch 425/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 8.3184\n",
      "Epoch 426/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 8.2295\n",
      "Epoch 427/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 7.9015\n",
      "Epoch 428/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 7.4750\n",
      "Epoch 429/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 7.7644\n",
      "Epoch 430/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 7.9856\n",
      "Epoch 431/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 10.4361\n",
      "Epoch 432/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 7.8276\n",
      "Epoch 433/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 7.7318\n",
      "Epoch 434/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 8.1773\n",
      "Epoch 435/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 8.0138\n",
      "Epoch 436/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 8.6010\n",
      "Epoch 437/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 8.8760\n",
      "Epoch 438/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 8.3303\n",
      "Epoch 439/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 8.1432\n",
      "Epoch 440/500\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 7.9312\n",
      "Epoch 441/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 7.4532\n",
      "Epoch 442/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 7.7151\n",
      "Epoch 443/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 9.2646\n",
      "Epoch 444/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 8.1241\n",
      "Epoch 445/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 7.6609\n",
      "Epoch 446/500\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 8.8599\n",
      "Epoch 447/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 8.0173\n",
      "Epoch 448/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 7.7494\n",
      "Epoch 449/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 7.5741\n",
      "Epoch 450/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 8.0082\n",
      "Epoch 451/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 7.9865\n",
      "Epoch 452/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 8.4382\n",
      "Epoch 453/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 8.3456\n",
      "Epoch 454/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 8.8673\n",
      "Epoch 455/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 8.1497\n",
      "Epoch 456/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 7.6388\n",
      "Epoch 457/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 7.9225\n",
      "Epoch 458/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 9.6312\n",
      "Epoch 459/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 7.4352\n",
      "Epoch 460/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 8.3394\n",
      "Epoch 461/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 8.8644\n",
      "Epoch 462/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 8.8564\n",
      "Epoch 463/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 8.7600\n",
      "Epoch 464/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 8.9642\n",
      "Epoch 465/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 7.7182\n",
      "Epoch 466/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 7.3123\n",
      "Epoch 467/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 8.9741\n",
      "Epoch 468/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 7.8066\n",
      "Epoch 469/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 8.1821\n",
      "Epoch 470/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 7.7392\n",
      "Epoch 471/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 8.4286\n",
      "Epoch 472/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 7.1405\n",
      "Epoch 473/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 9.3507\n",
      "Epoch 474/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 7.6546\n",
      "Epoch 475/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 7.3694\n",
      "Epoch 476/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 8.2347\n",
      "Epoch 477/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 7.8398\n",
      "Epoch 478/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 7.5869\n",
      "Epoch 479/500\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 6.8620\n",
      "Epoch 480/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 8.2324\n",
      "Epoch 481/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 7.3575\n",
      "Epoch 482/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 7.1522\n",
      "Epoch 483/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 8.0779\n",
      "Epoch 484/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 7.8112\n",
      "Epoch 485/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 7.4647\n",
      "Epoch 486/500\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 7.3902\n",
      "Epoch 487/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 7.5311\n",
      "Epoch 488/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 7.8315\n",
      "Epoch 489/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 7.7849\n",
      "Epoch 490/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 7.7599\n",
      "Epoch 491/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 7.2823\n",
      "Epoch 492/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 8.0417\n",
      "Epoch 493/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 9.8478\n",
      "Epoch 494/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 7.8157\n",
      "Epoch 495/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 9.0405\n",
      "Epoch 496/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 0s 5ms/step - loss: 7.9427\n",
      "Epoch 497/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 7.1666\n",
      "Epoch 498/500\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 7.4477\n",
      "Epoch 499/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 7.4969\n",
      "Epoch 500/500\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 7.5783\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 14.9653\n",
      "MSE: 14.965250015258789\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(X_train.shape[1], 1)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(3))\n",
    "\n",
    "adam = Adam(lr=0.001)\n",
    "model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=500, batch_size=32)\n",
    "\n",
    "mse = model.evaluate(X_test, y_test)\n",
    "print(\"MSE:\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e331cc00",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "68e93dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error, explained_variance_score, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4a8fe3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predict on test data\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "efddede3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 14.965249646651458\n",
      "Mean Absolute Error: 3.005593015680082\n",
      "Median Absolute Error: 2.332109593909172\n",
      "Explained Variance Score: 0.9793858888091069\n",
      "R2 Score: 0.9657818931185812\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
    "print(\"Mean Absolute Error:\", mean_absolute_error(y_test, y_pred))\n",
    "print(\"Median Absolute Error:\", median_absolute_error(y_test, y_pred))\n",
    "print(\"Explained Variance Score:\", explained_variance_score(y_test, y_pred))\n",
    "print(\"R2 Score:\", r2_score(y_test, y_pred))\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeb76c8",
   "metadata": {},
   "source": [
    "# Anthropic: Claude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7f7b27",
   "metadata": {},
   "source": [
    "# MSE: 1.166367769241333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e28925a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b27eca8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "62/62 [==============================] - 4s 23ms/step - loss: 37.2614 - val_loss: 37.8172\n",
      "Epoch 2/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 36.8640 - val_loss: 37.6818\n",
      "Epoch 3/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 35.8020 - val_loss: 36.1831\n",
      "Epoch 4/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 32.4145 - val_loss: 32.1887\n",
      "Epoch 5/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 29.3121 - val_loss: 25.4847\n",
      "Epoch 6/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 26.0631 - val_loss: 27.6205\n",
      "Epoch 7/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 23.8589 - val_loss: 25.0238\n",
      "Epoch 8/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 22.8260 - val_loss: 21.2005\n",
      "Epoch 9/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 20.9077 - val_loss: 19.3513\n",
      "Epoch 10/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 20.0885 - val_loss: 17.0631\n",
      "Epoch 11/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 18.2921 - val_loss: 17.3357\n",
      "Epoch 12/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 18.7708 - val_loss: 17.7282\n",
      "Epoch 13/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 18.2965 - val_loss: 15.5890\n",
      "Epoch 14/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 18.0860 - val_loss: 15.1957\n",
      "Epoch 15/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 17.1306 - val_loss: 16.1479\n",
      "Epoch 16/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 17.6611 - val_loss: 16.2518\n",
      "Epoch 17/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 16.4181 - val_loss: 15.1031\n",
      "Epoch 18/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 16.8326 - val_loss: 16.6360\n",
      "Epoch 19/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 16.8502 - val_loss: 14.8443\n",
      "Epoch 20/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 15.8273 - val_loss: 14.8133\n",
      "Epoch 21/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 15.7036 - val_loss: 13.9195\n",
      "Epoch 22/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 15.3529 - val_loss: 14.4018\n",
      "Epoch 23/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 15.5142 - val_loss: 13.7678\n",
      "Epoch 24/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 15.0817 - val_loss: 13.5280\n",
      "Epoch 25/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 15.2393 - val_loss: 13.4329\n",
      "Epoch 26/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 14.6666 - val_loss: 12.9953\n",
      "Epoch 27/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 14.9572 - val_loss: 13.9688\n",
      "Epoch 28/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 15.2818 - val_loss: 14.8581\n",
      "Epoch 29/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 15.1661 - val_loss: 13.4014\n",
      "Epoch 30/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 14.7512 - val_loss: 12.6169\n",
      "Epoch 31/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 14.7114 - val_loss: 12.6503\n",
      "Epoch 32/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 14.3293 - val_loss: 12.2928\n",
      "Epoch 33/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 14.5383 - val_loss: 12.2945\n",
      "Epoch 34/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 14.3116 - val_loss: 13.2383\n",
      "Epoch 35/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 14.0941 - val_loss: 11.8474\n",
      "Epoch 36/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 14.4074 - val_loss: 14.4042\n",
      "Epoch 37/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 14.3465 - val_loss: 11.7007\n",
      "Epoch 38/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 13.6196 - val_loss: 12.1005\n",
      "Epoch 39/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 13.7143 - val_loss: 11.2919\n",
      "Epoch 40/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 13.9170 - val_loss: 13.4024\n",
      "Epoch 41/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 13.1870 - val_loss: 10.9066\n",
      "Epoch 42/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 13.4108 - val_loss: 12.3604\n",
      "Epoch 43/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 12.9572 - val_loss: 11.4659\n",
      "Epoch 44/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 12.7170 - val_loss: 10.2676\n",
      "Epoch 45/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 12.5460 - val_loss: 13.1766\n",
      "Epoch 46/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 12.5000 - val_loss: 9.6539\n",
      "Epoch 47/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 12.1356 - val_loss: 11.5363\n",
      "Epoch 48/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 12.0338 - val_loss: 9.4767\n",
      "Epoch 49/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 11.7484 - val_loss: 9.5145\n",
      "Epoch 50/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 11.6784 - val_loss: 9.6535\n",
      "Epoch 51/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 11.7986 - val_loss: 9.4971\n",
      "Epoch 52/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 11.5213 - val_loss: 9.6349\n",
      "Epoch 53/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 11.1940 - val_loss: 9.1977\n",
      "Epoch 54/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 11.4696 - val_loss: 9.1082\n",
      "Epoch 55/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 11.1999 - val_loss: 9.3678\n",
      "Epoch 56/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 10.9541 - val_loss: 9.8941\n",
      "Epoch 57/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 11.0857 - val_loss: 8.8233\n",
      "Epoch 58/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 10.6402 - val_loss: 8.2712\n",
      "Epoch 59/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 10.4638 - val_loss: 8.1994\n",
      "Epoch 60/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 10.0795 - val_loss: 9.2025\n",
      "Epoch 61/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 10.0777 - val_loss: 7.7601\n",
      "Epoch 62/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 9.6942 - val_loss: 8.5574\n",
      "Epoch 63/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 9.4401 - val_loss: 9.7171\n",
      "Epoch 64/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 8.9186 - val_loss: 6.7767\n",
      "Epoch 65/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 8.7446 - val_loss: 8.4335\n",
      "Epoch 66/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 8.2022 - val_loss: 8.5977\n",
      "Epoch 67/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 7.7372 - val_loss: 13.2231\n",
      "Epoch 68/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 7.3083 - val_loss: 5.9028\n",
      "Epoch 69/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 7.1195 - val_loss: 4.4088\n",
      "Epoch 70/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 6.5558 - val_loss: 5.4153\n",
      "Epoch 71/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 6.1943 - val_loss: 6.2481\n",
      "Epoch 72/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 6.1342 - val_loss: 4.9807\n",
      "Epoch 73/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.9889 - val_loss: 4.9560\n",
      "Epoch 74/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.6437 - val_loss: 5.3751\n",
      "Epoch 75/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 6.0092 - val_loss: 4.9177\n",
      "Epoch 76/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.6607 - val_loss: 3.3169\n",
      "Epoch 77/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.4520 - val_loss: 4.7711\n",
      "Epoch 78/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 5.6629 - val_loss: 4.4574\n",
      "Epoch 79/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 5.4563 - val_loss: 3.8164\n",
      "Epoch 80/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 5.4937 - val_loss: 3.9443\n",
      "Epoch 81/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 5.6431 - val_loss: 3.2365\n",
      "Epoch 82/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 5.3723 - val_loss: 3.5340\n",
      "Epoch 83/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 5.3321 - val_loss: 3.2471\n",
      "Epoch 84/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.9044 - val_loss: 2.6366\n",
      "Epoch 85/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.7617 - val_loss: 3.9335\n",
      "Epoch 86/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 5.1590 - val_loss: 3.5187\n",
      "Epoch 87/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.9966 - val_loss: 3.1540\n",
      "Epoch 88/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.6162 - val_loss: 8.4423\n",
      "Epoch 89/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 5.8291 - val_loss: 3.8768\n",
      "Epoch 90/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 5.3153 - val_loss: 2.7452\n",
      "Epoch 91/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.8631 - val_loss: 2.7522\n",
      "Epoch 92/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.7256 - val_loss: 3.2365\n",
      "Epoch 93/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.7823 - val_loss: 3.5228\n",
      "Epoch 94/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.8181 - val_loss: 3.5071\n",
      "Epoch 95/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.5050 - val_loss: 2.7620\n",
      "Epoch 96/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.8735 - val_loss: 2.8175\n",
      "Epoch 97/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.5486 - val_loss: 3.0185\n",
      "Epoch 98/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.5398 - val_loss: 3.1653\n",
      "Epoch 99/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.4999 - val_loss: 3.3271\n",
      "Epoch 100/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.1530 - val_loss: 2.4353\n",
      "Epoch 101/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.1926 - val_loss: 3.1042\n",
      "Epoch 102/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.3541 - val_loss: 2.7124\n",
      "Epoch 103/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.1050 - val_loss: 3.0544\n",
      "Epoch 104/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.5817 - val_loss: 3.3560\n",
      "Epoch 105/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.0076 - val_loss: 3.2236\n",
      "Epoch 106/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.5173 - val_loss: 2.6753\n",
      "Epoch 107/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.0001 - val_loss: 2.5252\n",
      "Epoch 108/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.3235 - val_loss: 2.7663\n",
      "Epoch 109/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.1607 - val_loss: 2.7677\n",
      "Epoch 110/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.0921 - val_loss: 2.4229\n",
      "Epoch 111/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.2946 - val_loss: 2.2766\n",
      "Epoch 112/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.1737 - val_loss: 3.0309\n",
      "Epoch 113/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.2386 - val_loss: 2.3497\n",
      "Epoch 114/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.9912 - val_loss: 2.8053\n",
      "Epoch 115/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.0291 - val_loss: 2.3369\n",
      "Epoch 116/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.0102 - val_loss: 2.0840\n",
      "Epoch 117/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.7257 - val_loss: 2.6375\n",
      "Epoch 118/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.7060 - val_loss: 2.8557\n",
      "Epoch 119/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.5485 - val_loss: 2.5548\n",
      "Epoch 120/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.4497 - val_loss: 2.5405\n",
      "Epoch 121/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.7848 - val_loss: 3.3504\n",
      "Epoch 122/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.0205 - val_loss: 3.0444\n",
      "Epoch 123/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.4407 - val_loss: 2.6058\n",
      "Epoch 124/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.9559 - val_loss: 2.2290\n",
      "Epoch 125/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.5247 - val_loss: 1.9231\n",
      "Epoch 126/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.6802 - val_loss: 2.2214\n",
      "Epoch 127/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.5857 - val_loss: 1.9940\n",
      "Epoch 128/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.5553 - val_loss: 2.7544\n",
      "Epoch 129/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.6393 - val_loss: 2.3073\n",
      "Epoch 130/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.6562 - val_loss: 2.7979\n",
      "Epoch 131/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.1853 - val_loss: 2.3554\n",
      "Epoch 132/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.4044 - val_loss: 1.9768\n",
      "Epoch 133/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.5284 - val_loss: 2.3482\n",
      "Epoch 134/1000\n",
      "62/62 [==============================] - 1s 23ms/step - loss: 3.7874 - val_loss: 2.2185\n",
      "Epoch 135/1000\n",
      "62/62 [==============================] - 1s 23ms/step - loss: 3.5775 - val_loss: 3.5880\n",
      "Epoch 136/1000\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 4.0112 - val_loss: 2.0884\n",
      "Epoch 137/1000\n",
      "62/62 [==============================] - 1s 21ms/step - loss: 3.4170 - val_loss: 2.2161\n",
      "Epoch 138/1000\n",
      "62/62 [==============================] - 1s 16ms/step - loss: 3.4431 - val_loss: 1.8381\n",
      "Epoch 139/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.4564 - val_loss: 2.1726\n",
      "Epoch 140/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.6559 - val_loss: 2.1929\n",
      "Epoch 141/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.2076 - val_loss: 1.8976\n",
      "Epoch 142/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.4428 - val_loss: 2.3826\n",
      "Epoch 143/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.3573 - val_loss: 2.0931\n",
      "Epoch 144/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.2157 - val_loss: 2.1438\n",
      "Epoch 145/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.2748 - val_loss: 2.4762\n",
      "Epoch 146/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.3832 - val_loss: 2.2227\n",
      "Epoch 147/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.4104 - val_loss: 2.0056\n",
      "Epoch 148/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.3924 - val_loss: 2.5022\n",
      "Epoch 149/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.2969 - val_loss: 1.6863\n",
      "Epoch 150/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.2387 - val_loss: 2.3251\n",
      "Epoch 151/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.4084 - val_loss: 1.6830\n",
      "Epoch 152/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.2234 - val_loss: 1.6917\n",
      "Epoch 153/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.1346 - val_loss: 2.1924\n",
      "Epoch 154/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.1926 - val_loss: 2.6810\n",
      "Epoch 155/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.1866 - val_loss: 2.0302\n",
      "Epoch 156/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.9869 - val_loss: 2.5570\n",
      "Epoch 157/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.1044 - val_loss: 2.8778\n",
      "Epoch 158/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.2355 - val_loss: 1.6766\n",
      "Epoch 159/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.9887 - val_loss: 2.2945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.2166 - val_loss: 1.8390\n",
      "Epoch 161/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.2100 - val_loss: 2.1400\n",
      "Epoch 162/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.9425 - val_loss: 1.6095\n",
      "Epoch 163/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.3811 - val_loss: 1.8739\n",
      "Epoch 164/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.0853 - val_loss: 1.6760\n",
      "Epoch 165/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.9758 - val_loss: 2.0496\n",
      "Epoch 166/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.0759 - val_loss: 1.8718\n",
      "Epoch 167/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.9538 - val_loss: 1.6743\n",
      "Epoch 168/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.9684 - val_loss: 2.9779\n",
      "Epoch 169/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.3622 - val_loss: 2.8662\n",
      "Epoch 170/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.5936 - val_loss: 1.9020\n",
      "Epoch 171/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.1104 - val_loss: 2.3260\n",
      "Epoch 172/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.0681 - val_loss: 2.0708\n",
      "Epoch 173/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.9320 - val_loss: 2.1541\n",
      "Epoch 174/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.1095 - val_loss: 2.1139\n",
      "Epoch 175/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.0345 - val_loss: 1.8961\n",
      "Epoch 176/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.0662 - val_loss: 2.2308\n",
      "Epoch 177/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.9916 - val_loss: 1.9272\n",
      "Epoch 178/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.0272 - val_loss: 2.1736\n",
      "Epoch 179/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.8609 - val_loss: 2.1698\n",
      "Epoch 180/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.7737 - val_loss: 1.7687\n",
      "Epoch 181/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.7783 - val_loss: 1.6676\n",
      "Epoch 182/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.8563 - val_loss: 2.6055\n",
      "Epoch 183/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.9398 - val_loss: 2.2254\n",
      "Epoch 184/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.8537 - val_loss: 1.4974\n",
      "Epoch 185/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.8619 - val_loss: 1.6941\n",
      "Epoch 186/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.1232 - val_loss: 1.9232\n",
      "Epoch 187/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.0335 - val_loss: 1.6307\n",
      "Epoch 188/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.7379 - val_loss: 1.6851\n",
      "Epoch 189/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.8306 - val_loss: 1.8846\n",
      "Epoch 190/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.7057 - val_loss: 2.2301\n",
      "Epoch 191/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.8986 - val_loss: 2.1555\n",
      "Epoch 192/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.9703 - val_loss: 1.6446\n",
      "Epoch 193/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.7426 - val_loss: 1.6359\n",
      "Epoch 194/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.8835 - val_loss: 1.6788\n",
      "Epoch 195/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.7601 - val_loss: 2.0845\n",
      "Epoch 196/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.7426 - val_loss: 2.0897\n",
      "Epoch 197/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.5718 - val_loss: 1.6359\n",
      "Epoch 198/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.7172 - val_loss: 1.6705\n",
      "Epoch 199/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.6283 - val_loss: 1.4855\n",
      "Epoch 200/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.7163 - val_loss: 1.5077\n",
      "Epoch 201/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.6845 - val_loss: 2.2294\n",
      "Epoch 202/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.6520 - val_loss: 1.7112\n",
      "Epoch 203/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.6981 - val_loss: 1.6343\n",
      "Epoch 204/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.5795 - val_loss: 1.9135\n",
      "Epoch 205/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.6223 - val_loss: 2.4853\n",
      "Epoch 206/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.9809 - val_loss: 2.3184\n",
      "Epoch 207/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.7716 - val_loss: 1.4846\n",
      "Epoch 208/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.6580 - val_loss: 1.4460\n",
      "Epoch 209/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.5867 - val_loss: 2.3620\n",
      "Epoch 210/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.6443 - val_loss: 1.6217\n",
      "Epoch 211/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.6677 - val_loss: 1.8006\n",
      "Epoch 212/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.5858 - val_loss: 1.9818\n",
      "Epoch 213/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.5341 - val_loss: 1.7520\n",
      "Epoch 214/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.5522 - val_loss: 1.6943\n",
      "Epoch 215/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.4148 - val_loss: 1.9607\n",
      "Epoch 216/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.6714 - val_loss: 2.0529\n",
      "Epoch 217/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.4295 - val_loss: 1.4934\n",
      "Epoch 218/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.5582 - val_loss: 1.9263\n",
      "Epoch 219/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.5196 - val_loss: 1.8433\n",
      "Epoch 220/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.4921 - val_loss: 1.6179\n",
      "Epoch 221/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.4355 - val_loss: 1.6298\n",
      "Epoch 222/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.4410 - val_loss: 1.3811\n",
      "Epoch 223/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.3502 - val_loss: 1.6039\n",
      "Epoch 224/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.4232 - val_loss: 1.6860\n",
      "Epoch 225/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.5610 - val_loss: 2.0252\n",
      "Epoch 226/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.3341 - val_loss: 2.0325\n",
      "Epoch 227/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.4742 - val_loss: 1.6850\n",
      "Epoch 228/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.3367 - val_loss: 1.8528\n",
      "Epoch 229/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.3972 - val_loss: 2.4895\n",
      "Epoch 230/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.3710 - val_loss: 1.7037\n",
      "Epoch 231/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.5138 - val_loss: 2.1050\n",
      "Epoch 232/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.4424 - val_loss: 1.4635\n",
      "Epoch 233/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.2616 - val_loss: 1.4246\n",
      "Epoch 234/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.3609 - val_loss: 1.3861\n",
      "Epoch 235/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.2912 - val_loss: 1.7401\n",
      "Epoch 236/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.5589 - val_loss: 1.8412\n",
      "Epoch 237/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.1907 - val_loss: 1.5643\n",
      "Epoch 238/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.3870 - val_loss: 1.8112\n",
      "Epoch 239/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.4652 - val_loss: 1.7338\n",
      "Epoch 240/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.5682 - val_loss: 1.5807\n",
      "Epoch 241/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.3678 - val_loss: 1.8032\n",
      "Epoch 242/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.4330 - val_loss: 1.5387\n",
      "Epoch 243/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.3458 - val_loss: 1.7640\n",
      "Epoch 244/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.4874 - val_loss: 1.4220\n",
      "Epoch 245/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.3007 - val_loss: 1.4432\n",
      "Epoch 246/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.3065 - val_loss: 1.3532\n",
      "Epoch 247/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.2565 - val_loss: 1.8611\n",
      "Epoch 248/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.3454 - val_loss: 1.4177\n",
      "Epoch 249/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.0718 - val_loss: 1.5765\n",
      "Epoch 250/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.2507 - val_loss: 1.8357\n",
      "Epoch 251/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.3781 - val_loss: 1.4221\n",
      "Epoch 252/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.3273 - val_loss: 1.2292\n",
      "Epoch 253/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.1981 - val_loss: 1.3658\n",
      "Epoch 254/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.2837 - val_loss: 1.7536\n",
      "Epoch 255/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.2729 - val_loss: 1.9967\n",
      "Epoch 256/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.3702 - val_loss: 1.4626\n",
      "Epoch 257/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.3988 - val_loss: 1.3666\n",
      "Epoch 258/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.1631 - val_loss: 1.6836\n",
      "Epoch 259/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.2164 - val_loss: 1.7163\n",
      "Epoch 260/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.1137 - val_loss: 2.4675\n",
      "Epoch 261/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.3370 - val_loss: 1.8431\n",
      "Epoch 262/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.3012 - val_loss: 1.5799\n",
      "Epoch 263/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.3091 - val_loss: 1.4217\n",
      "Epoch 264/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.2987 - val_loss: 2.3484\n",
      "Epoch 265/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.2436 - val_loss: 1.3396\n",
      "Epoch 266/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.1977 - val_loss: 1.3822\n",
      "Epoch 267/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.1183 - val_loss: 1.2198\n",
      "Epoch 268/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.1299 - val_loss: 1.5538\n",
      "Epoch 269/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.1871 - val_loss: 1.6903\n",
      "Epoch 270/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.1024 - val_loss: 1.7270\n",
      "Epoch 271/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.2501 - val_loss: 1.3029\n",
      "Epoch 272/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.2803 - val_loss: 1.5201\n",
      "Epoch 273/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.0470 - val_loss: 1.3688\n",
      "Epoch 274/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.0877 - val_loss: 1.2400\n",
      "Epoch 275/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.0099 - val_loss: 1.8912\n",
      "Epoch 276/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.0681 - val_loss: 1.3461\n",
      "Epoch 277/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.1884 - val_loss: 1.6097\n",
      "Epoch 278/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.0899 - val_loss: 1.6379\n",
      "Epoch 279/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.0833 - val_loss: 1.5479\n",
      "Epoch 280/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.0634 - val_loss: 1.2407\n",
      "Epoch 281/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.0462 - val_loss: 1.2662\n",
      "Epoch 282/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.1010 - val_loss: 1.2916\n",
      "Epoch 283/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.0627 - val_loss: 1.6379\n",
      "Epoch 284/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.1177 - val_loss: 1.3955\n",
      "Epoch 285/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.3255 - val_loss: 1.6096\n",
      "Epoch 286/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.0364 - val_loss: 1.3322\n",
      "Epoch 287/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.0517 - val_loss: 1.3764\n",
      "Epoch 288/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.0090 - val_loss: 1.2582\n",
      "Epoch 289/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.1102 - val_loss: 1.3877\n",
      "Epoch 290/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.0150 - val_loss: 1.4111\n",
      "Epoch 291/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.8950 - val_loss: 1.8133\n",
      "Epoch 292/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.9050 - val_loss: 1.3117\n",
      "Epoch 293/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.0777 - val_loss: 1.3850\n",
      "Epoch 294/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.1964 - val_loss: 1.2194\n",
      "Epoch 295/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.9749 - val_loss: 1.3309\n",
      "Epoch 296/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.0792 - val_loss: 1.5191\n",
      "Epoch 297/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.1532 - val_loss: 1.2905\n",
      "Epoch 298/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.1697 - val_loss: 1.6947\n",
      "Epoch 299/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.0205 - val_loss: 1.4353\n",
      "Epoch 300/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.9957 - val_loss: 1.8168\n",
      "Epoch 301/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.0763 - val_loss: 2.0405\n",
      "Epoch 302/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.4562 - val_loss: 1.4480\n",
      "Epoch 303/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.0050 - val_loss: 1.5633\n",
      "Epoch 304/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.1161 - val_loss: 1.8065\n",
      "Epoch 305/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.0891 - val_loss: 1.5069\n",
      "Epoch 306/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.0561 - val_loss: 1.3504\n",
      "Epoch 307/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.9730 - val_loss: 1.3546\n",
      "Epoch 308/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.9336 - val_loss: 1.3976\n",
      "Epoch 309/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.8443 - val_loss: 1.3192\n",
      "Epoch 310/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.8370 - val_loss: 1.2863\n",
      "Epoch 311/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.1465 - val_loss: 1.4773\n",
      "Epoch 312/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.8626 - val_loss: 1.3180\n",
      "Epoch 313/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.0721 - val_loss: 1.2616\n",
      "Epoch 314/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.8749 - val_loss: 1.2852\n",
      "Epoch 315/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.8326 - val_loss: 1.3710\n",
      "Epoch 316/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.8935 - val_loss: 1.2911\n",
      "Epoch 317/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.7762 - val_loss: 1.2343\n",
      "Epoch 318/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 1s 12ms/step - loss: 1.8888 - val_loss: 1.2232\n",
      "Epoch 319/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.0651 - val_loss: 1.1646\n",
      "Epoch 320/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.9433 - val_loss: 1.3123\n",
      "Epoch 321/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.9510 - val_loss: 1.3867\n",
      "Epoch 322/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.9836 - val_loss: 1.3881\n",
      "Epoch 323/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.0034 - val_loss: 1.5092\n",
      "Epoch 324/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.0031 - val_loss: 1.4069\n",
      "Epoch 325/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.9283 - val_loss: 1.2111\n",
      "Epoch 326/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.8042 - val_loss: 1.3422\n",
      "Epoch 327/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.6995 - val_loss: 1.2630\n",
      "Epoch 328/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.8687 - val_loss: 1.9411\n",
      "Epoch 329/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.0327 - val_loss: 1.8259\n",
      "Epoch 330/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.1283 - val_loss: 1.4601\n",
      "Epoch 331/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.0365 - val_loss: 1.1732\n",
      "Epoch 332/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.8469 - val_loss: 1.4794\n",
      "Epoch 333/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.9715 - val_loss: 1.5244\n",
      "Epoch 334/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.8358 - val_loss: 1.3017\n",
      "Epoch 335/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.7962 - val_loss: 1.3161\n",
      "Epoch 336/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.9609 - val_loss: 1.3948\n",
      "Epoch 337/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.6996 - val_loss: 1.4432\n",
      "Epoch 338/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.6802 - val_loss: 1.2345\n",
      "Epoch 339/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.8028 - val_loss: 1.4498\n",
      "Epoch 340/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.7469 - val_loss: 1.6110\n",
      "Epoch 341/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.9297 - val_loss: 1.3156\n",
      "Epoch 342/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.8980 - val_loss: 1.2057\n",
      "Epoch 343/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.8048 - val_loss: 1.2701\n",
      "Epoch 344/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.7813 - val_loss: 1.2863\n",
      "Epoch 345/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.9157 - val_loss: 1.3039\n",
      "Epoch 346/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.8577 - val_loss: 1.2613\n",
      "Epoch 347/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.9060 - val_loss: 1.2636\n",
      "Epoch 348/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.8789 - val_loss: 1.2505\n",
      "Epoch 349/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.7055 - val_loss: 1.5020\n",
      "Epoch 350/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.7297 - val_loss: 1.2466\n",
      "Epoch 351/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.7722 - val_loss: 1.1835\n",
      "Epoch 352/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.6833 - val_loss: 1.3557\n",
      "Epoch 353/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.8324 - val_loss: 1.6680\n",
      "Epoch 354/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.8405 - val_loss: 1.2532\n",
      "Epoch 355/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.8598 - val_loss: 1.4053\n",
      "Epoch 356/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.8735 - val_loss: 1.4738\n",
      "Epoch 357/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.6999 - val_loss: 1.2532\n",
      "Epoch 358/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.8522 - val_loss: 1.2348\n",
      "Epoch 359/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.7742 - val_loss: 1.3942\n",
      "Epoch 360/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.7415 - val_loss: 1.2405\n",
      "Epoch 361/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.7931 - val_loss: 1.3168\n",
      "Epoch 362/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.8648 - val_loss: 1.3177\n",
      "Epoch 363/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.6899 - val_loss: 1.4012\n",
      "Epoch 364/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.7751 - val_loss: 1.8032\n",
      "Epoch 365/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.8028 - val_loss: 1.4543\n",
      "Epoch 366/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.7480 - val_loss: 1.2547\n",
      "Epoch 367/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.5962 - val_loss: 1.2141\n",
      "Epoch 368/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.7296 - val_loss: 1.1349\n",
      "Epoch 369/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.5719 - val_loss: 1.4365\n",
      "Epoch 370/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.6502 - val_loss: 1.2495\n",
      "Epoch 371/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.6193 - val_loss: 1.2349\n",
      "Epoch 372/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.6812 - val_loss: 1.2021\n",
      "Epoch 373/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.7728 - val_loss: 1.4984\n",
      "Epoch 374/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.8181 - val_loss: 1.2892\n",
      "Epoch 375/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.7033 - val_loss: 1.3174\n",
      "Epoch 376/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.7170 - val_loss: 1.4645\n",
      "Epoch 377/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.7231 - val_loss: 1.2451\n",
      "Epoch 378/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.7343 - val_loss: 1.1790\n",
      "Epoch 379/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.5958 - val_loss: 1.6247\n",
      "Epoch 380/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.7371 - val_loss: 1.4641\n",
      "Epoch 381/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.5729 - val_loss: 1.1916\n",
      "Epoch 382/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.7162 - val_loss: 1.2366\n",
      "Epoch 383/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.7242 - val_loss: 1.2913\n",
      "Epoch 384/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.6930 - val_loss: 1.2582\n",
      "Epoch 385/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.6414 - val_loss: 1.1730\n",
      "Epoch 386/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.8130 - val_loss: 1.4517\n",
      "Epoch 387/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.5985 - val_loss: 1.1496\n",
      "Epoch 388/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.8934 - val_loss: 1.4526\n",
      "Epoch 389/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.6914 - val_loss: 1.3015\n",
      "Epoch 390/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.7044 - val_loss: 1.2017\n",
      "Epoch 391/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.8453 - val_loss: 1.2155\n",
      "Epoch 392/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.6202 - val_loss: 1.1820\n",
      "Epoch 393/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.5492 - val_loss: 1.3103\n",
      "Epoch 394/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.6908 - val_loss: 1.4125\n",
      "Epoch 395/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.7889 - val_loss: 1.7104\n",
      "Epoch 396/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.6220 - val_loss: 1.4177\n",
      "Epoch 397/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.6985 - val_loss: 1.1902\n",
      "Epoch 398/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.5904 - val_loss: 1.3981\n",
      "Epoch 399/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.6073 - val_loss: 1.2819\n",
      "Epoch 400/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.5200 - val_loss: 1.1998\n",
      "Epoch 401/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.6707 - val_loss: 1.2538\n",
      "Epoch 402/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.6269 - val_loss: 1.2547\n",
      "Epoch 403/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.5634 - val_loss: 1.3355\n",
      "Epoch 404/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.5278 - val_loss: 1.1768\n",
      "Epoch 405/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.6297 - val_loss: 1.2900\n",
      "Epoch 406/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.6873 - val_loss: 1.2959\n",
      "Epoch 407/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.5571 - val_loss: 1.1504\n",
      "Epoch 408/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.5384 - val_loss: 1.4155\n",
      "Epoch 409/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.6133 - val_loss: 1.2269\n",
      "Epoch 410/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.6036 - val_loss: 1.2697\n",
      "Epoch 411/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.6735 - val_loss: 1.3508\n",
      "Epoch 412/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.5774 - val_loss: 1.4369\n",
      "Epoch 413/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.5478 - val_loss: 1.0858\n",
      "Epoch 414/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.5128 - val_loss: 1.1672\n",
      "Epoch 415/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.5446 - val_loss: 1.1933\n",
      "Epoch 416/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.5297 - val_loss: 1.2748\n",
      "Epoch 417/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.4766 - val_loss: 1.1467\n",
      "Epoch 418/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.5941 - val_loss: 1.6286\n",
      "Epoch 419/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.5744 - val_loss: 1.1185\n",
      "Epoch 420/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.4777 - val_loss: 1.2384\n",
      "Epoch 421/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.5280 - val_loss: 1.1993\n",
      "Epoch 422/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.4908 - val_loss: 1.6669\n",
      "Epoch 423/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.5428 - val_loss: 1.1941\n",
      "Epoch 424/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.5271 - val_loss: 1.2990\n",
      "Epoch 425/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.4188 - val_loss: 1.3177\n",
      "Epoch 426/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.6834 - val_loss: 1.1732\n",
      "Epoch 427/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.5782 - val_loss: 1.5314\n",
      "Epoch 428/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.6255 - val_loss: 1.2260\n",
      "Epoch 429/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.5138 - val_loss: 1.3357\n",
      "Epoch 430/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.5064 - val_loss: 1.0998\n",
      "Epoch 431/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.4752 - val_loss: 1.0826\n",
      "Epoch 432/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.7739 - val_loss: 1.3557\n",
      "Epoch 433/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.5466 - val_loss: 1.1514\n",
      "Epoch 434/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.4920 - val_loss: 1.3030\n",
      "Epoch 435/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.5465 - val_loss: 1.2096\n",
      "Epoch 436/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.5334 - val_loss: 1.0861\n",
      "Epoch 437/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.4361 - val_loss: 1.2055\n",
      "Epoch 438/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.4548 - val_loss: 1.1886\n",
      "Epoch 439/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.7106 - val_loss: 1.3287\n",
      "Epoch 440/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.5385 - val_loss: 1.2227\n",
      "Epoch 441/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.4820 - val_loss: 1.1754\n",
      "Epoch 442/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.4821 - val_loss: 1.2159\n",
      "Epoch 443/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.6181 - val_loss: 1.2264\n",
      "Epoch 444/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.6042 - val_loss: 1.1931\n",
      "Epoch 445/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.4197 - val_loss: 1.0585\n",
      "Epoch 446/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.4884 - val_loss: 1.2528\n",
      "Epoch 447/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.4482 - val_loss: 1.5910\n",
      "Epoch 448/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.5487 - val_loss: 1.2403\n",
      "Epoch 449/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.5947 - val_loss: 1.2589\n",
      "Epoch 450/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.7309 - val_loss: 1.2464\n",
      "Epoch 451/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.4624 - val_loss: 1.1400\n",
      "Epoch 452/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.4284 - val_loss: 1.1854\n",
      "Epoch 453/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.4725 - val_loss: 1.1501\n",
      "Epoch 454/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.4797 - val_loss: 1.4227\n",
      "Epoch 455/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.5087 - val_loss: 1.2073\n",
      "Epoch 456/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.5733 - val_loss: 1.3468\n",
      "Epoch 457/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.5125 - val_loss: 1.2173\n",
      "Epoch 458/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.4420 - val_loss: 1.1862\n",
      "Epoch 459/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.4185 - val_loss: 1.1779\n",
      "Epoch 460/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.5353 - val_loss: 1.1703\n",
      "Epoch 461/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.3820 - val_loss: 1.3259\n",
      "Epoch 462/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.5044 - val_loss: 1.5033\n",
      "Epoch 463/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.6275 - val_loss: 1.1986\n",
      "Epoch 464/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.5036 - val_loss: 1.1264\n",
      "Epoch 465/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.4507 - val_loss: 1.1302\n",
      "Epoch 466/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.7187 - val_loss: 1.2559\n",
      "Epoch 467/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.6308 - val_loss: 1.4792\n",
      "Epoch 468/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.5147 - val_loss: 1.8486\n",
      "Epoch 469/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.9589 - val_loss: 1.2993\n",
      "Epoch 470/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.4454 - val_loss: 1.2365\n",
      "Epoch 471/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.4909 - val_loss: 1.1868\n",
      "Epoch 472/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.4531 - val_loss: 1.2960\n",
      "Epoch 473/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.4417 - val_loss: 1.1418\n",
      "Epoch 474/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.4285 - val_loss: 1.1427\n",
      "Epoch 475/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.4024 - val_loss: 1.2666\n",
      "Epoch 476/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 1s 12ms/step - loss: 1.4667 - val_loss: 1.0478\n",
      "Epoch 477/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.4919 - val_loss: 1.2610\n",
      "Epoch 478/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.3804 - val_loss: 1.1068\n",
      "Epoch 479/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.4654 - val_loss: 1.1030\n",
      "Epoch 480/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.3357 - val_loss: 1.0933\n",
      "Epoch 481/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.3254 - val_loss: 1.0339\n",
      "Epoch 482/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.4492 - val_loss: 1.2841\n",
      "Epoch 483/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.3301 - val_loss: 1.1873\n",
      "Epoch 484/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.4405 - val_loss: 1.3912\n",
      "Epoch 485/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.5430 - val_loss: 1.1373\n",
      "Epoch 486/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.3986 - val_loss: 1.2969\n",
      "Epoch 487/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.5909 - val_loss: 1.1442\n",
      "Epoch 488/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.3779 - val_loss: 1.1051\n",
      "Epoch 489/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.4493 - val_loss: 1.1317\n",
      "Epoch 490/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.4591 - val_loss: 1.0627\n",
      "Epoch 491/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.3741 - val_loss: 1.4258\n",
      "Epoch 492/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.3704 - val_loss: 1.1520\n",
      "Epoch 493/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.4136 - val_loss: 1.2684\n",
      "Epoch 494/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.4722 - val_loss: 1.2664\n",
      "Epoch 495/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.2983 - val_loss: 1.0650\n",
      "Epoch 496/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.2950 - val_loss: 1.6546\n",
      "Epoch 497/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.6095 - val_loss: 1.1216\n",
      "Epoch 498/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.3212 - val_loss: 1.3023\n",
      "Epoch 499/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.6427 - val_loss: 1.1975\n",
      "Epoch 500/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.3518 - val_loss: 1.0870\n",
      "Epoch 501/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.3295 - val_loss: 1.2370\n",
      "Epoch 502/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.3855 - val_loss: 1.0803\n",
      "Epoch 503/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.4065 - val_loss: 1.2022\n",
      "Epoch 504/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.3186 - val_loss: 1.3370\n",
      "Epoch 505/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.3310 - val_loss: 1.1219\n",
      "Epoch 506/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.3356 - val_loss: 1.2896\n",
      "Epoch 507/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.3725 - val_loss: 1.2114\n",
      "Epoch 508/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.2758 - val_loss: 1.0638\n",
      "Epoch 509/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.2476 - val_loss: 1.0561\n",
      "Epoch 510/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.3222 - val_loss: 1.2719\n",
      "Epoch 511/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.3365 - val_loss: 1.2693\n",
      "Epoch 512/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.2376 - val_loss: 1.1309\n",
      "Epoch 513/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.3675 - val_loss: 1.1607\n",
      "Epoch 514/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.4965 - val_loss: 1.3099\n",
      "Epoch 515/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.3310 - val_loss: 1.2954\n",
      "Epoch 516/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.3439 - val_loss: 1.1551\n",
      "Epoch 517/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.4325 - val_loss: 1.2143\n",
      "Epoch 518/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.4495 - val_loss: 1.2870\n",
      "Epoch 519/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.3888 - val_loss: 1.3622\n",
      "Epoch 520/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.2871 - val_loss: 1.1324\n",
      "Epoch 521/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.3146 - val_loss: 1.1820\n",
      "Epoch 522/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.2596 - val_loss: 1.4365\n",
      "Epoch 523/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.3024 - val_loss: 1.0950\n",
      "Epoch 524/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.2168 - val_loss: 1.2737\n",
      "Epoch 525/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.3272 - val_loss: 1.2843\n",
      "Epoch 526/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.3644 - val_loss: 1.1491\n",
      "Epoch 527/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.3687 - val_loss: 1.1301\n",
      "Epoch 528/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.3222 - val_loss: 1.1427\n",
      "Epoch 529/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.2810 - val_loss: 1.2332\n",
      "Epoch 530/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.4478 - val_loss: 1.2141\n",
      "Epoch 531/1000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.3652 - val_loss: 1.1664\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1.1664\n",
      "MSE: 1.166367769241333\n"
     ]
    }
   ],
   "source": [
    "model = Sequential() \n",
    "model.add(LSTM(256, return_sequences=True, input_shape=(X_train.shape[1], 1))) \n",
    "model.add(Dropout(0.2)) \n",
    "model.add(LSTM(256, return_sequences=True)) \n",
    "model.add(Dropout(0.2)) \n",
    "model.add(LSTM(128)) \n",
    "model.add(BatchNormalization()) \n",
    "\n",
    "model.add(Dense(64, activation='relu')) \n",
    "model.add(Dense(32, activation='relu')) \n",
    "model.add(Dense(3)) \n",
    "\n",
    "adam = Adam(lr=0.0001) \n",
    "# Or RMSprop(lr=0.0001) \n",
    "model.compile(loss='huber_loss', optimizer=adam) \n",
    "\n",
    "early_stopping = EarlyStopping(patience=50) \n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping]) \n",
    "\n",
    "mse = model.evaluate(X_test, y_test) \n",
    "print(\"MSE:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab69f9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960ae88f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "276be936",
   "metadata": {},
   "source": [
    "# Modified "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f14680c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "32b6be47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n",
      "8/8 [==============================] - 6s 272ms/step - loss: 1682.3567 - val_loss: 1703.6434\n",
      "Epoch 2/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 880.9927 - val_loss: 1424.7252\n",
      "Epoch 3/3000\n",
      "8/8 [==============================] - 1s 116ms/step - loss: 351.2012 - val_loss: 1338.7628\n",
      "Epoch 4/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 211.1852 - val_loss: 1182.8727\n",
      "Epoch 5/3000\n",
      "8/8 [==============================] - 1s 117ms/step - loss: 178.0515 - val_loss: 975.7734\n",
      "Epoch 6/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 153.1810 - val_loss: 862.4764\n",
      "Epoch 7/3000\n",
      "8/8 [==============================] - 1s 117ms/step - loss: 157.8810 - val_loss: 892.1490\n",
      "Epoch 8/3000\n",
      "8/8 [==============================] - 1s 116ms/step - loss: 109.4246 - val_loss: 870.0754\n",
      "Epoch 9/3000\n",
      "8/8 [==============================] - 1s 117ms/step - loss: 140.9859 - val_loss: 742.7391\n",
      "Epoch 10/3000\n",
      "8/8 [==============================] - 1s 117ms/step - loss: 112.3584 - val_loss: 692.1951\n",
      "Epoch 11/3000\n",
      "8/8 [==============================] - 1s 117ms/step - loss: 121.2537 - val_loss: 692.7830\n",
      "Epoch 12/3000\n",
      "8/8 [==============================] - 1s 116ms/step - loss: 91.1344 - val_loss: 558.0032\n",
      "Epoch 13/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 89.9885 - val_loss: 725.9174\n",
      "Epoch 14/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 79.5127 - val_loss: 862.3626\n",
      "Epoch 15/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 84.6311 - val_loss: 497.8584\n",
      "Epoch 16/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 148.2091 - val_loss: 500.1044\n",
      "Epoch 17/3000\n",
      "8/8 [==============================] - 1s 117ms/step - loss: 48.1796 - val_loss: 629.8615\n",
      "Epoch 18/3000\n",
      "8/8 [==============================] - 1s 116ms/step - loss: 53.2522 - val_loss: 451.5063\n",
      "Epoch 19/3000\n",
      "8/8 [==============================] - 1s 117ms/step - loss: 48.1031 - val_loss: 531.0038\n",
      "Epoch 20/3000\n",
      "8/8 [==============================] - 1s 117ms/step - loss: 67.3486 - val_loss: 442.2842\n",
      "Epoch 21/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 51.1062 - val_loss: 555.1296\n",
      "Epoch 22/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 42.7112 - val_loss: 710.8427\n",
      "Epoch 23/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 47.3542 - val_loss: 785.1946\n",
      "Epoch 24/3000\n",
      "8/8 [==============================] - 1s 117ms/step - loss: 61.1920 - val_loss: 314.2167\n",
      "Epoch 25/3000\n",
      "8/8 [==============================] - 1s 117ms/step - loss: 42.5391 - val_loss: 491.9337\n",
      "Epoch 26/3000\n",
      "8/8 [==============================] - 1s 117ms/step - loss: 37.2029 - val_loss: 229.0483\n",
      "Epoch 27/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 91.5013 - val_loss: 236.7285\n",
      "Epoch 28/3000\n",
      "8/8 [==============================] - 1s 117ms/step - loss: 41.6256 - val_loss: 169.7326\n",
      "Epoch 29/3000\n",
      "8/8 [==============================] - 1s 117ms/step - loss: 26.5701 - val_loss: 223.9242\n",
      "Epoch 30/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 29.4413 - val_loss: 145.9058\n",
      "Epoch 31/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 41.5694 - val_loss: 195.3850\n",
      "Epoch 32/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 30.5251 - val_loss: 157.3104\n",
      "Epoch 33/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 35.1742 - val_loss: 162.1253\n",
      "Epoch 34/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 32.3497 - val_loss: 85.0309\n",
      "Epoch 35/3000\n",
      "8/8 [==============================] - 1s 117ms/step - loss: 36.4884 - val_loss: 192.2157\n",
      "Epoch 36/3000\n",
      "8/8 [==============================] - 1s 117ms/step - loss: 41.1647 - val_loss: 108.2415\n",
      "Epoch 37/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 32.7859 - val_loss: 124.8717\n",
      "Epoch 38/3000\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 29.1279 - val_loss: 116.9535\n",
      "Epoch 39/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 26.0720 - val_loss: 45.8767\n",
      "Epoch 40/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 26.4433 - val_loss: 148.7359\n",
      "Epoch 41/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 25.8040 - val_loss: 177.2807\n",
      "Epoch 42/3000\n",
      "8/8 [==============================] - 1s 134ms/step - loss: 30.6566 - val_loss: 73.4057\n",
      "Epoch 43/3000\n",
      "8/8 [==============================] - 1s 138ms/step - loss: 41.8560 - val_loss: 93.7584\n",
      "Epoch 44/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 18.0497 - val_loss: 142.4561\n",
      "Epoch 45/3000\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 35.0324 - val_loss: 90.8341\n",
      "Epoch 46/3000\n",
      "8/8 [==============================] - 1s 126ms/step - loss: 17.8299 - val_loss: 37.0488\n",
      "Epoch 47/3000\n",
      "8/8 [==============================] - 1s 128ms/step - loss: 24.7059 - val_loss: 159.6061\n",
      "Epoch 48/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 39.2142 - val_loss: 227.2985\n",
      "Epoch 49/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 28.1360 - val_loss: 161.3602\n",
      "Epoch 50/3000\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 25.3135 - val_loss: 387.5794\n",
      "Epoch 51/3000\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 21.8138 - val_loss: 83.2565\n",
      "Epoch 52/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 21.2576 - val_loss: 32.0036\n",
      "Epoch 53/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 24.5477 - val_loss: 23.4535\n",
      "Epoch 54/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 21.0127 - val_loss: 65.4529\n",
      "Epoch 55/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 24.7537 - val_loss: 116.0354\n",
      "Epoch 56/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 19.2616 - val_loss: 442.6038\n",
      "Epoch 57/3000\n",
      "8/8 [==============================] - 1s 117ms/step - loss: 20.0394 - val_loss: 54.2485\n",
      "Epoch 58/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 22.6451 - val_loss: 353.7331\n",
      "Epoch 59/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 36.5120 - val_loss: 105.8688\n",
      "Epoch 60/3000\n",
      "8/8 [==============================] - 1s 117ms/step - loss: 15.8812 - val_loss: 157.2550\n",
      "Epoch 61/3000\n",
      "8/8 [==============================] - 1s 117ms/step - loss: 17.1852 - val_loss: 98.8742\n",
      "Epoch 62/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 20.3654 - val_loss: 57.0800\n",
      "Epoch 63/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 19.5310 - val_loss: 58.9431\n",
      "Epoch 64/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 18.0650 - val_loss: 191.5568\n",
      "Epoch 65/3000\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 24.1327 - val_loss: 46.2299\n",
      "Epoch 66/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 20.4528 - val_loss: 56.2040\n",
      "Epoch 67/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 19.0341 - val_loss: 37.6774\n",
      "Epoch 68/3000\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 20.2750 - val_loss: 402.8328\n",
      "Epoch 69/3000\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 27.2460 - val_loss: 31.8520\n",
      "Epoch 70/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 16.4879 - val_loss: 104.6142\n",
      "Epoch 71/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 20.9942 - val_loss: 369.9691\n",
      "Epoch 72/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 26.8884 - val_loss: 251.8880\n",
      "Epoch 73/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 19.8261 - val_loss: 27.6950\n",
      "Epoch 74/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 19.7222 - val_loss: 135.5888\n",
      "Epoch 75/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 14.4469 - val_loss: 51.0117\n",
      "Epoch 76/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 19.3803 - val_loss: 83.2918\n",
      "Epoch 77/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 20.5864 - val_loss: 350.5029\n",
      "Epoch 78/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 39.7164 - val_loss: 85.2015\n",
      "Epoch 79/3000\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 10.3160 - val_loss: 41.4343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 13.6883 - val_loss: 20.1809\n",
      "Epoch 81/3000\n",
      "8/8 [==============================] - 1s 128ms/step - loss: 20.8918 - val_loss: 57.4177\n",
      "Epoch 82/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 14.1428 - val_loss: 167.2418\n",
      "Epoch 83/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 14.6244 - val_loss: 32.8069\n",
      "Epoch 84/3000\n",
      "8/8 [==============================] - 1s 117ms/step - loss: 23.5319 - val_loss: 79.8751\n",
      "Epoch 85/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 18.8579 - val_loss: 21.1324\n",
      "Epoch 86/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 21.4569 - val_loss: 16.5124\n",
      "Epoch 87/3000\n",
      "8/8 [==============================] - 1s 117ms/step - loss: 18.2319 - val_loss: 57.1362\n",
      "Epoch 88/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 19.1295 - val_loss: 15.0293\n",
      "Epoch 89/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 17.6256 - val_loss: 360.1183\n",
      "Epoch 90/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 19.1131 - val_loss: 74.4063\n",
      "Epoch 91/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 13.3210 - val_loss: 44.1326\n",
      "Epoch 92/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 19.9827 - val_loss: 74.3712\n",
      "Epoch 93/3000\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 17.3436 - val_loss: 56.5148\n",
      "Epoch 94/3000\n",
      "8/8 [==============================] - 1s 126ms/step - loss: 13.9754 - val_loss: 121.5303\n",
      "Epoch 95/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 19.0666 - val_loss: 156.2981\n",
      "Epoch 96/3000\n",
      "8/8 [==============================] - 1s 126ms/step - loss: 16.2124 - val_loss: 64.6936\n",
      "Epoch 97/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 12.4723 - val_loss: 110.5297\n",
      "Epoch 98/3000\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 15.5559 - val_loss: 45.5141\n",
      "Epoch 99/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 12.3784 - val_loss: 38.2769\n",
      "Epoch 100/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 18.1707 - val_loss: 73.6848\n",
      "Epoch 101/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 17.8061 - val_loss: 56.4399\n",
      "Epoch 102/3000\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 12.5812 - val_loss: 134.5626\n",
      "Epoch 103/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 13.3307 - val_loss: 153.3139\n",
      "Epoch 104/3000\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 21.6000 - val_loss: 124.9672\n",
      "Epoch 105/3000\n",
      "8/8 [==============================] - 1s 134ms/step - loss: 14.2776 - val_loss: 55.3792\n",
      "Epoch 106/3000\n",
      "8/8 [==============================] - 1s 128ms/step - loss: 15.5189 - val_loss: 55.1766\n",
      "Epoch 107/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 17.0848 - val_loss: 48.0915\n",
      "Epoch 108/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 16.9452 - val_loss: 16.9410\n",
      "Epoch 109/3000\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 13.0966 - val_loss: 284.9120\n",
      "Epoch 110/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 13.5485 - val_loss: 17.0698\n",
      "Epoch 111/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 18.2249 - val_loss: 15.6981\n",
      "Epoch 112/3000\n",
      "8/8 [==============================] - 1s 130ms/step - loss: 14.0552 - val_loss: 256.9072\n",
      "Epoch 113/3000\n",
      "8/8 [==============================] - 1s 126ms/step - loss: 15.3628 - val_loss: 14.9192\n",
      "Epoch 114/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 14.5307 - val_loss: 18.2616\n",
      "Epoch 115/3000\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 11.7996 - val_loss: 108.4041\n",
      "Epoch 116/3000\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 18.6942 - val_loss: 71.8502\n",
      "Epoch 117/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 15.4669 - val_loss: 78.2656\n",
      "Epoch 118/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 12.4806 - val_loss: 51.0273\n",
      "Epoch 119/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 16.0044 - val_loss: 19.8060\n",
      "Epoch 120/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 12.4374 - val_loss: 42.8732\n",
      "Epoch 121/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 14.2860 - val_loss: 88.0800\n",
      "Epoch 122/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 15.7134 - val_loss: 75.6442\n",
      "Epoch 123/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 10.9885 - val_loss: 98.1505\n",
      "Epoch 124/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 16.3991 - val_loss: 31.7756\n",
      "Epoch 125/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 15.5011 - val_loss: 20.0226\n",
      "Epoch 126/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 10.7252 - val_loss: 43.1165\n",
      "Epoch 127/3000\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 15.9006 - val_loss: 27.3845\n",
      "Epoch 128/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 15.4179 - val_loss: 170.9425\n",
      "Epoch 129/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 14.0946 - val_loss: 19.1940\n",
      "Epoch 130/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 10.2607 - val_loss: 18.0155\n",
      "Epoch 131/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 15.2823 - val_loss: 61.7954\n",
      "Epoch 132/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 14.4015 - val_loss: 25.6880\n",
      "Epoch 133/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 15.1157 - val_loss: 88.4525\n",
      "Epoch 134/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 13.0916 - val_loss: 67.3676\n",
      "Epoch 135/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 14.9610 - val_loss: 51.4992\n",
      "Epoch 136/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 11.2861 - val_loss: 13.9450\n",
      "Epoch 137/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 11.5862 - val_loss: 212.8404\n",
      "Epoch 138/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 11.1114 - val_loss: 57.8640\n",
      "Epoch 139/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 13.2144 - val_loss: 98.2374\n",
      "Epoch 140/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 11.6731 - val_loss: 28.6923\n",
      "Epoch 141/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 13.8050 - val_loss: 112.0854\n",
      "Epoch 142/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 13.5370 - val_loss: 284.3130\n",
      "Epoch 143/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 15.5345 - val_loss: 62.4013\n",
      "Epoch 144/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 11.6967 - val_loss: 325.2882\n",
      "Epoch 145/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 10.0257 - val_loss: 40.2723\n",
      "Epoch 146/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 10.8980 - val_loss: 104.4977\n",
      "Epoch 147/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 11.9400 - val_loss: 14.8009\n",
      "Epoch 148/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 19.9129 - val_loss: 100.5173\n",
      "Epoch 149/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 11.6810 - val_loss: 15.3538\n",
      "Epoch 150/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 9.4541 - val_loss: 35.1649\n",
      "Epoch 151/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 15.8291 - val_loss: 65.3096\n",
      "Epoch 152/3000\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 11.3171 - val_loss: 157.2871\n",
      "Epoch 153/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 8.5537 - val_loss: 93.3633\n",
      "Epoch 154/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 12.1967 - val_loss: 42.7246\n",
      "Epoch 155/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 9.1552 - val_loss: 55.3554\n",
      "Epoch 156/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 10.7434 - val_loss: 48.2983\n",
      "Epoch 157/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 13.0748 - val_loss: 40.9617\n",
      "Epoch 158/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 15.1803 - val_loss: 19.0693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 159/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 9.7366 - val_loss: 41.6417\n",
      "Epoch 160/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 9.2232 - val_loss: 46.0600\n",
      "Epoch 161/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 13.7114 - val_loss: 19.9180\n",
      "Epoch 162/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 14.0857 - val_loss: 34.9036\n",
      "Epoch 163/3000\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 11.5468 - val_loss: 24.8091\n",
      "Epoch 164/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 12.3099 - val_loss: 253.6599\n",
      "Epoch 165/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 11.4179 - val_loss: 62.1537\n",
      "Epoch 166/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 9.1356 - val_loss: 48.2311\n",
      "Epoch 167/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 13.8271 - val_loss: 51.0693\n",
      "Epoch 168/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 9.9593 - val_loss: 40.8990\n",
      "Epoch 169/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 8.9194 - val_loss: 132.5928\n",
      "Epoch 170/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 13.5266 - val_loss: 22.9820\n",
      "Epoch 171/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 11.7838 - val_loss: 94.1142\n",
      "Epoch 172/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 8.9733 - val_loss: 28.1319\n",
      "Epoch 173/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 10.4457 - val_loss: 31.7985\n",
      "Epoch 174/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 14.3121 - val_loss: 32.9364\n",
      "Epoch 175/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 9.7726 - val_loss: 22.4919\n",
      "Epoch 176/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 12.8153 - val_loss: 33.5716\n",
      "Epoch 177/3000\n",
      "8/8 [==============================] - 1s 117ms/step - loss: 13.6710 - val_loss: 16.6645\n",
      "Epoch 178/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 11.9867 - val_loss: 28.7754\n",
      "Epoch 179/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 13.9205 - val_loss: 33.3976\n",
      "Epoch 180/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 13.0225 - val_loss: 8.4756\n",
      "Epoch 181/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 10.2772 - val_loss: 46.6570\n",
      "Epoch 182/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 8.7741 - val_loss: 29.7729\n",
      "Epoch 183/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 14.9841 - val_loss: 16.3195\n",
      "Epoch 184/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 9.9234 - val_loss: 44.2640\n",
      "Epoch 185/3000\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 10.4064 - val_loss: 33.2897\n",
      "Epoch 186/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 12.5352 - val_loss: 37.3969\n",
      "Epoch 187/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 8.4134 - val_loss: 15.9872\n",
      "Epoch 188/3000\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 13.3950 - val_loss: 118.7043\n",
      "Epoch 189/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 13.6269 - val_loss: 41.1826\n",
      "Epoch 190/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 8.5774 - val_loss: 11.7727\n",
      "Epoch 191/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 10.0313 - val_loss: 65.8958\n",
      "Epoch 192/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 15.2541 - val_loss: 8.4412\n",
      "Epoch 193/3000\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 12.3671 - val_loss: 9.8087\n",
      "Epoch 194/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 12.7856 - val_loss: 10.3239\n",
      "Epoch 195/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 6.6095 - val_loss: 11.0145\n",
      "Epoch 196/3000\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 12.8027 - val_loss: 17.6392\n",
      "Epoch 197/3000\n",
      "8/8 [==============================] - 1s 117ms/step - loss: 15.4746 - val_loss: 27.2614\n",
      "Epoch 198/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 11.7923 - val_loss: 19.4604\n",
      "Epoch 199/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 8.5533 - val_loss: 12.7195\n",
      "Epoch 200/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 10.4130 - val_loss: 23.6440\n",
      "Epoch 201/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 10.4250 - val_loss: 61.5387\n",
      "Epoch 202/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 10.7351 - val_loss: 27.7334\n",
      "Epoch 203/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 11.2080 - val_loss: 61.2640\n",
      "Epoch 204/3000\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 8.9883 - val_loss: 11.2245\n",
      "Epoch 205/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 9.9209 - val_loss: 21.5812\n",
      "Epoch 206/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 8.1922 - val_loss: 26.7294\n",
      "Epoch 207/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 9.1848 - val_loss: 38.5817\n",
      "Epoch 208/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 10.5572 - val_loss: 33.1020\n",
      "Epoch 209/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 9.0777 - val_loss: 28.9822\n",
      "Epoch 210/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 9.5507 - val_loss: 45.4039\n",
      "Epoch 211/3000\n",
      "8/8 [==============================] - 1s 117ms/step - loss: 7.4377 - val_loss: 8.2742\n",
      "Epoch 212/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 10.1837 - val_loss: 10.6204\n",
      "Epoch 213/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 10.4958 - val_loss: 32.5545\n",
      "Epoch 214/3000\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 10.5157 - val_loss: 81.9139\n",
      "Epoch 215/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 10.4680 - val_loss: 18.1178\n",
      "Epoch 216/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 6.5754 - val_loss: 18.0449\n",
      "Epoch 217/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 8.0582 - val_loss: 84.8139\n",
      "Epoch 218/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 12.5006 - val_loss: 11.0580\n",
      "Epoch 219/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 8.9804 - val_loss: 19.5717\n",
      "Epoch 220/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 8.3179 - val_loss: 28.6505\n",
      "Epoch 221/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 9.9347 - val_loss: 69.2054\n",
      "Epoch 222/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 9.9985 - val_loss: 7.7500\n",
      "Epoch 223/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 10.2297 - val_loss: 9.4071\n",
      "Epoch 224/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 9.1026 - val_loss: 41.7364\n",
      "Epoch 225/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 10.3203 - val_loss: 24.0577\n",
      "Epoch 226/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 11.0753 - val_loss: 13.2058\n",
      "Epoch 227/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 9.4926 - val_loss: 33.7869\n",
      "Epoch 228/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 7.9042 - val_loss: 88.4067\n",
      "Epoch 229/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 14.6120 - val_loss: 12.4210\n",
      "Epoch 230/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 7.0685 - val_loss: 20.5289\n",
      "Epoch 231/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 14.3756 - val_loss: 75.6943\n",
      "Epoch 232/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 10.2599 - val_loss: 8.4350\n",
      "Epoch 233/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 9.5672 - val_loss: 43.6256\n",
      "Epoch 234/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 8.8926 - val_loss: 7.8861\n",
      "Epoch 235/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 8.0615 - val_loss: 11.0936\n",
      "Epoch 236/3000\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 7.3642 - val_loss: 16.4818\n",
      "Epoch 237/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 9.6345 - val_loss: 46.3309\n",
      "Epoch 238/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 123ms/step - loss: 11.4484 - val_loss: 33.4268\n",
      "Epoch 239/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 8.6464 - val_loss: 22.2150\n",
      "Epoch 240/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 8.4557 - val_loss: 26.7543\n",
      "Epoch 241/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 6.9353 - val_loss: 10.8277\n",
      "Epoch 242/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 16.2415 - val_loss: 91.0359\n",
      "Epoch 243/3000\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 10.2366 - val_loss: 31.6037\n",
      "Epoch 244/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 11.6833 - val_loss: 11.1268\n",
      "Epoch 245/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 6.6679 - val_loss: 14.8769\n",
      "Epoch 246/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 8.4974 - val_loss: 53.5254\n",
      "Epoch 247/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 9.3787 - val_loss: 36.6665\n",
      "Epoch 248/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 9.7483 - val_loss: 34.5626\n",
      "Epoch 249/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 8.4420 - val_loss: 19.3808\n",
      "Epoch 250/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 7.0439 - val_loss: 17.0269\n",
      "Epoch 251/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 8.5105 - val_loss: 41.7835\n",
      "Epoch 252/3000\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 6.8798 - val_loss: 26.6567\n",
      "Epoch 253/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 9.7867 - val_loss: 32.2962\n",
      "Epoch 254/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 15.3581 - val_loss: 10.5615\n",
      "Epoch 255/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 8.4204 - val_loss: 27.6048\n",
      "Epoch 256/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 9.8829 - val_loss: 10.1870\n",
      "Epoch 257/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 7.6929 - val_loss: 18.3563\n",
      "Epoch 258/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 9.7888 - val_loss: 35.3431\n",
      "Epoch 259/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 8.6800 - val_loss: 7.4723\n",
      "Epoch 260/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 6.8503 - val_loss: 24.0993\n",
      "Epoch 261/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 6.6190 - val_loss: 12.6010\n",
      "Epoch 262/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 9.4453 - val_loss: 11.2015\n",
      "Epoch 263/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 6.5219 - val_loss: 35.2369\n",
      "Epoch 264/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 7.4353 - val_loss: 29.4391\n",
      "Epoch 265/3000\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 9.2786 - val_loss: 22.1653\n",
      "Epoch 266/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 8.2193 - val_loss: 81.5597\n",
      "Epoch 267/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 7.7470 - val_loss: 16.5702\n",
      "Epoch 268/3000\n",
      "8/8 [==============================] - 1s 126ms/step - loss: 7.2070 - val_loss: 11.9357\n",
      "Epoch 269/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 7.7816 - val_loss: 11.1066\n",
      "Epoch 270/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 7.4743 - val_loss: 88.3458\n",
      "Epoch 271/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 11.7811 - val_loss: 19.2538\n",
      "Epoch 272/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 8.1773 - val_loss: 22.7382\n",
      "Epoch 273/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 9.3010 - val_loss: 29.5212\n",
      "Epoch 274/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 6.7002 - val_loss: 21.6197\n",
      "Epoch 275/3000\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 8.6726 - val_loss: 12.2688\n",
      "Epoch 276/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 6.9338 - val_loss: 49.2285\n",
      "Epoch 277/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 8.2501 - val_loss: 21.4878\n",
      "Epoch 278/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 10.3009 - val_loss: 9.1663\n",
      "Epoch 279/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 7.6676 - val_loss: 24.5728\n",
      "Epoch 280/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 5.7999 - val_loss: 14.5811\n",
      "Epoch 281/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 6.9776 - val_loss: 15.3038\n",
      "Epoch 282/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 7.8206 - val_loss: 88.1344\n",
      "Epoch 283/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 7.3763 - val_loss: 27.8993\n",
      "Epoch 284/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 7.5448 - val_loss: 79.8106\n",
      "Epoch 285/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 7.4894 - val_loss: 27.6846\n",
      "Epoch 286/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 8.3492 - val_loss: 7.1633\n",
      "Epoch 287/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 6.2529 - val_loss: 6.8077\n",
      "Epoch 288/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 6.1342 - val_loss: 8.5256\n",
      "Epoch 289/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 12.4807 - val_loss: 174.5536\n",
      "Epoch 290/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 5.4890 - val_loss: 7.8540\n",
      "Epoch 291/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 7.2494 - val_loss: 8.1641\n",
      "Epoch 292/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 7.5988 - val_loss: 37.4629\n",
      "Epoch 293/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 6.3196 - val_loss: 19.1134\n",
      "Epoch 294/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 7.5875 - val_loss: 73.0107\n",
      "Epoch 295/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 8.0805 - val_loss: 18.1959\n",
      "Epoch 296/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 5.2461 - val_loss: 13.3433\n",
      "Epoch 297/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 7.2573 - val_loss: 14.3980\n",
      "Epoch 298/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 7.9967 - val_loss: 6.5639\n",
      "Epoch 299/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 6.1824 - val_loss: 15.7024\n",
      "Epoch 300/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 8.2430 - val_loss: 30.6261\n",
      "Epoch 301/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 8.1570 - val_loss: 17.1536\n",
      "Epoch 302/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 6.6142 - val_loss: 7.3776\n",
      "Epoch 303/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 7.3705 - val_loss: 45.8505\n",
      "Epoch 304/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 6.5634 - val_loss: 18.1821\n",
      "Epoch 305/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 12.5267 - val_loss: 13.1377\n",
      "Epoch 306/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 5.6388 - val_loss: 7.6601\n",
      "Epoch 307/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 6.6875 - val_loss: 8.8774\n",
      "Epoch 308/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 5.3970 - val_loss: 10.1449\n",
      "Epoch 309/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 8.9313 - val_loss: 10.0563\n",
      "Epoch 310/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 6.3384 - val_loss: 7.5500\n",
      "Epoch 311/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 5.6903 - val_loss: 10.7384\n",
      "Epoch 312/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 8.6373 - val_loss: 22.9146\n",
      "Epoch 313/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 6.5058 - val_loss: 26.4828\n",
      "Epoch 314/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 8.7416 - val_loss: 6.0903\n",
      "Epoch 315/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 5.8011 - val_loss: 11.2280\n",
      "Epoch 316/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 7.6643 - val_loss: 5.8565\n",
      "Epoch 317/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 120ms/step - loss: 6.7498 - val_loss: 53.8169\n",
      "Epoch 318/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 7.7708 - val_loss: 12.8397\n",
      "Epoch 319/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 5.0117 - val_loss: 21.3557\n",
      "Epoch 320/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 6.3598 - val_loss: 16.1996\n",
      "Epoch 321/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 6.3412 - val_loss: 12.7839\n",
      "Epoch 322/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 7.9280 - val_loss: 5.7243\n",
      "Epoch 323/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 5.6156 - val_loss: 7.2675\n",
      "Epoch 324/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 7.1644 - val_loss: 19.2424\n",
      "Epoch 325/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 6.2003 - val_loss: 27.2774\n",
      "Epoch 326/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 6.7221 - val_loss: 83.1152\n",
      "Epoch 327/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 8.9039 - val_loss: 9.1229\n",
      "Epoch 328/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 6.5214 - val_loss: 11.6295\n",
      "Epoch 329/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 7.0777 - val_loss: 24.8694\n",
      "Epoch 330/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 6.2607 - val_loss: 12.6015\n",
      "Epoch 331/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 6.2039 - val_loss: 15.2350\n",
      "Epoch 332/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 6.4161 - val_loss: 7.4977\n",
      "Epoch 333/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 7.1262 - val_loss: 55.5025\n",
      "Epoch 334/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 8.5715 - val_loss: 68.9513\n",
      "Epoch 335/3000\n",
      "8/8 [==============================] - 1s 126ms/step - loss: 6.5829 - val_loss: 8.3493\n",
      "Epoch 336/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 6.8658 - val_loss: 19.3746\n",
      "Epoch 337/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 7.3614 - val_loss: 6.1949\n",
      "Epoch 338/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 5.9777 - val_loss: 41.7932\n",
      "Epoch 339/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 6.9297 - val_loss: 27.1901\n",
      "Epoch 340/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 6.2132 - val_loss: 19.3984\n",
      "Epoch 341/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 6.0379 - val_loss: 26.2887\n",
      "Epoch 342/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 6.7378 - val_loss: 8.9018\n",
      "Epoch 343/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 6.8330 - val_loss: 11.5950\n",
      "Epoch 344/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 6.1449 - val_loss: 15.1075\n",
      "Epoch 345/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 6.0061 - val_loss: 7.6329\n",
      "Epoch 346/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 8.1620 - val_loss: 20.1501\n",
      "Epoch 347/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 5.5140 - val_loss: 7.4097\n",
      "Epoch 348/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 6.4639 - val_loss: 18.5335\n",
      "Epoch 349/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 7.5167 - val_loss: 17.8855\n",
      "Epoch 350/3000\n",
      "8/8 [==============================] - 1s 126ms/step - loss: 6.0736 - val_loss: 21.3936\n",
      "Epoch 351/3000\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 5.2425 - val_loss: 34.1457\n",
      "Epoch 352/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 9.2724 - val_loss: 21.3076\n",
      "Epoch 353/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 5.8851 - val_loss: 7.9934\n",
      "Epoch 354/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 4.4507 - val_loss: 12.7512\n",
      "Epoch 355/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 9.0717 - val_loss: 14.1398\n",
      "Epoch 356/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 5.3014 - val_loss: 10.2654\n",
      "Epoch 357/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 5.9309 - val_loss: 7.3061\n",
      "Epoch 358/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 8.7681 - val_loss: 20.0331\n",
      "Epoch 359/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 6.1262 - val_loss: 53.8015\n",
      "Epoch 360/3000\n",
      "8/8 [==============================] - 1s 116ms/step - loss: 6.8316 - val_loss: 7.9870\n",
      "Epoch 361/3000\n",
      "8/8 [==============================] - 1s 117ms/step - loss: 5.7603 - val_loss: 28.2205\n",
      "Epoch 362/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 6.5838 - val_loss: 79.9208\n",
      "Epoch 363/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 6.0740 - val_loss: 12.1466\n",
      "Epoch 364/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 5.2178 - val_loss: 18.1350\n",
      "Epoch 365/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 8.1127 - val_loss: 7.8028\n",
      "Epoch 366/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 4.8378 - val_loss: 13.5545\n",
      "Epoch 367/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 4.4666 - val_loss: 7.4789\n",
      "Epoch 368/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 5.7024 - val_loss: 11.3559\n",
      "Epoch 369/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 4.9786 - val_loss: 6.7319\n",
      "Epoch 370/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 5.2091 - val_loss: 21.8351\n",
      "Epoch 371/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 5.3871 - val_loss: 11.7601\n",
      "Epoch 372/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 5.4385 - val_loss: 14.0847\n",
      "Epoch 373/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 5.6549 - val_loss: 17.9064\n",
      "Epoch 374/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 6.7570 - val_loss: 9.2501\n",
      "Epoch 375/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 5.0354 - val_loss: 11.4360\n",
      "Epoch 376/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 5.1282 - val_loss: 10.7289\n",
      "Epoch 377/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 7.6963 - val_loss: 55.6113\n",
      "Epoch 378/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 5.7885 - val_loss: 32.3354\n",
      "Epoch 379/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 5.8438 - val_loss: 6.3007\n",
      "Epoch 380/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 5.1843 - val_loss: 35.1403\n",
      "Epoch 381/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 6.7923 - val_loss: 11.1841\n",
      "Epoch 382/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 7.2822 - val_loss: 19.7299\n",
      "Epoch 383/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 6.2353 - val_loss: 7.8063\n",
      "Epoch 384/3000\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 4.7161 - val_loss: 10.2725\n",
      "Epoch 385/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 5.5274 - val_loss: 7.4524\n",
      "Epoch 386/3000\n",
      "8/8 [==============================] - 1s 127ms/step - loss: 7.4929 - val_loss: 8.5845\n",
      "Epoch 387/3000\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 7.0976 - val_loss: 8.5761\n",
      "Epoch 388/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 6.1217 - val_loss: 85.8682\n",
      "Epoch 389/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 4.8330 - val_loss: 13.2864\n",
      "Epoch 390/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 4.6684 - val_loss: 5.3165\n",
      "Epoch 391/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 5.8339 - val_loss: 22.3228\n",
      "Epoch 392/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 6.5341 - val_loss: 21.4557\n",
      "Epoch 393/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 7.2206 - val_loss: 17.6506\n",
      "Epoch 394/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 5.7203 - val_loss: 8.9340\n",
      "Epoch 395/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 4.8880 - val_loss: 34.8085\n",
      "Epoch 396/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 5.4669 - val_loss: 7.6801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 397/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 6.1925 - val_loss: 17.3000\n",
      "Epoch 398/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 6.2976 - val_loss: 32.0092\n",
      "Epoch 399/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 3.3980 - val_loss: 10.5770\n",
      "Epoch 400/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 6.9825 - val_loss: 14.0440\n",
      "Epoch 401/3000\n",
      "8/8 [==============================] - 1s 126ms/step - loss: 5.7335 - val_loss: 10.9906\n",
      "Epoch 402/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 4.8998 - val_loss: 46.4680\n",
      "Epoch 403/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 6.7657 - val_loss: 12.1796\n",
      "Epoch 404/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 5.2426 - val_loss: 6.3953\n",
      "Epoch 405/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 3.7675 - val_loss: 17.2626\n",
      "Epoch 406/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 6.6627 - val_loss: 13.7483\n",
      "Epoch 407/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 5.5389 - val_loss: 40.5193\n",
      "Epoch 408/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 5.3945 - val_loss: 6.5242\n",
      "Epoch 409/3000\n",
      "8/8 [==============================] - 1s 127ms/step - loss: 4.8729 - val_loss: 13.3070\n",
      "Epoch 410/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 4.7135 - val_loss: 7.9377\n",
      "Epoch 411/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 4.8506 - val_loss: 32.7989\n",
      "Epoch 412/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 3.8621 - val_loss: 13.3435\n",
      "Epoch 413/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 7.1354 - val_loss: 8.4275\n",
      "Epoch 414/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 5.5801 - val_loss: 55.3391\n",
      "Epoch 415/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 6.4623 - val_loss: 12.4529\n",
      "Epoch 416/3000\n",
      "8/8 [==============================] - 1s 117ms/step - loss: 4.4046 - val_loss: 6.6753\n",
      "Epoch 417/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 5.1058 - val_loss: 46.3457\n",
      "Epoch 418/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 4.7379 - val_loss: 10.0562\n",
      "Epoch 419/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 6.7392 - val_loss: 11.6286\n",
      "Epoch 420/3000\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 3.9915 - val_loss: 6.2876\n",
      "Epoch 421/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 4.9295 - val_loss: 10.8967\n",
      "Epoch 422/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 7.5834 - val_loss: 6.2348\n",
      "Epoch 423/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 4.2389 - val_loss: 13.4204\n",
      "Epoch 424/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 5.6463 - val_loss: 11.9675\n",
      "Epoch 425/3000\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 4.3677 - val_loss: 9.6106\n",
      "Epoch 426/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 6.9735 - val_loss: 83.8791\n",
      "Epoch 427/3000\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 4.4535 - val_loss: 6.8068\n",
      "Epoch 428/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 3.4164 - val_loss: 8.6693\n",
      "Epoch 429/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 4.3974 - val_loss: 16.3692\n",
      "Epoch 430/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 3.5874 - val_loss: 17.3496\n",
      "Epoch 431/3000\n",
      "8/8 [==============================] - 1s 127ms/step - loss: 5.0298 - val_loss: 25.4508\n",
      "Epoch 432/3000\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 6.6218 - val_loss: 8.7094\n",
      "Epoch 433/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 6.2654 - val_loss: 6.4363\n",
      "Epoch 434/3000\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 4.2957 - val_loss: 20.6335\n",
      "Epoch 435/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 5.1462 - val_loss: 15.2235\n",
      "Epoch 436/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 5.2884 - val_loss: 9.9754\n",
      "Epoch 437/3000\n",
      "8/8 [==============================] - 1s 126ms/step - loss: 5.0509 - val_loss: 70.2001\n",
      "Epoch 438/3000\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 5.5000 - val_loss: 13.1586\n",
      "Epoch 439/3000\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 3.8705 - val_loss: 7.4897\n",
      "Epoch 440/3000\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 4.6088 - val_loss: 18.0858\n",
      "Epoch 441/3000\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 6.1123 - val_loss: 5.8061\n",
      "Epoch 442/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 6.7632 - val_loss: 10.3220\n",
      "Epoch 443/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 5.4576 - val_loss: 8.0959\n",
      "Epoch 444/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 4.0459 - val_loss: 27.9369\n",
      "Epoch 445/3000\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 6.0350 - val_loss: 12.9907\n",
      "Epoch 446/3000\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 5.2881 - val_loss: 39.1845\n",
      "Epoch 447/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 4.2213 - val_loss: 10.0488\n",
      "Epoch 448/3000\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 4.9724 - val_loss: 11.1634\n",
      "Epoch 449/3000\n",
      "8/8 [==============================] - 1s 129ms/step - loss: 6.0848 - val_loss: 5.3214\n",
      "Epoch 450/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 2.9553 - val_loss: 6.7031\n",
      "Epoch 451/3000\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 3.8903 - val_loss: 13.1104\n",
      "Epoch 452/3000\n",
      "8/8 [==============================] - 1s 126ms/step - loss: 4.8044 - val_loss: 7.9627\n",
      "Epoch 453/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 3.8577 - val_loss: 9.8532\n",
      "Epoch 454/3000\n",
      "8/8 [==============================] - 1s 127ms/step - loss: 5.1839 - val_loss: 5.9965\n",
      "Epoch 455/3000\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 3.2295 - val_loss: 9.2555\n",
      "Epoch 456/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 5.9730 - val_loss: 67.9448\n",
      "Epoch 457/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 6.0568 - val_loss: 5.9329\n",
      "Epoch 458/3000\n",
      "8/8 [==============================] - 1s 126ms/step - loss: 5.9745 - val_loss: 8.4448\n",
      "Epoch 459/3000\n",
      "8/8 [==============================] - 1s 136ms/step - loss: 6.0631 - val_loss: 6.0026\n",
      "Epoch 460/3000\n",
      "8/8 [==============================] - 1s 131ms/step - loss: 3.4592 - val_loss: 8.1520\n",
      "Epoch 461/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 3.8976 - val_loss: 8.3772\n",
      "Epoch 462/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 4.3935 - val_loss: 156.4500\n",
      "Epoch 463/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 6.8166 - val_loss: 10.0858\n",
      "Epoch 464/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 2.6768 - val_loss: 12.6811\n",
      "Epoch 465/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 4.1467 - val_loss: 8.9430\n",
      "Epoch 466/3000\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 4.0020 - val_loss: 17.5107\n",
      "Epoch 467/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 4.5282 - val_loss: 13.8287\n",
      "Epoch 468/3000\n",
      "8/8 [==============================] - 1s 127ms/step - loss: 6.2192 - val_loss: 9.4304\n",
      "Epoch 469/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 2.8772 - val_loss: 10.4694\n",
      "Epoch 470/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 4.0206 - val_loss: 6.6445\n",
      "Epoch 471/3000\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 5.0979 - val_loss: 25.4012\n",
      "Epoch 472/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 3.7030 - val_loss: 14.2763\n",
      "Epoch 473/3000\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 5.5907 - val_loss: 12.5247\n",
      "Epoch 474/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 5.5086 - val_loss: 7.0075\n",
      "Epoch 475/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 3.8510 - val_loss: 6.8664\n",
      "Epoch 476/3000\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 3.4488 - val_loss: 7.1705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 477/3000\n",
      "8/8 [==============================] - 1s 128ms/step - loss: 3.5128 - val_loss: 8.3528\n",
      "Epoch 478/3000\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 2.5517 - val_loss: 28.2586\n",
      "Epoch 479/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 5.0755 - val_loss: 9.7838\n",
      "Epoch 480/3000\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 4.7783 - val_loss: 20.3050\n",
      "Epoch 481/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 5.3957 - val_loss: 34.8764\n",
      "Epoch 482/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 4.7334 - val_loss: 6.2822\n",
      "Epoch 483/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 3.9375 - val_loss: 22.4767\n",
      "Epoch 484/3000\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 5.1111 - val_loss: 11.4539\n",
      "Epoch 485/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 3.5216 - val_loss: 20.4303\n",
      "Epoch 486/3000\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 3.7444 - val_loss: 24.1256\n",
      "Epoch 487/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 5.3298 - val_loss: 13.2434\n",
      "Epoch 488/3000\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 4.9622 - val_loss: 22.2567\n",
      "Epoch 489/3000\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 3.9069 - val_loss: 14.3276\n",
      "Epoch 490/3000\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 5.2813 - val_loss: 9.3064\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x215c05dff10>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential() \n",
    "model.add(LSTM(1024, return_sequences=True, input_shape=(X_train.shape[1], 1))) \n",
    "\n",
    "model.add(LSTM(1024, return_sequences=True)) \n",
    "\n",
    "model.add(LSTM(512))\n",
    "model.add(BatchNormalization()) \n",
    "\n",
    "model.add(Dense(128, activation='relu')) \n",
    "model.add(Dense(64, activation='relu')) \n",
    "model.add(Dense(3)) \n",
    "model.compile(loss='mse', optimizer='RMSprop') \n",
    "\n",
    "early_stopping = EarlyStopping(patience=100) \n",
    "model.fit(X_train, y_train, epochs=3000, batch_size=256, validation_data=(X_test, y_test), callbacks=[early_stopping]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a70e0c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error, explained_variance_score, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a44dd46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 12ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predict on test data\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "22e127f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 5.273010001723975\n",
      "Mean Absolute Error: 1.5149930317308256\n",
      "Median Absolute Error: 1.1067445436132612\n",
      "Explained Variance Score: 0.989963418191707\n",
      "R2 Score: 0.9899481912234266\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
    "print(\"Mean Absolute Error:\", mean_absolute_error(y_test, y_pred))\n",
    "print(\"Median Absolute Error:\", median_absolute_error(y_test, y_pred))\n",
    "print(\"Explained Variance Score:\", explained_variance_score(y_test, y_pred))\n",
    "print(\"R2 Score:\", r2_score(y_test, y_pred))\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbf0006",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89a4538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df81fdb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce1a97a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78056a9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0896b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ec6bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5971a126",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6648abe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e734bbc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fee5f95e",
   "metadata": {},
   "source": [
    "# Hybrid Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7809856",
   "metadata": {},
   "source": [
    "This hybrid model combines LSTM, Convolutional layers and GRU layers. The Convolutional layer and GRU layer have been added to make the model more unique and powerful. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a093c1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 18s 24ms/step - loss: 37.1274 - val_loss: 37.7069\n",
      "Epoch 2/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 36.8630 - val_loss: 37.5235\n",
      "Epoch 3/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 36.5249 - val_loss: 37.2054\n",
      "Epoch 4/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 36.0971 - val_loss: 36.2183\n",
      "Epoch 5/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 35.6835 - val_loss: 35.6716\n",
      "Epoch 6/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 35.3395 - val_loss: 35.1317\n",
      "Epoch 7/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 34.8672 - val_loss: 34.8978\n",
      "Epoch 8/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 34.5142 - val_loss: 34.6103\n",
      "Epoch 9/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 34.2679 - val_loss: 34.5393\n",
      "Epoch 10/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 34.0755 - val_loss: 34.3316\n",
      "Epoch 11/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 33.9166 - val_loss: 34.2671\n",
      "Epoch 12/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 33.7874 - val_loss: 34.0960\n",
      "Epoch 13/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 33.6478 - val_loss: 34.0522\n",
      "Epoch 14/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 33.5283 - val_loss: 33.9243\n",
      "Epoch 15/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 33.4340 - val_loss: 33.9746\n",
      "Epoch 16/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 33.3838 - val_loss: 33.6531\n",
      "Epoch 17/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 33.2465 - val_loss: 33.5462\n",
      "Epoch 18/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 33.1045 - val_loss: 33.6032\n",
      "Epoch 19/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 33.0122 - val_loss: 33.3152\n",
      "Epoch 20/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 32.8950 - val_loss: 33.2289\n",
      "Epoch 21/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 32.8003 - val_loss: 33.1517\n",
      "Epoch 22/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 32.7229 - val_loss: 33.3623\n",
      "Epoch 23/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 32.6563 - val_loss: 33.3167\n",
      "Epoch 24/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 32.5771 - val_loss: 32.9519\n",
      "Epoch 25/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 32.4726 - val_loss: 32.8101\n",
      "Epoch 26/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 32.3811 - val_loss: 32.7823\n",
      "Epoch 27/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 32.2966 - val_loss: 33.0385\n",
      "Epoch 28/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 32.2561 - val_loss: 32.6416\n",
      "Epoch 29/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 32.1324 - val_loss: 32.5868\n",
      "Epoch 30/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 32.0572 - val_loss: 32.3904\n",
      "Epoch 31/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 31.9783 - val_loss: 32.3211\n",
      "Epoch 32/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 31.8950 - val_loss: 32.4508\n",
      "Epoch 33/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 31.8005 - val_loss: 32.1242\n",
      "Epoch 34/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 31.6979 - val_loss: 32.0378\n",
      "Epoch 35/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 31.6388 - val_loss: 32.0281\n",
      "Epoch 36/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 31.5519 - val_loss: 31.9442\n",
      "Epoch 37/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 31.4600 - val_loss: 31.8944\n",
      "Epoch 38/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 31.3705 - val_loss: 31.8083\n",
      "Epoch 39/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 31.4713 - val_loss: 31.7174\n",
      "Epoch 40/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 31.2401 - val_loss: 31.5518\n",
      "Epoch 41/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 31.1654 - val_loss: 31.6268\n",
      "Epoch 42/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 31.0992 - val_loss: 31.3883\n",
      "Epoch 43/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 30.9618 - val_loss: 31.3446\n",
      "Epoch 44/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 30.8988 - val_loss: 31.2245\n",
      "Epoch 45/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 30.8192 - val_loss: 31.1902\n",
      "Epoch 46/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 30.7526 - val_loss: 31.0267\n",
      "Epoch 47/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 30.6734 - val_loss: 30.9982\n",
      "Epoch 48/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 30.5805 - val_loss: 30.8661\n",
      "Epoch 49/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 30.5016 - val_loss: 30.9481\n",
      "Epoch 50/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 30.4275 - val_loss: 30.8096\n",
      "Epoch 51/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 30.3412 - val_loss: 30.6231\n",
      "Epoch 52/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 30.2846 - val_loss: 30.5719\n",
      "Epoch 53/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 30.1718 - val_loss: 30.5002\n",
      "Epoch 54/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 30.1141 - val_loss: 30.4490\n",
      "Epoch 55/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 30.0052 - val_loss: 30.3494\n",
      "Epoch 56/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 29.9630 - val_loss: 30.2606\n",
      "Epoch 57/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 29.8792 - val_loss: 30.2305\n",
      "Epoch 58/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 29.7952 - val_loss: 30.1058\n",
      "Epoch 59/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 29.7246 - val_loss: 30.0713\n",
      "Epoch 60/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 29.6681 - val_loss: 29.9798\n",
      "Epoch 61/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 29.5650 - val_loss: 29.9297\n",
      "Epoch 62/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 29.5250 - val_loss: 29.8786\n",
      "Epoch 63/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 29.4212 - val_loss: 29.7339\n",
      "Epoch 64/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 29.3624 - val_loss: 29.6513\n",
      "Epoch 65/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 29.2856 - val_loss: 29.7174\n",
      "Epoch 66/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 29.2007 - val_loss: 29.5469\n",
      "Epoch 67/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 29.1585 - val_loss: 29.5105\n",
      "Epoch 68/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 29.1168 - val_loss: 29.3665\n",
      "Epoch 69/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 28.9823 - val_loss: 29.2920\n",
      "Epoch 70/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 28.9077 - val_loss: 29.2188\n",
      "Epoch 71/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 28.8302 - val_loss: 29.1399\n",
      "Epoch 72/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 28.7618 - val_loss: 29.1773\n",
      "Epoch 73/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 28.6900 - val_loss: 29.0086\n",
      "Epoch 74/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 28.6427 - val_loss: 28.9479\n",
      "Epoch 75/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 28.5441 - val_loss: 28.8602\n",
      "Epoch 76/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 28.4797 - val_loss: 28.8423\n",
      "Epoch 77/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 28.4095 - val_loss: 28.7228\n",
      "Epoch 78/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 28.3256 - val_loss: 28.6182\n",
      "Epoch 79/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 28.2514 - val_loss: 28.6217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 28.1848 - val_loss: 28.5777\n",
      "Epoch 81/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 28.2088 - val_loss: 28.4370\n",
      "Epoch 82/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 28.0372 - val_loss: 28.3630\n",
      "Epoch 83/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 27.9721 - val_loss: 28.2768\n",
      "Epoch 84/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 27.8899 - val_loss: 28.2474\n",
      "Epoch 85/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 27.8181 - val_loss: 28.1327\n",
      "Epoch 86/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 27.7893 - val_loss: 28.0726\n",
      "Epoch 87/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 27.6857 - val_loss: 27.9939\n",
      "Epoch 88/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 27.6173 - val_loss: 27.9073\n",
      "Epoch 89/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 27.5287 - val_loss: 27.8388\n",
      "Epoch 90/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 27.4659 - val_loss: 27.8119\n",
      "Epoch 91/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 27.3804 - val_loss: 27.7025\n",
      "Epoch 92/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 27.3135 - val_loss: 27.6501\n",
      "Epoch 93/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 27.2451 - val_loss: 27.5437\n",
      "Epoch 94/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 27.1644 - val_loss: 27.4872\n",
      "Epoch 95/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 27.1063 - val_loss: 27.4017\n",
      "Epoch 96/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 27.0376 - val_loss: 27.3162\n",
      "Epoch 97/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 26.9584 - val_loss: 27.2578\n",
      "Epoch 98/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 26.8922 - val_loss: 27.2278\n",
      "Epoch 99/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 26.8390 - val_loss: 27.1167\n",
      "Epoch 100/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 26.7290 - val_loss: 27.0583\n",
      "Epoch 101/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 26.6679 - val_loss: 26.9723\n",
      "Epoch 102/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 26.5905 - val_loss: 26.9229\n",
      "Epoch 103/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 26.5255 - val_loss: 26.8417\n",
      "Epoch 104/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 26.4447 - val_loss: 26.7769\n",
      "Epoch 105/1000\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 26.3787 - val_loss: 26.6740\n",
      "Epoch 106/1000\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 26.2968 - val_loss: 26.6010\n",
      "Epoch 107/1000\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 26.2226 - val_loss: 26.5294\n",
      "Epoch 108/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 26.1766 - val_loss: 26.5878\n",
      "Epoch 109/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 26.0840 - val_loss: 26.4146\n",
      "Epoch 110/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 26.0198 - val_loss: 26.3182\n",
      "Epoch 111/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 25.9254 - val_loss: 26.2355\n",
      "Epoch 112/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 25.8589 - val_loss: 26.1945\n",
      "Epoch 113/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 25.7970 - val_loss: 26.1954\n",
      "Epoch 114/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 25.7591 - val_loss: 26.1333\n",
      "Epoch 115/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 25.6615 - val_loss: 25.9547\n",
      "Epoch 116/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 25.5685 - val_loss: 26.0336\n",
      "Epoch 117/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 25.5462 - val_loss: 25.8948\n",
      "Epoch 118/1000\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 25.4510 - val_loss: 25.7353\n",
      "Epoch 119/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 25.3825 - val_loss: 25.6613\n",
      "Epoch 120/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 25.3208 - val_loss: 25.6032\n",
      "Epoch 121/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 25.2460 - val_loss: 25.5513\n",
      "Epoch 122/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 25.2017 - val_loss: 25.4849\n",
      "Epoch 123/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 25.1248 - val_loss: 25.4462\n",
      "Epoch 124/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 25.0941 - val_loss: 25.4337\n",
      "Epoch 125/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 25.0181 - val_loss: 25.3062\n",
      "Epoch 126/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 24.9481 - val_loss: 25.2204\n",
      "Epoch 127/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 24.8988 - val_loss: 25.1686\n",
      "Epoch 128/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 24.8458 - val_loss: 25.0909\n",
      "Epoch 129/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 24.7452 - val_loss: 25.1434\n",
      "Epoch 130/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 24.6878 - val_loss: 24.9650\n",
      "Epoch 131/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 24.6250 - val_loss: 24.9042\n",
      "Epoch 132/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 24.5802 - val_loss: 24.9306\n",
      "Epoch 133/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 24.5315 - val_loss: 24.9387\n",
      "Epoch 134/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 24.4498 - val_loss: 24.7364\n",
      "Epoch 135/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 24.3924 - val_loss: 24.6555\n",
      "Epoch 136/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 24.3400 - val_loss: 24.6830\n",
      "Epoch 137/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 24.2910 - val_loss: 24.5461\n",
      "Epoch 138/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 24.2209 - val_loss: 24.5020\n",
      "Epoch 139/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 24.1530 - val_loss: 24.5531\n",
      "Epoch 140/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 24.1151 - val_loss: 24.3983\n",
      "Epoch 141/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 24.0445 - val_loss: 24.4647\n",
      "Epoch 142/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 23.9757 - val_loss: 24.2540\n",
      "Epoch 143/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 23.9254 - val_loss: 24.2752\n",
      "Epoch 144/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 23.8747 - val_loss: 24.1716\n",
      "Epoch 145/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 23.8278 - val_loss: 24.0712\n",
      "Epoch 146/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 23.7530 - val_loss: 24.0415\n",
      "Epoch 147/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 23.6998 - val_loss: 23.9629\n",
      "Epoch 148/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 23.6824 - val_loss: 24.0001\n",
      "Epoch 149/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 23.6135 - val_loss: 23.9544\n",
      "Epoch 150/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 23.5536 - val_loss: 23.8227\n",
      "Epoch 151/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 23.4885 - val_loss: 23.7677\n",
      "Epoch 152/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 23.4699 - val_loss: 23.7768\n",
      "Epoch 153/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 23.4730 - val_loss: 23.6940\n",
      "Epoch 154/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 23.3498 - val_loss: 23.6297\n",
      "Epoch 155/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 23.2932 - val_loss: 23.5448\n",
      "Epoch 156/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 23.2566 - val_loss: 23.5413\n",
      "Epoch 157/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 23.2102 - val_loss: 23.4459\n",
      "Epoch 158/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 23.1525 - val_loss: 23.4076\n",
      "Epoch 159/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 23.0844 - val_loss: 23.3392\n",
      "Epoch 160/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 23.0144 - val_loss: 23.2886\n",
      "Epoch 161/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 22.9789 - val_loss: 23.3040\n",
      "Epoch 162/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 22.9594 - val_loss: 23.3868\n",
      "Epoch 163/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 22.9269 - val_loss: 23.1876\n",
      "Epoch 164/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 22.8404 - val_loss: 23.1843\n",
      "Epoch 165/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 22.7969 - val_loss: 23.0552\n",
      "Epoch 166/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 22.7604 - val_loss: 22.9892\n",
      "Epoch 167/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 22.7011 - val_loss: 22.9221\n",
      "Epoch 168/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 22.6728 - val_loss: 22.9909\n",
      "Epoch 169/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 22.5842 - val_loss: 22.8266\n",
      "Epoch 170/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 22.5287 - val_loss: 22.8224\n",
      "Epoch 171/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 22.4867 - val_loss: 22.7383\n",
      "Epoch 172/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 22.4633 - val_loss: 22.7635\n",
      "Epoch 173/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 22.4009 - val_loss: 22.6079\n",
      "Epoch 174/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 22.3659 - val_loss: 22.6528\n",
      "Epoch 175/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 22.3162 - val_loss: 22.5101\n",
      "Epoch 176/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 22.2577 - val_loss: 22.4448\n",
      "Epoch 177/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 22.2319 - val_loss: 22.6957\n",
      "Epoch 178/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 22.1430 - val_loss: 22.3889\n",
      "Epoch 179/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 22.0975 - val_loss: 22.3486\n",
      "Epoch 180/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 22.0726 - val_loss: 22.3339\n",
      "Epoch 181/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 21.9881 - val_loss: 22.2079\n",
      "Epoch 182/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 21.9473 - val_loss: 22.2627\n",
      "Epoch 183/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 21.9335 - val_loss: 22.1926\n",
      "Epoch 184/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 21.8952 - val_loss: 22.1716\n",
      "Epoch 185/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 21.8156 - val_loss: 22.1445\n",
      "Epoch 186/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 21.7707 - val_loss: 22.0201\n",
      "Epoch 187/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 21.7462 - val_loss: 21.9358\n",
      "Epoch 188/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 21.6831 - val_loss: 21.9105\n",
      "Epoch 189/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 21.6293 - val_loss: 21.8795\n",
      "Epoch 190/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 21.5959 - val_loss: 21.7715\n",
      "Epoch 191/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 21.5701 - val_loss: 21.7135\n",
      "Epoch 192/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 21.5278 - val_loss: 21.7360\n",
      "Epoch 193/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 21.5331 - val_loss: 21.7327\n",
      "Epoch 194/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 21.4649 - val_loss: 21.7262\n",
      "Epoch 195/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 21.4287 - val_loss: 21.6342\n",
      "Epoch 196/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 21.3703 - val_loss: 21.6908\n",
      "Epoch 197/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 21.3240 - val_loss: 21.4914\n",
      "Epoch 198/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 21.2892 - val_loss: 21.4990\n",
      "Epoch 199/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 21.2726 - val_loss: 21.4222\n",
      "Epoch 200/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 21.2467 - val_loss: 21.3813\n",
      "Epoch 201/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 21.1598 - val_loss: 21.3480\n",
      "Epoch 202/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 21.1240 - val_loss: 21.3210\n",
      "Epoch 203/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 21.1386 - val_loss: 21.2638\n",
      "Epoch 204/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 21.0555 - val_loss: 21.2817\n",
      "Epoch 205/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 21.0539 - val_loss: 21.2285\n",
      "Epoch 206/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 20.9791 - val_loss: 21.2074\n",
      "Epoch 207/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 20.9915 - val_loss: 21.1678\n",
      "Epoch 208/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 20.9382 - val_loss: 21.1162\n",
      "Epoch 209/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 20.9059 - val_loss: 21.0756\n",
      "Epoch 210/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 20.8521 - val_loss: 21.1262\n",
      "Epoch 211/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 20.9399 - val_loss: 20.9852\n",
      "Epoch 212/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 20.8100 - val_loss: 20.9519\n",
      "Epoch 213/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 20.7587 - val_loss: 20.9307\n",
      "Epoch 214/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 20.7459 - val_loss: 20.9273\n",
      "Epoch 215/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 20.6918 - val_loss: 20.8392\n",
      "Epoch 216/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 20.6527 - val_loss: 20.8040\n",
      "Epoch 217/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 20.5723 - val_loss: 20.7176\n",
      "Epoch 218/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 20.5127 - val_loss: 20.6998\n",
      "Epoch 219/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 20.4786 - val_loss: 20.6511\n",
      "Epoch 220/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 20.4323 - val_loss: 20.6023\n",
      "Epoch 221/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 20.4157 - val_loss: 20.6141\n",
      "Epoch 222/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 20.3756 - val_loss: 20.5310\n",
      "Epoch 223/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 20.2932 - val_loss: 20.3966\n",
      "Epoch 224/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 20.2074 - val_loss: 20.4564\n",
      "Epoch 225/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 20.1719 - val_loss: 20.2092\n",
      "Epoch 226/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 19.9569 - val_loss: 20.1222\n",
      "Epoch 227/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 19.9722 - val_loss: 20.1260\n",
      "Epoch 228/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 19.8116 - val_loss: 20.0856\n",
      "Epoch 229/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 19.8358 - val_loss: 19.9794\n",
      "Epoch 230/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 19.7457 - val_loss: 19.9025\n",
      "Epoch 231/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 19.6710 - val_loss: 19.9412\n",
      "Epoch 232/1000\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 19.6318 - val_loss: 19.8578\n",
      "Epoch 233/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 19.6063 - val_loss: 19.8099\n",
      "Epoch 234/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 19.5653 - val_loss: 19.8023\n",
      "Epoch 235/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 19.6033 - val_loss: 19.7581\n",
      "Epoch 236/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 1s 14ms/step - loss: 19.4633 - val_loss: 19.6562\n",
      "Epoch 237/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 19.4342 - val_loss: 19.6074\n",
      "Epoch 238/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 19.4060 - val_loss: 19.5975\n",
      "Epoch 239/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 19.3513 - val_loss: 19.5532\n",
      "Epoch 240/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 19.3212 - val_loss: 19.5728\n",
      "Epoch 241/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 19.2640 - val_loss: 19.4700\n",
      "Epoch 242/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 19.2363 - val_loss: 19.4171\n",
      "Epoch 243/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 19.1871 - val_loss: 19.4230\n",
      "Epoch 244/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 19.1630 - val_loss: 19.3967\n",
      "Epoch 245/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 19.2078 - val_loss: 19.3205\n",
      "Epoch 246/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 19.0776 - val_loss: 19.2678\n",
      "Epoch 247/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 19.0210 - val_loss: 19.2619\n",
      "Epoch 248/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 19.0684 - val_loss: 19.2107\n",
      "Epoch 249/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 18.9756 - val_loss: 19.1686\n",
      "Epoch 250/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 18.9188 - val_loss: 19.1748\n",
      "Epoch 251/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 18.8861 - val_loss: 19.0663\n",
      "Epoch 252/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 18.8303 - val_loss: 19.0207\n",
      "Epoch 253/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 18.8225 - val_loss: 19.3083\n",
      "Epoch 254/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 18.8150 - val_loss: 18.9974\n",
      "Epoch 255/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 18.7746 - val_loss: 18.8998\n",
      "Epoch 256/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 18.6777 - val_loss: 18.8506\n",
      "Epoch 257/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 18.6518 - val_loss: 18.8619\n",
      "Epoch 258/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 18.6159 - val_loss: 18.7637\n",
      "Epoch 259/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 18.5728 - val_loss: 18.7634\n",
      "Epoch 260/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 18.4632 - val_loss: 18.6137\n",
      "Epoch 261/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 18.4009 - val_loss: 18.5356\n",
      "Epoch 262/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 18.3175 - val_loss: 18.4919\n",
      "Epoch 263/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 18.2633 - val_loss: 18.4799\n",
      "Epoch 264/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 18.2853 - val_loss: 18.4711\n",
      "Epoch 265/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 18.2660 - val_loss: 18.4133\n",
      "Epoch 266/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 18.2479 - val_loss: 18.3831\n",
      "Epoch 267/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 18.0810 - val_loss: 18.1768\n",
      "Epoch 268/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 17.9490 - val_loss: 18.1204\n",
      "Epoch 269/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 17.9736 - val_loss: 18.1438\n",
      "Epoch 270/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 17.9665 - val_loss: 18.0239\n",
      "Epoch 271/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 17.8102 - val_loss: 18.0063\n",
      "Epoch 272/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 17.7646 - val_loss: 17.9413\n",
      "Epoch 273/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 17.7176 - val_loss: 17.9096\n",
      "Epoch 274/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 17.7620 - val_loss: 17.8918\n",
      "Epoch 275/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 17.6449 - val_loss: 17.8972\n",
      "Epoch 276/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 17.6196 - val_loss: 17.8021\n",
      "Epoch 277/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 17.5508 - val_loss: 17.7239\n",
      "Epoch 278/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 17.4549 - val_loss: 17.6267\n",
      "Epoch 279/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 17.3827 - val_loss: 17.5640\n",
      "Epoch 280/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 17.3482 - val_loss: 17.7120\n",
      "Epoch 281/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 17.4748 - val_loss: 17.5198\n",
      "Epoch 282/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 17.2535 - val_loss: 17.4018\n",
      "Epoch 283/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 17.1866 - val_loss: 17.3683\n",
      "Epoch 284/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 17.2190 - val_loss: 17.3153\n",
      "Epoch 285/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 17.1492 - val_loss: 17.2754\n",
      "Epoch 286/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 17.0433 - val_loss: 17.2188\n",
      "Epoch 287/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 17.0288 - val_loss: 17.1969\n",
      "Epoch 288/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 17.0120 - val_loss: 17.1483\n",
      "Epoch 289/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 16.9841 - val_loss: 17.2319\n",
      "Epoch 290/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 16.8995 - val_loss: 17.0728\n",
      "Epoch 291/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 16.8649 - val_loss: 17.0232\n",
      "Epoch 292/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 16.8202 - val_loss: 17.0006\n",
      "Epoch 293/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 16.7852 - val_loss: 16.9783\n",
      "Epoch 294/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 16.8910 - val_loss: 16.9773\n",
      "Epoch 295/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 16.7394 - val_loss: 16.8795\n",
      "Epoch 296/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 16.6619 - val_loss: 16.8660\n",
      "Epoch 297/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 16.6357 - val_loss: 16.8542\n",
      "Epoch 298/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 16.6151 - val_loss: 16.7225\n",
      "Epoch 299/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 16.6952 - val_loss: 16.7294\n",
      "Epoch 300/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 16.5884 - val_loss: 16.7181\n",
      "Epoch 301/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 16.4695 - val_loss: 16.6325\n",
      "Epoch 302/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 16.4453 - val_loss: 16.6063\n",
      "Epoch 303/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 16.4938 - val_loss: 16.5568\n",
      "Epoch 304/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 16.3721 - val_loss: 16.4903\n",
      "Epoch 305/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 16.3083 - val_loss: 16.5223\n",
      "Epoch 306/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 16.3047 - val_loss: 16.4711\n",
      "Epoch 307/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 16.3056 - val_loss: 16.4159\n",
      "Epoch 308/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 16.2086 - val_loss: 16.3503\n",
      "Epoch 309/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 16.1740 - val_loss: 16.3367\n",
      "Epoch 310/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 16.1206 - val_loss: 16.2746\n",
      "Epoch 311/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 16.1521 - val_loss: 16.2716\n",
      "Epoch 312/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 16.0680 - val_loss: 16.3438\n",
      "Epoch 313/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 16.1136 - val_loss: 16.2326\n",
      "Epoch 314/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 15.9760 - val_loss: 16.1371\n",
      "Epoch 315/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 16.0253 - val_loss: 16.0833\n",
      "Epoch 316/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 15.9286 - val_loss: 16.0879\n",
      "Epoch 317/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 15.8724 - val_loss: 16.0806\n",
      "Epoch 318/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 15.8402 - val_loss: 15.9905\n",
      "Epoch 319/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 15.7670 - val_loss: 15.9222\n",
      "Epoch 320/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 15.7480 - val_loss: 15.9489\n",
      "Epoch 321/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 15.7416 - val_loss: 15.8735\n",
      "Epoch 322/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 15.7111 - val_loss: 15.8218\n",
      "Epoch 323/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 15.6625 - val_loss: 15.8850\n",
      "Epoch 324/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 15.6393 - val_loss: 15.8954\n",
      "Epoch 325/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 15.6252 - val_loss: 15.7277\n",
      "Epoch 326/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 15.5241 - val_loss: 15.7085\n",
      "Epoch 327/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 15.4574 - val_loss: 15.6421\n",
      "Epoch 328/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 15.4410 - val_loss: 15.5920\n",
      "Epoch 329/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 15.4573 - val_loss: 15.5089\n",
      "Epoch 330/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 15.3668 - val_loss: 15.5260\n",
      "Epoch 331/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 15.3945 - val_loss: 15.5180\n",
      "Epoch 332/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 15.3585 - val_loss: 15.4516\n",
      "Epoch 333/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 15.2680 - val_loss: 15.4314\n",
      "Epoch 334/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 15.1816 - val_loss: 15.3536\n",
      "Epoch 335/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 15.1340 - val_loss: 15.3111\n",
      "Epoch 336/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 15.0970 - val_loss: 15.2826\n",
      "Epoch 337/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 15.0866 - val_loss: 15.2803\n",
      "Epoch 338/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 15.0484 - val_loss: 15.3351\n",
      "Epoch 339/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 14.9944 - val_loss: 15.1356\n",
      "Epoch 340/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 14.9265 - val_loss: 15.1854\n",
      "Epoch 341/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 14.8820 - val_loss: 15.1917\n",
      "Epoch 342/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 14.9344 - val_loss: 15.0281\n",
      "Epoch 343/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 14.8198 - val_loss: 14.9905\n",
      "Epoch 344/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 14.7771 - val_loss: 15.0294\n",
      "Epoch 345/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 14.7406 - val_loss: 14.8999\n",
      "Epoch 346/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 14.7232 - val_loss: 14.8940\n",
      "Epoch 347/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 14.6736 - val_loss: 14.8202\n",
      "Epoch 348/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 14.6452 - val_loss: 14.7574\n",
      "Epoch 349/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 14.5827 - val_loss: 14.7089\n",
      "Epoch 350/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 14.5585 - val_loss: 14.7023\n",
      "Epoch 351/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 14.5428 - val_loss: 14.7001\n",
      "Epoch 352/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 14.5000 - val_loss: 14.6468\n",
      "Epoch 353/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 14.4851 - val_loss: 14.6432\n",
      "Epoch 354/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 14.4165 - val_loss: 14.5432\n",
      "Epoch 355/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 14.3874 - val_loss: 14.5093\n",
      "Epoch 356/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 14.3559 - val_loss: 14.5539\n",
      "Epoch 357/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 14.3299 - val_loss: 14.4297\n",
      "Epoch 358/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 14.2384 - val_loss: 14.4345\n",
      "Epoch 359/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 14.3184 - val_loss: 14.3706\n",
      "Epoch 360/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 14.1776 - val_loss: 14.3311\n",
      "Epoch 361/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 14.1224 - val_loss: 14.3510\n",
      "Epoch 362/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 14.2716 - val_loss: 14.2852\n",
      "Epoch 363/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 14.0938 - val_loss: 14.2115\n",
      "Epoch 364/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 14.0337 - val_loss: 14.1597\n",
      "Epoch 365/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 14.0022 - val_loss: 14.1241\n",
      "Epoch 366/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 14.0023 - val_loss: 14.1857\n",
      "Epoch 367/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 13.9734 - val_loss: 14.1868\n",
      "Epoch 368/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 13.8954 - val_loss: 14.1023\n",
      "Epoch 369/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 13.8998 - val_loss: 13.9636\n",
      "Epoch 370/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 13.8565 - val_loss: 13.9167\n",
      "Epoch 371/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 13.7731 - val_loss: 13.8953\n",
      "Epoch 372/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 13.7377 - val_loss: 13.8597\n",
      "Epoch 373/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 13.7304 - val_loss: 13.8495\n",
      "Epoch 374/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 13.6977 - val_loss: 13.7952\n",
      "Epoch 375/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 13.6323 - val_loss: 13.8009\n",
      "Epoch 376/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 13.6143 - val_loss: 13.8187\n",
      "Epoch 377/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 13.5537 - val_loss: 13.7018\n",
      "Epoch 378/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 13.5463 - val_loss: 13.6654\n",
      "Epoch 379/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 13.5976 - val_loss: 13.9008\n",
      "Epoch 380/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 13.5972 - val_loss: 13.6567\n",
      "Epoch 381/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 13.5787 - val_loss: 13.5805\n",
      "Epoch 382/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 13.4738 - val_loss: 13.6301\n",
      "Epoch 383/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 13.6342 - val_loss: 13.6243\n",
      "Epoch 384/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 13.3705 - val_loss: 13.5449\n",
      "Epoch 385/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 13.3511 - val_loss: 13.4582\n",
      "Epoch 386/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 13.2927 - val_loss: 13.3391\n",
      "Epoch 387/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 13.2568 - val_loss: 13.3468\n",
      "Epoch 388/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 13.2479 - val_loss: 13.3451\n",
      "Epoch 389/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 13.2137 - val_loss: 13.2712\n",
      "Epoch 390/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 13.1281 - val_loss: 13.2111\n",
      "Epoch 391/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 13.0939 - val_loss: 13.2040\n",
      "Epoch 392/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 1s 13ms/step - loss: 13.0458 - val_loss: 13.1644\n",
      "Epoch 393/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 13.0282 - val_loss: 13.1325\n",
      "Epoch 394/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 12.9943 - val_loss: 13.1019\n",
      "Epoch 395/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 12.9831 - val_loss: 13.1313\n",
      "Epoch 396/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 12.9552 - val_loss: 13.1988\n",
      "Epoch 397/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 13.0446 - val_loss: 13.0407\n",
      "Epoch 398/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 12.9159 - val_loss: 13.1768\n",
      "Epoch 399/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 13.2109 - val_loss: 13.1091\n",
      "Epoch 400/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 12.9384 - val_loss: 13.0747\n",
      "Epoch 401/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 12.8394 - val_loss: 12.9351\n",
      "Epoch 402/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 12.7888 - val_loss: 12.8992\n",
      "Epoch 403/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 12.7540 - val_loss: 12.7723\n",
      "Epoch 404/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 12.7130 - val_loss: 12.8221\n",
      "Epoch 405/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 12.6657 - val_loss: 12.7935\n",
      "Epoch 406/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 12.6992 - val_loss: 12.7720\n",
      "Epoch 407/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 12.6397 - val_loss: 12.7062\n",
      "Epoch 408/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 12.6932 - val_loss: 12.6107\n",
      "Epoch 409/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 12.5487 - val_loss: 12.6007\n",
      "Epoch 410/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 12.5139 - val_loss: 12.5345\n",
      "Epoch 411/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 12.4297 - val_loss: 12.5955\n",
      "Epoch 412/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 12.4128 - val_loss: 12.6883\n",
      "Epoch 413/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 12.4459 - val_loss: 12.5303\n",
      "Epoch 414/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 12.4095 - val_loss: 12.4479\n",
      "Epoch 415/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 12.3053 - val_loss: 12.3877\n",
      "Epoch 416/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 12.2590 - val_loss: 12.4258\n",
      "Epoch 417/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 12.3399 - val_loss: 12.3219\n",
      "Epoch 418/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 12.1843 - val_loss: 12.2900\n",
      "Epoch 419/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 12.1380 - val_loss: 12.2822\n",
      "Epoch 420/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 12.1078 - val_loss: 12.1861\n",
      "Epoch 421/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 12.0964 - val_loss: 12.1695\n",
      "Epoch 422/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 12.1288 - val_loss: 12.1201\n",
      "Epoch 423/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 12.0459 - val_loss: 12.0992\n",
      "Epoch 424/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 11.9947 - val_loss: 12.0404\n",
      "Epoch 425/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 11.9461 - val_loss: 12.1148\n",
      "Epoch 426/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 11.9300 - val_loss: 11.9963\n",
      "Epoch 427/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 11.8714 - val_loss: 11.9207\n",
      "Epoch 428/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 11.8166 - val_loss: 11.9312\n",
      "Epoch 429/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 11.7789 - val_loss: 11.8369\n",
      "Epoch 430/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 11.7400 - val_loss: 11.8345\n",
      "Epoch 431/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 11.7395 - val_loss: 11.7888\n",
      "Epoch 432/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 11.7087 - val_loss: 11.8128\n",
      "Epoch 433/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 11.6412 - val_loss: 11.8011\n",
      "Epoch 434/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 11.6348 - val_loss: 11.6759\n",
      "Epoch 435/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 11.6193 - val_loss: 11.7677\n",
      "Epoch 436/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 12.1415 - val_loss: 11.7049\n",
      "Epoch 437/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 11.5718 - val_loss: 11.6882\n",
      "Epoch 438/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 11.6426 - val_loss: 11.6320\n",
      "Epoch 439/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 11.4803 - val_loss: 11.5875\n",
      "Epoch 440/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 11.4517 - val_loss: 11.5081\n",
      "Epoch 441/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 11.4790 - val_loss: 11.5668\n",
      "Epoch 442/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 11.3792 - val_loss: 11.4374\n",
      "Epoch 443/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 11.3337 - val_loss: 11.3519\n",
      "Epoch 444/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 11.2499 - val_loss: 11.3400\n",
      "Epoch 445/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 11.2596 - val_loss: 11.4558\n",
      "Epoch 446/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 11.2428 - val_loss: 11.3104\n",
      "Epoch 447/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 11.2109 - val_loss: 11.2594\n",
      "Epoch 448/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 11.1484 - val_loss: 11.2842\n",
      "Epoch 449/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 11.1106 - val_loss: 11.2720\n",
      "Epoch 450/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 11.0803 - val_loss: 11.2210\n",
      "Epoch 451/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 11.0152 - val_loss: 11.0584\n",
      "Epoch 452/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 11.0009 - val_loss: 11.0632\n",
      "Epoch 453/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 11.0699 - val_loss: 11.1594\n",
      "Epoch 454/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 11.0858 - val_loss: 11.0321\n",
      "Epoch 455/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 10.9250 - val_loss: 10.9413\n",
      "Epoch 456/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 10.8989 - val_loss: 10.9572\n",
      "Epoch 457/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 10.8192 - val_loss: 10.9989\n",
      "Epoch 458/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 10.8781 - val_loss: 10.9120\n",
      "Epoch 459/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 10.7694 - val_loss: 10.7649\n",
      "Epoch 460/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 10.7037 - val_loss: 10.8441\n",
      "Epoch 461/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 10.8495 - val_loss: 10.9163\n",
      "Epoch 462/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 10.7985 - val_loss: 10.7426\n",
      "Epoch 463/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 10.6190 - val_loss: 10.6835\n",
      "Epoch 464/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 10.5746 - val_loss: 10.6432\n",
      "Epoch 465/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 10.5188 - val_loss: 10.6042\n",
      "Epoch 466/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 10.5754 - val_loss: 10.5551\n",
      "Epoch 467/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 10.4926 - val_loss: 10.5836\n",
      "Epoch 468/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 10.5069 - val_loss: 10.5243\n",
      "Epoch 469/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 10.4190 - val_loss: 10.4877\n",
      "Epoch 470/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 10.3490 - val_loss: 10.4866\n",
      "Epoch 471/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 10.4004 - val_loss: 10.4667\n",
      "Epoch 472/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 10.3196 - val_loss: 10.4217\n",
      "Epoch 473/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 10.2859 - val_loss: 10.4612\n",
      "Epoch 474/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 10.7102 - val_loss: 10.4783\n",
      "Epoch 475/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 10.3370 - val_loss: 10.2927\n",
      "Epoch 476/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 10.2130 - val_loss: 10.3489\n",
      "Epoch 477/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 10.3710 - val_loss: 10.2546\n",
      "Epoch 478/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 10.1591 - val_loss: 10.2433\n",
      "Epoch 479/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 10.1046 - val_loss: 10.1814\n",
      "Epoch 480/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 10.0890 - val_loss: 10.1721\n",
      "Epoch 481/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 10.0145 - val_loss: 10.0866\n",
      "Epoch 482/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 9.9649 - val_loss: 10.0235\n",
      "Epoch 483/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 9.9269 - val_loss: 10.0589\n",
      "Epoch 484/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 9.9436 - val_loss: 10.0155\n",
      "Epoch 485/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 9.8784 - val_loss: 9.9896\n",
      "Epoch 486/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 9.8502 - val_loss: 9.9985\n",
      "Epoch 487/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 9.8423 - val_loss: 9.9440\n",
      "Epoch 488/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 9.8165 - val_loss: 9.9171\n",
      "Epoch 489/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 9.7768 - val_loss: 9.8393\n",
      "Epoch 490/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 9.6908 - val_loss: 9.9098\n",
      "Epoch 491/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 9.6739 - val_loss: 9.7352\n",
      "Epoch 492/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 9.6541 - val_loss: 9.7690\n",
      "Epoch 493/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 9.6294 - val_loss: 9.6704\n",
      "Epoch 494/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 9.6299 - val_loss: 10.4022\n",
      "Epoch 495/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 9.9697 - val_loss: 9.7976\n",
      "Epoch 496/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 9.6172 - val_loss: 9.6881\n",
      "Epoch 497/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 9.5111 - val_loss: 9.5830\n",
      "Epoch 498/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 9.4769 - val_loss: 9.5503\n",
      "Epoch 499/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 9.4481 - val_loss: 9.5218\n",
      "Epoch 500/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 9.4212 - val_loss: 9.5291\n",
      "Epoch 501/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 9.3580 - val_loss: 9.5127\n",
      "Epoch 502/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 9.4993 - val_loss: 9.4043\n",
      "Epoch 503/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 9.3100 - val_loss: 9.4276\n",
      "Epoch 504/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 9.3352 - val_loss: 9.3607\n",
      "Epoch 505/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 9.2689 - val_loss: 9.2989\n",
      "Epoch 506/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 9.2007 - val_loss: 9.4131\n",
      "Epoch 507/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 9.4182 - val_loss: 9.3224\n",
      "Epoch 508/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 9.2163 - val_loss: 9.2307\n",
      "Epoch 509/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 9.0767 - val_loss: 9.3612\n",
      "Epoch 510/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 9.2777 - val_loss: 9.2022\n",
      "Epoch 511/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 9.0193 - val_loss: 9.1011\n",
      "Epoch 512/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 9.0339 - val_loss: 9.1005\n",
      "Epoch 513/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 8.9826 - val_loss: 9.1689\n",
      "Epoch 514/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 8.9603 - val_loss: 9.0576\n",
      "Epoch 515/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 8.9624 - val_loss: 8.9899\n",
      "Epoch 516/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 8.9546 - val_loss: 8.9820\n",
      "Epoch 517/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 8.8898 - val_loss: 8.9725\n",
      "Epoch 518/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 8.9282 - val_loss: 8.9050\n",
      "Epoch 519/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 8.8328 - val_loss: 9.0182\n",
      "Epoch 520/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 8.7797 - val_loss: 8.8200\n",
      "Epoch 521/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 8.8140 - val_loss: 8.9491\n",
      "Epoch 522/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 8.7413 - val_loss: 8.7863\n",
      "Epoch 523/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 8.6796 - val_loss: 8.7947\n",
      "Epoch 524/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 8.6274 - val_loss: 8.7222\n",
      "Epoch 525/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 8.5723 - val_loss: 8.6863\n",
      "Epoch 526/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 8.5892 - val_loss: 8.6319\n",
      "Epoch 527/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 8.5938 - val_loss: 8.7321\n",
      "Epoch 528/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 8.6319 - val_loss: 8.6224\n",
      "Epoch 529/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 8.4797 - val_loss: 8.5859\n",
      "Epoch 530/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 8.4287 - val_loss: 8.5034\n",
      "Epoch 531/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 8.4048 - val_loss: 8.5042\n",
      "Epoch 532/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 8.3514 - val_loss: 8.4162\n",
      "Epoch 533/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 8.3278 - val_loss: 8.4649\n",
      "Epoch 534/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 8.3616 - val_loss: 8.3719\n",
      "Epoch 535/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 8.4169 - val_loss: 8.3037\n",
      "Epoch 536/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 8.2707 - val_loss: 8.3892\n",
      "Epoch 537/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 8.2827 - val_loss: 8.2874\n",
      "Epoch 538/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 8.1970 - val_loss: 8.2369\n",
      "Epoch 539/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 8.1690 - val_loss: 8.2108\n",
      "Epoch 540/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 8.1221 - val_loss: 8.1709\n",
      "Epoch 541/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 8.0890 - val_loss: 8.2290\n",
      "Epoch 542/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 8.0879 - val_loss: 8.1316\n",
      "Epoch 543/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 8.0343 - val_loss: 8.2901\n",
      "Epoch 544/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 8.3520 - val_loss: 8.1030\n",
      "Epoch 545/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 7.9731 - val_loss: 8.1909\n",
      "Epoch 546/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 8.0014 - val_loss: 8.0877\n",
      "Epoch 547/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 7.9574 - val_loss: 7.9695\n",
      "Epoch 548/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 7.9065 - val_loss: 7.9533\n",
      "Epoch 549/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 1s 14ms/step - loss: 7.8589 - val_loss: 7.9467\n",
      "Epoch 550/1000\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 7.8203 - val_loss: 7.9899\n",
      "Epoch 551/1000\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 8.1112 - val_loss: 8.3308\n",
      "Epoch 552/1000\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 8.0746 - val_loss: 7.8537\n",
      "Epoch 553/1000\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 7.7893 - val_loss: 7.7901\n",
      "Epoch 554/1000\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 7.7558 - val_loss: 7.8547\n",
      "Epoch 555/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 7.8164 - val_loss: 7.7369\n",
      "Epoch 556/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 7.6537 - val_loss: 7.6646\n",
      "Epoch 557/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 7.6115 - val_loss: 7.6739\n",
      "Epoch 558/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 7.6380 - val_loss: 7.7317\n",
      "Epoch 559/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 7.5843 - val_loss: 7.6423\n",
      "Epoch 560/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 7.5266 - val_loss: 7.6560\n",
      "Epoch 561/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 7.5715 - val_loss: 7.7644\n",
      "Epoch 562/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 7.5600 - val_loss: 7.5804\n",
      "Epoch 563/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 7.5540 - val_loss: 7.5535\n",
      "Epoch 564/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 7.4551 - val_loss: 7.5202\n",
      "Epoch 565/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 7.5241 - val_loss: 7.4458\n",
      "Epoch 566/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 7.3838 - val_loss: 7.4405\n",
      "Epoch 567/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 7.3746 - val_loss: 7.4348\n",
      "Epoch 568/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 7.2765 - val_loss: 7.3599\n",
      "Epoch 569/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 7.2569 - val_loss: 7.2935\n",
      "Epoch 570/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 7.2080 - val_loss: 7.2700\n",
      "Epoch 571/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 7.2319 - val_loss: 7.2501\n",
      "Epoch 572/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 7.2183 - val_loss: 7.2328\n",
      "Epoch 573/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 7.1884 - val_loss: 7.2041\n",
      "Epoch 574/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 7.1386 - val_loss: 7.1466\n",
      "Epoch 575/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 7.2012 - val_loss: 7.1886\n",
      "Epoch 576/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 7.0670 - val_loss: 7.0932\n",
      "Epoch 577/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 7.0259 - val_loss: 7.0518\n",
      "Epoch 578/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 6.9836 - val_loss: 7.0164\n",
      "Epoch 579/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 7.0121 - val_loss: 7.4239\n",
      "Epoch 580/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 7.2043 - val_loss: 7.0990\n",
      "Epoch 581/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 7.1811 - val_loss: 7.0299\n",
      "Epoch 582/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 6.9270 - val_loss: 6.9194\n",
      "Epoch 583/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 6.8563 - val_loss: 6.8764\n",
      "Epoch 584/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 6.8902 - val_loss: 6.8931\n",
      "Epoch 585/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 6.8188 - val_loss: 6.8363\n",
      "Epoch 586/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 6.8176 - val_loss: 6.8028\n",
      "Epoch 587/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 6.7367 - val_loss: 6.7659\n",
      "Epoch 588/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 6.7407 - val_loss: 6.7330\n",
      "Epoch 589/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 6.7016 - val_loss: 6.7502\n",
      "Epoch 590/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 6.6762 - val_loss: 6.7371\n",
      "Epoch 591/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 6.6621 - val_loss: 6.6671\n",
      "Epoch 592/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 6.6110 - val_loss: 6.6007\n",
      "Epoch 593/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 6.5524 - val_loss: 6.5642\n",
      "Epoch 594/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 6.5603 - val_loss: 6.8941\n",
      "Epoch 595/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 6.7708 - val_loss: 6.6082\n",
      "Epoch 596/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 6.5656 - val_loss: 6.5410\n",
      "Epoch 597/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 6.4489 - val_loss: 6.5219\n",
      "Epoch 598/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 6.6366 - val_loss: 6.4533\n",
      "Epoch 599/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 6.4985 - val_loss: 6.4891\n",
      "Epoch 600/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 6.4485 - val_loss: 6.4511\n",
      "Epoch 601/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 6.3748 - val_loss: 6.4569\n",
      "Epoch 602/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 6.4059 - val_loss: 6.3761\n",
      "Epoch 603/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 6.3288 - val_loss: 6.5572\n",
      "Epoch 604/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 7.0664 - val_loss: 6.4932\n",
      "Epoch 605/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 6.4644 - val_loss: 6.4456\n",
      "Epoch 606/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 6.3842 - val_loss: 6.4841\n",
      "Epoch 607/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 6.4422 - val_loss: 6.2961\n",
      "Epoch 608/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 6.1944 - val_loss: 6.2229\n",
      "Epoch 609/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 6.1702 - val_loss: 6.1971\n",
      "Epoch 610/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 6.1421 - val_loss: 6.2540\n",
      "Epoch 611/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 6.1103 - val_loss: 6.1268\n",
      "Epoch 612/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 6.1404 - val_loss: 6.1097\n",
      "Epoch 613/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 6.1367 - val_loss: 6.0880\n",
      "Epoch 614/1000\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 6.0527 - val_loss: 6.0743\n",
      "Epoch 615/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 6.0706 - val_loss: 6.0029\n",
      "Epoch 616/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.9695 - val_loss: 6.0139\n",
      "Epoch 617/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 6.0053 - val_loss: 6.0245\n",
      "Epoch 618/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.9570 - val_loss: 6.0618\n",
      "Epoch 619/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 5.8870 - val_loss: 5.9444\n",
      "Epoch 620/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.8674 - val_loss: 5.8903\n",
      "Epoch 621/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.8238 - val_loss: 5.8975\n",
      "Epoch 622/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 5.8249 - val_loss: 5.8436\n",
      "Epoch 623/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 5.8489 - val_loss: 6.4584\n",
      "Epoch 624/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 6.9911 - val_loss: 6.1424\n",
      "Epoch 625/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 6.0482 - val_loss: 5.8652\n",
      "Epoch 626/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.8934 - val_loss: 5.7713\n",
      "Epoch 627/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.9955 - val_loss: 5.7261\n",
      "Epoch 628/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.7379 - val_loss: 5.7842\n",
      "Epoch 629/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.7306 - val_loss: 5.7669\n",
      "Epoch 630/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.7293 - val_loss: 5.6592\n",
      "Epoch 631/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.6165 - val_loss: 5.6276\n",
      "Epoch 632/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.6518 - val_loss: 5.6995\n",
      "Epoch 633/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.7393 - val_loss: 5.6012\n",
      "Epoch 634/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.5910 - val_loss: 5.5411\n",
      "Epoch 635/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.5645 - val_loss: 5.4893\n",
      "Epoch 636/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.4780 - val_loss: 5.4625\n",
      "Epoch 637/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.4787 - val_loss: 5.4676\n",
      "Epoch 638/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.4719 - val_loss: 5.4896\n",
      "Epoch 639/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.5117 - val_loss: 5.5994\n",
      "Epoch 640/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 5.4192 - val_loss: 5.4309\n",
      "Epoch 641/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.3821 - val_loss: 5.3712\n",
      "Epoch 642/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.3617 - val_loss: 5.3717\n",
      "Epoch 643/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.3540 - val_loss: 5.3602\n",
      "Epoch 644/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.4098 - val_loss: 5.3473\n",
      "Epoch 645/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.2830 - val_loss: 5.3995\n",
      "Epoch 646/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.8182 - val_loss: 5.3761\n",
      "Epoch 647/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.2915 - val_loss: 5.3983\n",
      "Epoch 648/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.4170 - val_loss: 5.2713\n",
      "Epoch 649/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.2285 - val_loss: 5.2264\n",
      "Epoch 650/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.1675 - val_loss: 5.2081\n",
      "Epoch 651/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.1973 - val_loss: 5.1944\n",
      "Epoch 652/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.1376 - val_loss: 5.1979\n",
      "Epoch 653/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.1927 - val_loss: 5.3902\n",
      "Epoch 654/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.4624 - val_loss: 5.1381\n",
      "Epoch 655/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 5.1571 - val_loss: 5.1140\n",
      "Epoch 656/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.0732 - val_loss: 5.1081\n",
      "Epoch 657/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 5.0823 - val_loss: 5.0016\n",
      "Epoch 658/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 5.0074 - val_loss: 5.4574\n",
      "Epoch 659/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.5732 - val_loss: 5.0440\n",
      "Epoch 660/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.9787 - val_loss: 5.0181\n",
      "Epoch 661/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.9688 - val_loss: 4.9700\n",
      "Epoch 662/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.9328 - val_loss: 4.9505\n",
      "Epoch 663/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.8974 - val_loss: 4.9763\n",
      "Epoch 664/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.7916 - val_loss: 5.1119\n",
      "Epoch 665/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.0081 - val_loss: 4.9098\n",
      "Epoch 666/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.9125 - val_loss: 4.8280\n",
      "Epoch 667/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.8746 - val_loss: 4.7997\n",
      "Epoch 668/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.8168 - val_loss: 4.7898\n",
      "Epoch 669/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.7504 - val_loss: 4.7987\n",
      "Epoch 670/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 4.7763 - val_loss: 4.7384\n",
      "Epoch 671/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.7467 - val_loss: 4.7061\n",
      "Epoch 672/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.7696 - val_loss: 4.6971\n",
      "Epoch 673/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.6950 - val_loss: 4.6881\n",
      "Epoch 674/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 4.9305 - val_loss: 4.6775\n",
      "Epoch 675/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.6699 - val_loss: 4.7405\n",
      "Epoch 676/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.6793 - val_loss: 4.6327\n",
      "Epoch 677/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.5930 - val_loss: 4.5784\n",
      "Epoch 678/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.5939 - val_loss: 4.7523\n",
      "Epoch 679/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.6313 - val_loss: 4.5454\n",
      "Epoch 680/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 4.5361 - val_loss: 4.5417\n",
      "Epoch 681/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.5425 - val_loss: 4.4951\n",
      "Epoch 682/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.5494 - val_loss: 4.5165\n",
      "Epoch 683/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.4831 - val_loss: 4.4920\n",
      "Epoch 684/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.5161 - val_loss: 4.4278\n",
      "Epoch 685/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.5282 - val_loss: 4.4739\n",
      "Epoch 686/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 4.4658 - val_loss: 4.4365\n",
      "Epoch 687/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 4.4441 - val_loss: 4.7284\n",
      "Epoch 688/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.3638 - val_loss: 4.5734\n",
      "Epoch 689/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.6782 - val_loss: 4.4470\n",
      "Epoch 690/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.5115 - val_loss: 4.4745\n",
      "Epoch 691/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.4674 - val_loss: 4.4414\n",
      "Epoch 692/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.5097 - val_loss: 4.3756\n",
      "Epoch 693/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 4.3706 - val_loss: 4.3639\n",
      "Epoch 694/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.4126 - val_loss: 4.4577\n",
      "Epoch 695/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.3747 - val_loss: 4.2391\n",
      "Epoch 696/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.3202 - val_loss: 4.2467\n",
      "Epoch 697/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.2627 - val_loss: 4.2276\n",
      "Epoch 698/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.2498 - val_loss: 4.3015\n",
      "Epoch 699/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 4.2444 - val_loss: 4.1350\n",
      "Epoch 700/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.2408 - val_loss: 4.2550\n",
      "Epoch 701/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.2249 - val_loss: 4.1861\n",
      "Epoch 702/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 4.2193 - val_loss: 4.1754\n",
      "Epoch 703/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.2234 - val_loss: 4.2146\n",
      "Epoch 704/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.3508 - val_loss: 4.0876\n",
      "Epoch 705/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.1193 - val_loss: 4.0495\n",
      "Epoch 706/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.1269 - val_loss: 4.3123\n",
      "Epoch 707/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 1s 14ms/step - loss: 4.1311 - val_loss: 4.2189\n",
      "Epoch 708/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.2665 - val_loss: 4.1268\n",
      "Epoch 709/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 4.1017 - val_loss: 4.0188\n",
      "Epoch 710/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.9898 - val_loss: 4.0245\n",
      "Epoch 711/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.0293 - val_loss: 3.8988\n",
      "Epoch 712/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.0032 - val_loss: 3.9004\n",
      "Epoch 713/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 3.9731 - val_loss: 4.0417\n",
      "Epoch 714/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 4.0176 - val_loss: 3.9319\n",
      "Epoch 715/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.9539 - val_loss: 3.8784\n",
      "Epoch 716/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 3.9238 - val_loss: 4.0558\n",
      "Epoch 717/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 3.9553 - val_loss: 3.8655\n",
      "Epoch 718/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.8778 - val_loss: 3.8084\n",
      "Epoch 719/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.8516 - val_loss: 3.9397\n",
      "Epoch 720/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.9692 - val_loss: 3.8895\n",
      "Epoch 721/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 3.8401 - val_loss: 3.7658\n",
      "Epoch 722/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 3.8021 - val_loss: 3.7738\n",
      "Epoch 723/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.8418 - val_loss: 3.8109\n",
      "Epoch 724/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 4.0602 - val_loss: 3.8409\n",
      "Epoch 725/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.9465 - val_loss: 3.8217\n",
      "Epoch 726/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 3.8558 - val_loss: 3.8172\n",
      "Epoch 727/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.8931 - val_loss: 3.6823\n",
      "Epoch 728/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.7635 - val_loss: 3.7735\n",
      "Epoch 729/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.7727 - val_loss: 3.5977\n",
      "Epoch 730/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 3.7362 - val_loss: 3.7806\n",
      "Epoch 731/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.7170 - val_loss: 3.6356\n",
      "Epoch 732/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.6829 - val_loss: 3.6383\n",
      "Epoch 733/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.6801 - val_loss: 3.5708\n",
      "Epoch 734/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 3.6058 - val_loss: 3.7127\n",
      "Epoch 735/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.8816 - val_loss: 3.5598\n",
      "Epoch 736/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.6281 - val_loss: 3.5174\n",
      "Epoch 737/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.5993 - val_loss: 3.5260\n",
      "Epoch 738/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.5950 - val_loss: 3.5302\n",
      "Epoch 739/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.5694 - val_loss: 3.7400\n",
      "Epoch 740/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.8902 - val_loss: 3.6171\n",
      "Epoch 741/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.5489 - val_loss: 3.4955\n",
      "Epoch 742/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.5953 - val_loss: 3.4348\n",
      "Epoch 743/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.5177 - val_loss: 3.4204\n",
      "Epoch 744/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.5062 - val_loss: 3.4688\n",
      "Epoch 745/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 3.5780 - val_loss: 3.4574\n",
      "Epoch 746/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 3.5219 - val_loss: 3.4863\n",
      "Epoch 747/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.4448 - val_loss: 3.4814\n",
      "Epoch 748/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.4559 - val_loss: 3.3465\n",
      "Epoch 749/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.4423 - val_loss: 3.3278\n",
      "Epoch 750/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.4253 - val_loss: 3.4086\n",
      "Epoch 751/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.4320 - val_loss: 3.4546\n",
      "Epoch 752/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.3931 - val_loss: 3.4472\n",
      "Epoch 753/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.6423 - val_loss: 3.6550\n",
      "Epoch 754/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.5979 - val_loss: 3.2945\n",
      "Epoch 755/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.4224 - val_loss: 3.3228\n",
      "Epoch 756/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.3231 - val_loss: 3.2570\n",
      "Epoch 757/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.3819 - val_loss: 3.2586\n",
      "Epoch 758/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.3001 - val_loss: 3.2689\n",
      "Epoch 759/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.2675 - val_loss: 3.1920\n",
      "Epoch 760/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.2450 - val_loss: 3.2288\n",
      "Epoch 761/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.3723 - val_loss: 3.2108\n",
      "Epoch 762/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.2746 - val_loss: 3.3847\n",
      "Epoch 763/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.2668 - val_loss: 3.1818\n",
      "Epoch 764/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.2002 - val_loss: 3.2260\n",
      "Epoch 765/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.2900 - val_loss: 3.1579\n",
      "Epoch 766/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.1947 - val_loss: 3.1242\n",
      "Epoch 767/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.1695 - val_loss: 3.0749\n",
      "Epoch 768/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 3.1945 - val_loss: 3.1173\n",
      "Epoch 769/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.2355 - val_loss: 3.0685\n",
      "Epoch 770/1000\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 3.1934 - val_loss: 3.1322\n",
      "Epoch 771/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 3.0918 - val_loss: 3.1281\n",
      "Epoch 772/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 3.1292 - val_loss: 3.0890\n",
      "Epoch 773/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.1620 - val_loss: 3.1022\n",
      "Epoch 774/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.0887 - val_loss: 3.1092\n",
      "Epoch 775/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.1359 - val_loss: 3.0111\n",
      "Epoch 776/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.0612 - val_loss: 3.0275\n",
      "Epoch 777/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.0304 - val_loss: 2.9900\n",
      "Epoch 778/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.0374 - val_loss: 2.9638\n",
      "Epoch 779/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.9954 - val_loss: 3.4228\n",
      "Epoch 780/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.5860 - val_loss: 3.0697\n",
      "Epoch 781/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.0085 - val_loss: 3.0245\n",
      "Epoch 782/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.0723 - val_loss: 2.9258\n",
      "Epoch 783/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.9974 - val_loss: 2.9991\n",
      "Epoch 784/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.9912 - val_loss: 2.9295\n",
      "Epoch 785/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.9172 - val_loss: 2.8886\n",
      "Epoch 786/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.0348 - val_loss: 2.9644\n",
      "Epoch 787/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.9499 - val_loss: 2.9435\n",
      "Epoch 788/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.9954 - val_loss: 2.8995\n",
      "Epoch 789/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.9295 - val_loss: 2.9686\n",
      "Epoch 790/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.0344 - val_loss: 2.9167\n",
      "Epoch 791/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.0425 - val_loss: 2.9045\n",
      "Epoch 792/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.9157 - val_loss: 2.9198\n",
      "Epoch 793/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.9852 - val_loss: 2.8640\n",
      "Epoch 794/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.8502 - val_loss: 2.9173\n",
      "Epoch 795/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.8479 - val_loss: 2.8827\n",
      "Epoch 796/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 2.8474 - val_loss: 2.8336\n",
      "Epoch 797/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 2.8377 - val_loss: 2.7957\n",
      "Epoch 798/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 2.8394 - val_loss: 2.9477\n",
      "Epoch 799/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.9366 - val_loss: 2.7951\n",
      "Epoch 800/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.7785 - val_loss: 2.8721\n",
      "Epoch 801/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 2.8412 - val_loss: 2.8226\n",
      "Epoch 802/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 2.8070 - val_loss: 2.8616\n",
      "Epoch 803/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 2.9855 - val_loss: 2.7393\n",
      "Epoch 804/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.7933 - val_loss: 2.7172\n",
      "Epoch 805/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.8150 - val_loss: 2.7224\n",
      "Epoch 806/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.7825 - val_loss: 2.8283\n",
      "Epoch 807/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 3.1980 - val_loss: 2.7456\n",
      "Epoch 808/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 2.7388 - val_loss: 2.6543\n",
      "Epoch 809/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.7301 - val_loss: 2.6494\n",
      "Epoch 810/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.6977 - val_loss: 2.6555\n",
      "Epoch 811/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.7111 - val_loss: 2.7072\n",
      "Epoch 812/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.7216 - val_loss: 2.6628\n",
      "Epoch 813/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.6448 - val_loss: 2.8043\n",
      "Epoch 814/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.6252 - val_loss: 2.9379\n",
      "Epoch 815/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.8237 - val_loss: 2.7411\n",
      "Epoch 816/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.7444 - val_loss: 2.6955\n",
      "Epoch 817/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.6916 - val_loss: 2.6858\n",
      "Epoch 818/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.7693 - val_loss: 2.6668\n",
      "Epoch 819/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.6400 - val_loss: 2.6776\n",
      "Epoch 820/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.6586 - val_loss: 2.7057\n",
      "Epoch 821/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.5997 - val_loss: 2.5714\n",
      "Epoch 822/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.7356 - val_loss: 2.9561\n",
      "Epoch 823/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.8591 - val_loss: 2.9606\n",
      "Epoch 824/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.9734 - val_loss: 2.6572\n",
      "Epoch 825/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.6299 - val_loss: 2.5430\n",
      "Epoch 826/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.5816 - val_loss: 2.5252\n",
      "Epoch 827/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.5255 - val_loss: 2.4988\n",
      "Epoch 828/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.5597 - val_loss: 2.5022\n",
      "Epoch 829/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.5200 - val_loss: 2.5562\n",
      "Epoch 830/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.4909 - val_loss: 2.5235\n",
      "Epoch 831/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.5127 - val_loss: 2.5121\n",
      "Epoch 832/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.5046 - val_loss: 2.4610\n",
      "Epoch 833/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.5064 - val_loss: 2.4821\n",
      "Epoch 834/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.4718 - val_loss: 2.4385\n",
      "Epoch 835/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.4553 - val_loss: 2.4293\n",
      "Epoch 836/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.4414 - val_loss: 2.4143\n",
      "Epoch 837/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.4000 - val_loss: 2.4197\n",
      "Epoch 838/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.4006 - val_loss: 2.4897\n",
      "Epoch 839/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.3995 - val_loss: 2.4063\n",
      "Epoch 840/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.4074 - val_loss: 2.4184\n",
      "Epoch 841/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.4255 - val_loss: 2.3909\n",
      "Epoch 842/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.4107 - val_loss: 2.3781\n",
      "Epoch 843/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.4090 - val_loss: 2.4559\n",
      "Epoch 844/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.4711 - val_loss: 2.5665\n",
      "Epoch 845/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.5284 - val_loss: 2.4405\n",
      "Epoch 846/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.4167 - val_loss: 2.3966\n",
      "Epoch 847/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.3480 - val_loss: 2.3753\n",
      "Epoch 848/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.3986 - val_loss: 2.3572\n",
      "Epoch 849/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.3438 - val_loss: 2.4302\n",
      "Epoch 850/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.4373 - val_loss: 2.4317\n",
      "Epoch 851/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.8069 - val_loss: 2.3691\n",
      "Epoch 852/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.3498 - val_loss: 2.4122\n",
      "Epoch 853/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.3580 - val_loss: 2.3151\n",
      "Epoch 854/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.3444 - val_loss: 2.4240\n",
      "Epoch 855/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.6097 - val_loss: 2.4643\n",
      "Epoch 856/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.1841 - val_loss: 2.4404\n",
      "Epoch 857/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.4125 - val_loss: 2.4262\n",
      "Epoch 858/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.3574 - val_loss: 2.3281\n",
      "Epoch 859/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.2826 - val_loss: 2.3853\n",
      "Epoch 860/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.4197 - val_loss: 2.2566\n",
      "Epoch 861/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.2491 - val_loss: 2.3190\n",
      "Epoch 862/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.3187 - val_loss: 2.2839\n",
      "Epoch 863/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.2370 - val_loss: 2.2479\n",
      "Epoch 864/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.2424 - val_loss: 2.4516\n",
      "Epoch 865/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 1s 13ms/step - loss: 2.2323 - val_loss: 2.3528\n",
      "Epoch 866/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.2195 - val_loss: 2.2085\n",
      "Epoch 867/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.2090 - val_loss: 2.2102\n",
      "Epoch 868/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.2224 - val_loss: 2.3333\n",
      "Epoch 869/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.2051 - val_loss: 2.2364\n",
      "Epoch 870/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.1479 - val_loss: 2.2913\n",
      "Epoch 871/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.2752 - val_loss: 2.2656\n",
      "Epoch 872/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.1562 - val_loss: 2.4344\n",
      "Epoch 873/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.5453 - val_loss: 2.3396\n",
      "Epoch 874/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.1734 - val_loss: 2.1907\n",
      "Epoch 875/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.1427 - val_loss: 2.1990\n",
      "Epoch 876/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.1003 - val_loss: 2.1883\n",
      "Epoch 877/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.1105 - val_loss: 2.1771\n",
      "Epoch 878/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.0976 - val_loss: 2.1655\n",
      "Epoch 879/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.0766 - val_loss: 2.1109\n",
      "Epoch 880/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.0448 - val_loss: 2.1536\n",
      "Epoch 881/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.1796 - val_loss: 2.1474\n",
      "Epoch 882/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.0725 - val_loss: 2.0916\n",
      "Epoch 883/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.0743 - val_loss: 2.0939\n",
      "Epoch 884/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.1055 - val_loss: 2.1264\n",
      "Epoch 885/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.0957 - val_loss: 2.0936\n",
      "Epoch 886/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.1037 - val_loss: 2.0367\n",
      "Epoch 887/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.0325 - val_loss: 2.0810\n",
      "Epoch 888/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.0035 - val_loss: 2.0578\n",
      "Epoch 889/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.9889 - val_loss: 2.0338\n",
      "Epoch 890/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.9773 - val_loss: 2.0560\n",
      "Epoch 891/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.0407 - val_loss: 2.1678\n",
      "Epoch 892/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.0269 - val_loss: 2.0615\n",
      "Epoch 893/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.1192 - val_loss: 2.0579\n",
      "Epoch 894/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.0223 - val_loss: 2.0015\n",
      "Epoch 895/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.0110 - val_loss: 2.0705\n",
      "Epoch 896/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.1291 - val_loss: 2.0583\n",
      "Epoch 897/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.9899 - val_loss: 2.0044\n",
      "Epoch 898/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.9894 - val_loss: 1.9669\n",
      "Epoch 899/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.0230 - val_loss: 2.0244\n",
      "Epoch 900/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.9502 - val_loss: 2.0969\n",
      "Epoch 901/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.9448 - val_loss: 2.0421\n",
      "Epoch 902/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 2.0177 - val_loss: 1.9758\n",
      "Epoch 903/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.9193 - val_loss: 2.1278\n",
      "Epoch 904/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.1452 - val_loss: 2.1197\n",
      "Epoch 905/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.9870 - val_loss: 2.0211\n",
      "Epoch 906/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.9003 - val_loss: 2.0129\n",
      "Epoch 907/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.9468 - val_loss: 1.9831\n",
      "Epoch 908/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.9086 - val_loss: 1.9611\n",
      "Epoch 909/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.9391 - val_loss: 2.0206\n",
      "Epoch 910/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.8805 - val_loss: 1.9556\n",
      "Epoch 911/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.9410 - val_loss: 2.0338\n",
      "Epoch 912/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.1808 - val_loss: 2.0624\n",
      "Epoch 913/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.9841 - val_loss: 1.9900\n",
      "Epoch 914/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.8216 - val_loss: 1.9394\n",
      "Epoch 915/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.8640 - val_loss: 2.1042\n",
      "Epoch 916/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.0145 - val_loss: 2.1244\n",
      "Epoch 917/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.0404 - val_loss: 1.9502\n",
      "Epoch 918/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.8395 - val_loss: 1.9004\n",
      "Epoch 919/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 1.9022 - val_loss: 2.1182\n",
      "Epoch 920/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.0474 - val_loss: 1.9758\n",
      "Epoch 921/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.8439 - val_loss: 1.9486\n",
      "Epoch 922/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.8247 - val_loss: 2.0858\n",
      "Epoch 923/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.2599 - val_loss: 1.9573\n",
      "Epoch 924/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.9276 - val_loss: 1.9119\n",
      "Epoch 925/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.8081 - val_loss: 1.9602\n",
      "Epoch 926/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.8160 - val_loss: 1.9990\n",
      "Epoch 927/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.8556 - val_loss: 1.8225\n",
      "Epoch 928/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.7705 - val_loss: 1.8707\n",
      "Epoch 929/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.8734 - val_loss: 1.9278\n",
      "Epoch 930/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.8454 - val_loss: 1.8856\n",
      "Epoch 931/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.9423 - val_loss: 1.9905\n",
      "Epoch 932/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.7708 - val_loss: 1.9180\n",
      "Epoch 933/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.9238 - val_loss: 2.1610\n",
      "Epoch 934/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.0966 - val_loss: 1.9571\n",
      "Epoch 935/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.8192 - val_loss: 2.1343\n",
      "Epoch 936/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.3488 - val_loss: 1.9110\n",
      "Epoch 937/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.7641 - val_loss: 1.8765\n",
      "Epoch 938/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.7711 - val_loss: 1.8062\n",
      "Epoch 939/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.7327 - val_loss: 1.7956\n",
      "Epoch 940/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.8051 - val_loss: 1.7855\n",
      "Epoch 941/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.7702 - val_loss: 1.8234\n",
      "Epoch 942/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.7260 - val_loss: 1.9227\n",
      "Epoch 943/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.7052 - val_loss: 1.8346\n",
      "Epoch 944/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.7662 - val_loss: 1.8052\n",
      "Epoch 945/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.6916 - val_loss: 1.7884\n",
      "Epoch 946/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.6521 - val_loss: 1.7424\n",
      "Epoch 947/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.6726 - val_loss: 1.8564\n",
      "Epoch 948/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.6764 - val_loss: 1.8787\n",
      "Epoch 949/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.7999 - val_loss: 1.9192\n",
      "Epoch 950/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.8609 - val_loss: 1.8582\n",
      "Epoch 951/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.6608 - val_loss: 1.7593\n",
      "Epoch 952/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.7201 - val_loss: 1.7643\n",
      "Epoch 953/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.6677 - val_loss: 1.7980\n",
      "Epoch 954/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.7314 - val_loss: 1.7464\n",
      "Epoch 955/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.7250 - val_loss: 1.7649\n",
      "Epoch 956/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.6786 - val_loss: 1.7672\n",
      "Epoch 957/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.6496 - val_loss: 1.7560\n",
      "Epoch 958/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.6681 - val_loss: 1.8493\n",
      "Epoch 959/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 1.6360 - val_loss: 1.7321\n",
      "Epoch 960/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 1.6330 - val_loss: 1.7570\n",
      "Epoch 961/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 1.6327 - val_loss: 1.8240\n",
      "Epoch 962/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 1.6329 - val_loss: 1.7046\n",
      "Epoch 963/1000\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 1.6180 - val_loss: 1.6983\n",
      "Epoch 964/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 1.6439 - val_loss: 1.7159\n",
      "Epoch 965/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 1.6238 - val_loss: 1.6993\n",
      "Epoch 966/1000\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 1.6642 - val_loss: 1.8034\n",
      "Epoch 967/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 1.6847 - val_loss: 1.7653\n",
      "Epoch 968/1000\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 1.6763 - val_loss: 1.8056\n",
      "Epoch 969/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 1.6243 - val_loss: 1.7344\n",
      "Epoch 970/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 1.6295 - val_loss: 1.7708\n",
      "Epoch 971/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 1.6489 - val_loss: 1.7500\n",
      "Epoch 972/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 1.5685 - val_loss: 1.7439\n",
      "Epoch 973/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 1.5649 - val_loss: 1.6736\n",
      "Epoch 974/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 1.5568 - val_loss: 1.6453\n",
      "Epoch 975/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 1.5273 - val_loss: 1.6980\n",
      "Epoch 976/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 1.5136 - val_loss: 1.7754\n",
      "Epoch 977/1000\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 1.8175 - val_loss: 1.6840\n",
      "Epoch 978/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 1.5402 - val_loss: 1.7105\n",
      "Epoch 979/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 1.6920 - val_loss: 1.7605\n",
      "Epoch 980/1000\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 1.5767 - val_loss: 1.7164\n",
      "Epoch 981/1000\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 1.5613 - val_loss: 1.7022\n",
      "Epoch 982/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 1.5206 - val_loss: 1.7532\n",
      "Epoch 983/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 1.5787 - val_loss: 1.6531\n",
      "Epoch 984/1000\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 1.5986 - val_loss: 1.7122\n",
      "Epoch 985/1000\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 1.6724 - val_loss: 1.6361\n",
      "Epoch 986/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 1.5496 - val_loss: 1.7367\n",
      "Epoch 987/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 1.5477 - val_loss: 1.6549\n",
      "Epoch 988/1000\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 1.5265 - val_loss: 1.7601\n",
      "Epoch 989/1000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 1.5348 - val_loss: 1.6194\n",
      "Epoch 990/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.6375 - val_loss: 1.6211\n",
      "Epoch 991/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.5030 - val_loss: 1.6578\n",
      "Epoch 992/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.4753 - val_loss: 1.6668\n",
      "Epoch 993/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.4803 - val_loss: 1.6935\n",
      "Epoch 994/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.5548 - val_loss: 1.6498\n",
      "Epoch 995/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.5489 - val_loss: 1.7216\n",
      "Epoch 996/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.6975 - val_loss: 1.6961\n",
      "Epoch 997/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.6195 - val_loss: 1.6527\n",
      "Epoch 998/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.4936 - val_loss: 1.6171\n",
      "Epoch 999/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.4298 - val_loss: 1.6160\n",
      "Epoch 1000/1000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.4570 - val_loss: 1.5849\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1.5849\n",
      "MSE: 1.5848628282546997\n"
     ]
    }
   ],
   "source": [
    "model = Sequential() \n",
    "model.add(LSTM(256, return_sequences=True, input_shape=(X_train.shape[1], 1))) \n",
    "model.add(Dropout(0.2)) \n",
    "\n",
    "model.add(LSTM(256, return_sequences=True)) \n",
    "model.add(Conv1D(256, 3, activation='relu'))  # Added Convolutional layer \n",
    "model.add(Dropout(0.2)) \n",
    "\n",
    "model.add(LSTM(128)) \n",
    "model.add(BatchNormalization()) \n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Reshape((1, 64)))  # Reshape input to (batch_size, timesteps, input_dim)\n",
    "model.add(GRU(32, recurrent_activation='sigmoid', activation='tanh'))  # Changed to GRU \n",
    "\n",
    "model.add(Dense(3)) \n",
    "\n",
    "adam = Adam(lr=0.0001) \n",
    "# Or RMSprop(lr=0.0001) \n",
    "model.compile(loss='huber_loss', optimizer=adam) \n",
    "\n",
    "early_stopping = EarlyStopping(patience=50) \n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping]) \n",
    "\n",
    "mse = model.evaluate(X_test, y_test) \n",
    "print(\"MSE:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3245019c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "216178da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GRU\n",
    "from keras.layers import Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88467b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
