{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_1_Scattered_iReg3.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101.870132</td>\n",
       "      <td>134.252574</td>\n",
       "      <td>72.801390</td>\n",
       "      <td>108.446568</td>\n",
       "      <td>130.203502</td>\n",
       "      <td>150.760073</td>\n",
       "      <td>103.508252</td>\n",
       "      <td>125.193887</td>\n",
       "      <td>89.453295</td>\n",
       "      <td>97.318384</td>\n",
       "      <td>...</td>\n",
       "      <td>81.685404</td>\n",
       "      <td>84.830110</td>\n",
       "      <td>86.513881</td>\n",
       "      <td>81.048996</td>\n",
       "      <td>114.964811</td>\n",
       "      <td>120.010616</td>\n",
       "      <td>103.909997</td>\n",
       "      <td>133.568532</td>\n",
       "      <td>57.626093</td>\n",
       "      <td>109.708209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101.656974</td>\n",
       "      <td>133.421922</td>\n",
       "      <td>76.037698</td>\n",
       "      <td>115.578180</td>\n",
       "      <td>124.237134</td>\n",
       "      <td>144.797812</td>\n",
       "      <td>106.645699</td>\n",
       "      <td>137.372609</td>\n",
       "      <td>92.314999</td>\n",
       "      <td>112.314087</td>\n",
       "      <td>...</td>\n",
       "      <td>81.526583</td>\n",
       "      <td>92.908051</td>\n",
       "      <td>94.438277</td>\n",
       "      <td>89.628271</td>\n",
       "      <td>114.498751</td>\n",
       "      <td>106.887589</td>\n",
       "      <td>99.505693</td>\n",
       "      <td>128.544662</td>\n",
       "      <td>67.730350</td>\n",
       "      <td>113.436964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102.889598</td>\n",
       "      <td>140.640194</td>\n",
       "      <td>71.553137</td>\n",
       "      <td>106.871465</td>\n",
       "      <td>123.654012</td>\n",
       "      <td>148.446127</td>\n",
       "      <td>103.789337</td>\n",
       "      <td>135.667714</td>\n",
       "      <td>99.182335</td>\n",
       "      <td>106.232463</td>\n",
       "      <td>...</td>\n",
       "      <td>75.930487</td>\n",
       "      <td>82.432658</td>\n",
       "      <td>87.572150</td>\n",
       "      <td>90.919428</td>\n",
       "      <td>116.186110</td>\n",
       "      <td>121.150696</td>\n",
       "      <td>96.193748</td>\n",
       "      <td>134.116483</td>\n",
       "      <td>68.863500</td>\n",
       "      <td>116.446807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.508164</td>\n",
       "      <td>130.788193</td>\n",
       "      <td>73.179060</td>\n",
       "      <td>122.566756</td>\n",
       "      <td>128.558120</td>\n",
       "      <td>144.121205</td>\n",
       "      <td>102.460744</td>\n",
       "      <td>129.928887</td>\n",
       "      <td>86.763744</td>\n",
       "      <td>106.168512</td>\n",
       "      <td>...</td>\n",
       "      <td>79.984057</td>\n",
       "      <td>99.957787</td>\n",
       "      <td>93.313344</td>\n",
       "      <td>84.668294</td>\n",
       "      <td>111.953201</td>\n",
       "      <td>119.676628</td>\n",
       "      <td>106.414441</td>\n",
       "      <td>137.948662</td>\n",
       "      <td>69.634344</td>\n",
       "      <td>114.024685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>104.073940</td>\n",
       "      <td>127.217426</td>\n",
       "      <td>69.730286</td>\n",
       "      <td>115.690253</td>\n",
       "      <td>120.920018</td>\n",
       "      <td>148.666096</td>\n",
       "      <td>116.786233</td>\n",
       "      <td>139.061346</td>\n",
       "      <td>83.559242</td>\n",
       "      <td>103.091764</td>\n",
       "      <td>...</td>\n",
       "      <td>75.279364</td>\n",
       "      <td>87.349475</td>\n",
       "      <td>97.655142</td>\n",
       "      <td>89.118820</td>\n",
       "      <td>126.637608</td>\n",
       "      <td>114.886056</td>\n",
       "      <td>101.361093</td>\n",
       "      <td>126.482809</td>\n",
       "      <td>66.133931</td>\n",
       "      <td>109.168340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>126.520010</td>\n",
       "      <td>110.917507</td>\n",
       "      <td>102.822108</td>\n",
       "      <td>62.853700</td>\n",
       "      <td>149.747652</td>\n",
       "      <td>127.099840</td>\n",
       "      <td>123.942335</td>\n",
       "      <td>108.196626</td>\n",
       "      <td>107.105731</td>\n",
       "      <td>96.441980</td>\n",
       "      <td>...</td>\n",
       "      <td>91.496394</td>\n",
       "      <td>121.729389</td>\n",
       "      <td>87.948166</td>\n",
       "      <td>77.602308</td>\n",
       "      <td>127.656991</td>\n",
       "      <td>114.668824</td>\n",
       "      <td>127.756278</td>\n",
       "      <td>109.362652</td>\n",
       "      <td>102.983525</td>\n",
       "      <td>78.077730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>122.656116</td>\n",
       "      <td>114.785269</td>\n",
       "      <td>98.718438</td>\n",
       "      <td>76.449249</td>\n",
       "      <td>152.660776</td>\n",
       "      <td>140.174139</td>\n",
       "      <td>136.835759</td>\n",
       "      <td>113.267986</td>\n",
       "      <td>104.631338</td>\n",
       "      <td>98.998328</td>\n",
       "      <td>...</td>\n",
       "      <td>92.880258</td>\n",
       "      <td>108.747017</td>\n",
       "      <td>88.541794</td>\n",
       "      <td>75.344392</td>\n",
       "      <td>125.557441</td>\n",
       "      <td>111.031434</td>\n",
       "      <td>134.494231</td>\n",
       "      <td>116.813742</td>\n",
       "      <td>112.599318</td>\n",
       "      <td>79.992646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>126.597748</td>\n",
       "      <td>119.491178</td>\n",
       "      <td>105.538634</td>\n",
       "      <td>75.394449</td>\n",
       "      <td>145.766322</td>\n",
       "      <td>143.595590</td>\n",
       "      <td>129.875574</td>\n",
       "      <td>120.944104</td>\n",
       "      <td>106.966013</td>\n",
       "      <td>96.617547</td>\n",
       "      <td>...</td>\n",
       "      <td>89.648431</td>\n",
       "      <td>106.485343</td>\n",
       "      <td>93.400271</td>\n",
       "      <td>71.177932</td>\n",
       "      <td>123.918015</td>\n",
       "      <td>105.789520</td>\n",
       "      <td>127.670906</td>\n",
       "      <td>109.512188</td>\n",
       "      <td>104.166149</td>\n",
       "      <td>83.022547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>139.564043</td>\n",
       "      <td>109.141995</td>\n",
       "      <td>106.936201</td>\n",
       "      <td>78.218026</td>\n",
       "      <td>144.561741</td>\n",
       "      <td>141.507291</td>\n",
       "      <td>125.361425</td>\n",
       "      <td>123.071554</td>\n",
       "      <td>105.897605</td>\n",
       "      <td>91.914775</td>\n",
       "      <td>...</td>\n",
       "      <td>86.126272</td>\n",
       "      <td>106.959002</td>\n",
       "      <td>88.494586</td>\n",
       "      <td>63.991014</td>\n",
       "      <td>129.409898</td>\n",
       "      <td>109.907911</td>\n",
       "      <td>126.391262</td>\n",
       "      <td>111.268189</td>\n",
       "      <td>100.508162</td>\n",
       "      <td>70.592735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>132.013398</td>\n",
       "      <td>115.430773</td>\n",
       "      <td>105.461899</td>\n",
       "      <td>71.804618</td>\n",
       "      <td>151.624598</td>\n",
       "      <td>143.982949</td>\n",
       "      <td>127.958184</td>\n",
       "      <td>113.784393</td>\n",
       "      <td>97.022346</td>\n",
       "      <td>99.972913</td>\n",
       "      <td>...</td>\n",
       "      <td>88.589209</td>\n",
       "      <td>107.322913</td>\n",
       "      <td>86.795897</td>\n",
       "      <td>75.659668</td>\n",
       "      <td>122.322131</td>\n",
       "      <td>117.782888</td>\n",
       "      <td>126.797409</td>\n",
       "      <td>117.722182</td>\n",
       "      <td>110.106607</td>\n",
       "      <td>76.549859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5   \\\n",
       "0     101.870132  134.252574   72.801390  108.446568  130.203502  150.760073   \n",
       "1     101.656974  133.421922   76.037698  115.578180  124.237134  144.797812   \n",
       "2     102.889598  140.640194   71.553137  106.871465  123.654012  148.446127   \n",
       "3     100.508164  130.788193   73.179060  122.566756  128.558120  144.121205   \n",
       "4     104.073940  127.217426   69.730286  115.690253  120.920018  148.666096   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  126.520010  110.917507  102.822108   62.853700  149.747652  127.099840   \n",
       "2439  122.656116  114.785269   98.718438   76.449249  152.660776  140.174139   \n",
       "2440  126.597748  119.491178  105.538634   75.394449  145.766322  143.595590   \n",
       "2441  139.564043  109.141995  106.936201   78.218026  144.561741  141.507291   \n",
       "2442  132.013398  115.430773  105.461899   71.804618  151.624598  143.982949   \n",
       "\n",
       "              6           7           8           9   ...         38  \\\n",
       "0     103.508252  125.193887   89.453295   97.318384  ...  81.685404   \n",
       "1     106.645699  137.372609   92.314999  112.314087  ...  81.526583   \n",
       "2     103.789337  135.667714   99.182335  106.232463  ...  75.930487   \n",
       "3     102.460744  129.928887   86.763744  106.168512  ...  79.984057   \n",
       "4     116.786233  139.061346   83.559242  103.091764  ...  75.279364   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  123.942335  108.196626  107.105731   96.441980  ...  91.496394   \n",
       "2439  136.835759  113.267986  104.631338   98.998328  ...  92.880258   \n",
       "2440  129.875574  120.944104  106.966013   96.617547  ...  89.648431   \n",
       "2441  125.361425  123.071554  105.897605   91.914775  ...  86.126272   \n",
       "2442  127.958184  113.784393   97.022346   99.972913  ...  88.589209   \n",
       "\n",
       "              39         40         41          42          43          44  \\\n",
       "0      84.830110  86.513881  81.048996  114.964811  120.010616  103.909997   \n",
       "1      92.908051  94.438277  89.628271  114.498751  106.887589   99.505693   \n",
       "2      82.432658  87.572150  90.919428  116.186110  121.150696   96.193748   \n",
       "3      99.957787  93.313344  84.668294  111.953201  119.676628  106.414441   \n",
       "4      87.349475  97.655142  89.118820  126.637608  114.886056  101.361093   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  121.729389  87.948166  77.602308  127.656991  114.668824  127.756278   \n",
       "2439  108.747017  88.541794  75.344392  125.557441  111.031434  134.494231   \n",
       "2440  106.485343  93.400271  71.177932  123.918015  105.789520  127.670906   \n",
       "2441  106.959002  88.494586  63.991014  129.409898  109.907911  126.391262   \n",
       "2442  107.322913  86.795897  75.659668  122.322131  117.782888  126.797409   \n",
       "\n",
       "              45          46          47  \n",
       "0     133.568532   57.626093  109.708209  \n",
       "1     128.544662   67.730350  113.436964  \n",
       "2     134.116483   68.863500  116.446807  \n",
       "3     137.948662   69.634344  114.024685  \n",
       "4     126.482809   66.133931  109.168340  \n",
       "...          ...         ...         ...  \n",
       "2438  109.362652  102.983525   78.077730  \n",
       "2439  116.813742  112.599318   79.992646  \n",
       "2440  109.512188  104.166149   83.022547  \n",
       "2441  111.268189  100.508162   70.592735  \n",
       "2442  117.722182  110.106607   76.549859  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_iReg3.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101.870132</td>\n",
       "      <td>134.252574</td>\n",
       "      <td>72.801390</td>\n",
       "      <td>108.446568</td>\n",
       "      <td>130.203502</td>\n",
       "      <td>150.760073</td>\n",
       "      <td>103.508252</td>\n",
       "      <td>125.193887</td>\n",
       "      <td>89.453295</td>\n",
       "      <td>97.318384</td>\n",
       "      <td>...</td>\n",
       "      <td>81.685404</td>\n",
       "      <td>84.830110</td>\n",
       "      <td>86.513881</td>\n",
       "      <td>81.048996</td>\n",
       "      <td>114.964811</td>\n",
       "      <td>120.010616</td>\n",
       "      <td>103.909997</td>\n",
       "      <td>133.568532</td>\n",
       "      <td>57.626093</td>\n",
       "      <td>109.708209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101.656974</td>\n",
       "      <td>133.421922</td>\n",
       "      <td>76.037698</td>\n",
       "      <td>115.578180</td>\n",
       "      <td>124.237134</td>\n",
       "      <td>144.797812</td>\n",
       "      <td>106.645699</td>\n",
       "      <td>137.372609</td>\n",
       "      <td>92.314999</td>\n",
       "      <td>112.314087</td>\n",
       "      <td>...</td>\n",
       "      <td>81.526583</td>\n",
       "      <td>92.908051</td>\n",
       "      <td>94.438277</td>\n",
       "      <td>89.628271</td>\n",
       "      <td>114.498751</td>\n",
       "      <td>106.887589</td>\n",
       "      <td>99.505693</td>\n",
       "      <td>128.544662</td>\n",
       "      <td>67.730350</td>\n",
       "      <td>113.436964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102.889598</td>\n",
       "      <td>140.640194</td>\n",
       "      <td>71.553137</td>\n",
       "      <td>106.871465</td>\n",
       "      <td>123.654012</td>\n",
       "      <td>148.446127</td>\n",
       "      <td>103.789337</td>\n",
       "      <td>135.667714</td>\n",
       "      <td>99.182335</td>\n",
       "      <td>106.232463</td>\n",
       "      <td>...</td>\n",
       "      <td>75.930487</td>\n",
       "      <td>82.432658</td>\n",
       "      <td>87.572150</td>\n",
       "      <td>90.919428</td>\n",
       "      <td>116.186110</td>\n",
       "      <td>121.150696</td>\n",
       "      <td>96.193748</td>\n",
       "      <td>134.116483</td>\n",
       "      <td>68.863500</td>\n",
       "      <td>116.446807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.508164</td>\n",
       "      <td>130.788193</td>\n",
       "      <td>73.179060</td>\n",
       "      <td>122.566756</td>\n",
       "      <td>128.558120</td>\n",
       "      <td>144.121205</td>\n",
       "      <td>102.460744</td>\n",
       "      <td>129.928887</td>\n",
       "      <td>86.763744</td>\n",
       "      <td>106.168512</td>\n",
       "      <td>...</td>\n",
       "      <td>79.984057</td>\n",
       "      <td>99.957787</td>\n",
       "      <td>93.313344</td>\n",
       "      <td>84.668294</td>\n",
       "      <td>111.953201</td>\n",
       "      <td>119.676628</td>\n",
       "      <td>106.414441</td>\n",
       "      <td>137.948662</td>\n",
       "      <td>69.634344</td>\n",
       "      <td>114.024685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>104.073940</td>\n",
       "      <td>127.217426</td>\n",
       "      <td>69.730286</td>\n",
       "      <td>115.690253</td>\n",
       "      <td>120.920018</td>\n",
       "      <td>148.666096</td>\n",
       "      <td>116.786233</td>\n",
       "      <td>139.061346</td>\n",
       "      <td>83.559242</td>\n",
       "      <td>103.091764</td>\n",
       "      <td>...</td>\n",
       "      <td>75.279364</td>\n",
       "      <td>87.349475</td>\n",
       "      <td>97.655142</td>\n",
       "      <td>89.118820</td>\n",
       "      <td>126.637608</td>\n",
       "      <td>114.886056</td>\n",
       "      <td>101.361093</td>\n",
       "      <td>126.482809</td>\n",
       "      <td>66.133931</td>\n",
       "      <td>109.168340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>126.520010</td>\n",
       "      <td>110.917507</td>\n",
       "      <td>102.822108</td>\n",
       "      <td>62.853700</td>\n",
       "      <td>149.747652</td>\n",
       "      <td>127.099840</td>\n",
       "      <td>123.942335</td>\n",
       "      <td>108.196626</td>\n",
       "      <td>107.105731</td>\n",
       "      <td>96.441980</td>\n",
       "      <td>...</td>\n",
       "      <td>91.496394</td>\n",
       "      <td>121.729389</td>\n",
       "      <td>87.948166</td>\n",
       "      <td>77.602308</td>\n",
       "      <td>127.656991</td>\n",
       "      <td>114.668824</td>\n",
       "      <td>127.756278</td>\n",
       "      <td>109.362652</td>\n",
       "      <td>102.983525</td>\n",
       "      <td>78.077730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>122.656116</td>\n",
       "      <td>114.785269</td>\n",
       "      <td>98.718438</td>\n",
       "      <td>76.449249</td>\n",
       "      <td>152.660776</td>\n",
       "      <td>140.174139</td>\n",
       "      <td>136.835759</td>\n",
       "      <td>113.267986</td>\n",
       "      <td>104.631338</td>\n",
       "      <td>98.998328</td>\n",
       "      <td>...</td>\n",
       "      <td>92.880258</td>\n",
       "      <td>108.747017</td>\n",
       "      <td>88.541794</td>\n",
       "      <td>75.344392</td>\n",
       "      <td>125.557441</td>\n",
       "      <td>111.031434</td>\n",
       "      <td>134.494231</td>\n",
       "      <td>116.813742</td>\n",
       "      <td>112.599318</td>\n",
       "      <td>79.992646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>126.597748</td>\n",
       "      <td>119.491178</td>\n",
       "      <td>105.538634</td>\n",
       "      <td>75.394449</td>\n",
       "      <td>145.766322</td>\n",
       "      <td>143.595590</td>\n",
       "      <td>129.875574</td>\n",
       "      <td>120.944104</td>\n",
       "      <td>106.966013</td>\n",
       "      <td>96.617547</td>\n",
       "      <td>...</td>\n",
       "      <td>89.648431</td>\n",
       "      <td>106.485343</td>\n",
       "      <td>93.400271</td>\n",
       "      <td>71.177932</td>\n",
       "      <td>123.918015</td>\n",
       "      <td>105.789520</td>\n",
       "      <td>127.670906</td>\n",
       "      <td>109.512188</td>\n",
       "      <td>104.166149</td>\n",
       "      <td>83.022547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>139.564043</td>\n",
       "      <td>109.141995</td>\n",
       "      <td>106.936201</td>\n",
       "      <td>78.218026</td>\n",
       "      <td>144.561741</td>\n",
       "      <td>141.507291</td>\n",
       "      <td>125.361425</td>\n",
       "      <td>123.071554</td>\n",
       "      <td>105.897605</td>\n",
       "      <td>91.914775</td>\n",
       "      <td>...</td>\n",
       "      <td>86.126272</td>\n",
       "      <td>106.959002</td>\n",
       "      <td>88.494586</td>\n",
       "      <td>63.991014</td>\n",
       "      <td>129.409898</td>\n",
       "      <td>109.907911</td>\n",
       "      <td>126.391262</td>\n",
       "      <td>111.268189</td>\n",
       "      <td>100.508162</td>\n",
       "      <td>70.592735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>132.013398</td>\n",
       "      <td>115.430773</td>\n",
       "      <td>105.461899</td>\n",
       "      <td>71.804618</td>\n",
       "      <td>151.624598</td>\n",
       "      <td>143.982949</td>\n",
       "      <td>127.958184</td>\n",
       "      <td>113.784393</td>\n",
       "      <td>97.022346</td>\n",
       "      <td>99.972913</td>\n",
       "      <td>...</td>\n",
       "      <td>88.589209</td>\n",
       "      <td>107.322913</td>\n",
       "      <td>86.795897</td>\n",
       "      <td>75.659668</td>\n",
       "      <td>122.322131</td>\n",
       "      <td>117.782888</td>\n",
       "      <td>126.797409</td>\n",
       "      <td>117.722182</td>\n",
       "      <td>110.106607</td>\n",
       "      <td>76.549859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     101.870132  134.252574   72.801390  108.446568  130.203502  150.760073   \n",
       "1     101.656974  133.421922   76.037698  115.578180  124.237134  144.797812   \n",
       "2     102.889598  140.640194   71.553137  106.871465  123.654012  148.446127   \n",
       "3     100.508164  130.788193   73.179060  122.566756  128.558120  144.121205   \n",
       "4     104.073940  127.217426   69.730286  115.690253  120.920018  148.666096   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  126.520010  110.917507  102.822108   62.853700  149.747652  127.099840   \n",
       "2439  122.656116  114.785269   98.718438   76.449249  152.660776  140.174139   \n",
       "2440  126.597748  119.491178  105.538634   75.394449  145.766322  143.595590   \n",
       "2441  139.564043  109.141995  106.936201   78.218026  144.561741  141.507291   \n",
       "2442  132.013398  115.430773  105.461899   71.804618  151.624598  143.982949   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10  ...   sensor39  \\\n",
       "0     103.508252  125.193887   89.453295   97.318384  ...  81.685404   \n",
       "1     106.645699  137.372609   92.314999  112.314087  ...  81.526583   \n",
       "2     103.789337  135.667714   99.182335  106.232463  ...  75.930487   \n",
       "3     102.460744  129.928887   86.763744  106.168512  ...  79.984057   \n",
       "4     116.786233  139.061346   83.559242  103.091764  ...  75.279364   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  123.942335  108.196626  107.105731   96.441980  ...  91.496394   \n",
       "2439  136.835759  113.267986  104.631338   98.998328  ...  92.880258   \n",
       "2440  129.875574  120.944104  106.966013   96.617547  ...  89.648431   \n",
       "2441  125.361425  123.071554  105.897605   91.914775  ...  86.126272   \n",
       "2442  127.958184  113.784393   97.022346   99.972913  ...  88.589209   \n",
       "\n",
       "        sensor40   sensor41   sensor42    sensor43    sensor44    sensor45  \\\n",
       "0      84.830110  86.513881  81.048996  114.964811  120.010616  103.909997   \n",
       "1      92.908051  94.438277  89.628271  114.498751  106.887589   99.505693   \n",
       "2      82.432658  87.572150  90.919428  116.186110  121.150696   96.193748   \n",
       "3      99.957787  93.313344  84.668294  111.953201  119.676628  106.414441   \n",
       "4      87.349475  97.655142  89.118820  126.637608  114.886056  101.361093   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  121.729389  87.948166  77.602308  127.656991  114.668824  127.756278   \n",
       "2439  108.747017  88.541794  75.344392  125.557441  111.031434  134.494231   \n",
       "2440  106.485343  93.400271  71.177932  123.918015  105.789520  127.670906   \n",
       "2441  106.959002  88.494586  63.991014  129.409898  109.907911  126.391262   \n",
       "2442  107.322913  86.795897  75.659668  122.322131  117.782888  126.797409   \n",
       "\n",
       "        sensor46    sensor47    sensor48  \n",
       "0     133.568532   57.626093  109.708209  \n",
       "1     128.544662   67.730350  113.436964  \n",
       "2     134.116483   68.863500  116.446807  \n",
       "3     137.948662   69.634344  114.024685  \n",
       "4     126.482809   66.133931  109.168340  \n",
       "...          ...         ...         ...  \n",
       "2438  109.362652  102.983525   78.077730  \n",
       "2439  116.813742  112.599318   79.992646  \n",
       "2440  109.512188  104.166149   83.022547  \n",
       "2441  111.268189  100.508162   70.592735  \n",
       "2442  117.722182  110.106607   76.549859  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data\n",
    "\n",
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e14f1e",
   "metadata": {},
   "source": [
    "# Splitting data for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07066ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam,RMSprop\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, BatchNormalization, Activation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(128))\n",
    "\n",
    "\n",
    "model.add(Dense(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_7 (LSTM)               (None, 48, 512)           1052672   \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 48, 512)          2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 48, 512)           0         \n",
      "                                                                 \n",
      " lstm_8 (LSTM)               (None, 48, 256)           787456    \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 48, 256)          1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,040,707\n",
      "Trainable params: 2,039,171\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45375995",
   "metadata": {},
   "source": [
    "# 3. Compile Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7d6ee1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "adam = Adam(lr=0.0001)\n",
    "model.compile(loss='mean_squared_error', optimizer=adam) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 4. Fit Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "391/391 [==============================] - 12s 20ms/step - loss: 1310.6185 - val_loss: 1319.3435\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1121.3108 - val_loss: 1118.8988\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1011.8242 - val_loss: 995.4927\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 918.7687 - val_loss: 926.3727\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 834.1472 - val_loss: 820.4957\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 757.3186 - val_loss: 747.2802\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 685.1519 - val_loss: 691.9946\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 623.9124 - val_loss: 750.1937\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 566.3286 - val_loss: 559.9067\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 509.6236 - val_loss: 494.9974\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 459.1194 - val_loss: 536.9418\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 413.4916 - val_loss: 404.2917\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 371.8258 - val_loss: 373.7376\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 334.9179 - val_loss: 330.3524\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 299.5528 - val_loss: 292.4629\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 267.6852 - val_loss: 269.9569\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 238.5696 - val_loss: 242.5235\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 210.8722 - val_loss: 210.5873\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 186.3487 - val_loss: 194.8501\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 164.4130 - val_loss: 163.9326\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 144.1922 - val_loss: 140.4125\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 126.6492 - val_loss: 137.4823\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 111.6466 - val_loss: 111.3361\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 96.9338 - val_loss: 97.8275\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 84.1982 - val_loss: 81.3892\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 73.1229 - val_loss: 72.9703\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 63.8134 - val_loss: 64.9714\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 54.9480 - val_loss: 56.9122\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 47.0355 - val_loss: 47.8139\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 41.7032 - val_loss: 42.3917\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 35.6074 - val_loss: 47.5277\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 30.6145 - val_loss: 34.2840\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 27.0454 - val_loss: 27.7926\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 23.8411 - val_loss: 24.6678\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 20.6502 - val_loss: 25.2292\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 18.3986 - val_loss: 20.6223\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 16.3341 - val_loss: 20.1438\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 13.9341 - val_loss: 17.2053\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 13.2039 - val_loss: 16.9237\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 11.5545 - val_loss: 18.4626\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 10.3688 - val_loss: 14.4926\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 9.4393 - val_loss: 13.5778\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 9.9714 - val_loss: 12.8053\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 8.1537 - val_loss: 13.4351\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 7.4156 - val_loss: 12.8572\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 6.7920 - val_loss: 11.6737\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 6.0416 - val_loss: 10.3727\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 5.9125 - val_loss: 11.4846\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 5.5174 - val_loss: 8.8098\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 5.0989 - val_loss: 9.7570\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 4.9748 - val_loss: 11.6152\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 5.4855 - val_loss: 9.5786\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 4.3239 - val_loss: 6.9821\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 4.1406 - val_loss: 7.7508\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 4.1787 - val_loss: 8.8675\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 4.1589 - val_loss: 10.6606\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 4.2733 - val_loss: 9.6228\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3.6963 - val_loss: 8.4778\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3.3906 - val_loss: 7.3125\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3.2507 - val_loss: 7.3178\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 3.3014 - val_loss: 7.5444\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 3.4058 - val_loss: 19.6657\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 3.4742 - val_loss: 7.0584\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 2.8840 - val_loss: 8.4390\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 3.0533 - val_loss: 6.9091\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 3.2237 - val_loss: 9.8111\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3.1182 - val_loss: 9.4263\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.8238 - val_loss: 7.1275\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.5133 - val_loss: 6.9073\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.3343 - val_loss: 7.3614\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.4809 - val_loss: 8.7224\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2.7786 - val_loss: 7.5262\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2032 - val_loss: 7.7719\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2874 - val_loss: 7.6798\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 4.0199 - val_loss: 8.2265\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.4286 - val_loss: 7.6717\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.0071 - val_loss: 7.8352\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1.9224 - val_loss: 8.5354\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 7s 18ms/step - loss: 1.9018 - val_loss: 6.9712\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 1.9993 - val_loss: 7.1089\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.6520 - val_loss: 9.7104\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 2.0396 - val_loss: 8.0430\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 2.0083 - val_loss: 6.6495\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1.6501 - val_loss: 7.2956\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.9435 - val_loss: 7.3039\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1.7238 - val_loss: 7.7997\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1.8751 - val_loss: 6.8103\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.6689 - val_loss: 7.5706\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1.7088 - val_loss: 7.4210\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.4821 - val_loss: 7.6422\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.4981 - val_loss: 6.7588\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5524 - val_loss: 7.7464\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.3405 - val_loss: 6.5098\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2413 - val_loss: 6.5134\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1.6182 - val_loss: 7.4844\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 1.3653 - val_loss: 6.8007\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 1.1642 - val_loss: 6.4850\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 1.1804 - val_loss: 7.0873\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 1.3703 - val_loss: 8.7108\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.4454 - val_loss: 14.0556\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2976 - val_loss: 7.0273\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 1.0811 - val_loss: 7.6957\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 1.4421 - val_loss: 7.4591\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2686 - val_loss: 8.3163\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.1249 - val_loss: 7.0695\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.0089 - val_loss: 7.8441\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.0807 - val_loss: 7.3573\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.0243 - val_loss: 8.0900\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.6440 - val_loss: 7.2717\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.9753 - val_loss: 6.5828\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 0.8706 - val_loss: 6.0237\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 0.9124 - val_loss: 7.3019\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1.6938 - val_loss: 23.2730\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1.3364 - val_loss: 6.7118\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 0.9874 - val_loss: 6.2368\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.7746 - val_loss: 6.2594\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6783 - val_loss: 6.0730\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.7746 - val_loss: 6.7951\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.9293 - val_loss: 8.4144\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.9635 - val_loss: 11.1373\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.1846 - val_loss: 6.4246\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7217 - val_loss: 6.2886\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.9549 - val_loss: 35.1504\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.8775 - val_loss: 6.5128\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.7450 - val_loss: 6.1678\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5342 - val_loss: 6.0545\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5885 - val_loss: 6.1861\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.6505 - val_loss: 6.5762\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.7009 - val_loss: 6.6165\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.6709 - val_loss: 6.6791\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 0.6791 - val_loss: 6.7737\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 0.5923 - val_loss: 7.1622\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 0.7318 - val_loss: 7.2203\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.7509 - val_loss: 7.2981\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.9061 - val_loss: 10.1353\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.2938 - val_loss: 13.6424\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.7844 - val_loss: 6.1854\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.5542 - val_loss: 6.2260\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.6041 - val_loss: 6.3342\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6808 - val_loss: 7.0064\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5368 - val_loss: 6.6158\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.5434 - val_loss: 6.6061\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5852 - val_loss: 7.4723\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6438 - val_loss: 7.2829\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.5098 - val_loss: 6.7752\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6836 - val_loss: 6.2729\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4515 - val_loss: 6.6105\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.4544 - val_loss: 6.6226\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 0.5286 - val_loss: 6.3074\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 0.4778 - val_loss: 6.3735\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 0.5680 - val_loss: 6.8220\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7437 - val_loss: 7.1853\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.5180 - val_loss: 6.8563\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.5113 - val_loss: 7.0782\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5723 - val_loss: 7.1161\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4900 - val_loss: 6.2126\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.4849 - val_loss: 6.3125\n",
      "Epoch 158/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4995 - val_loss: 14.7034\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.9104 - val_loss: 6.5020\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4911 - val_loss: 5.9685\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3820 - val_loss: 6.7307\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.3830 - val_loss: 6.7013\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.4564 - val_loss: 7.8327\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.5219 - val_loss: 6.9002\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4940 - val_loss: 6.5268\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.4868 - val_loss: 6.4527\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.2521 - val_loss: 8.1359\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 0.6983 - val_loss: 6.3387\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 0.3425 - val_loss: 6.0751\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 0.2850 - val_loss: 6.3700\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.3071 - val_loss: 6.1018\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.3082 - val_loss: 8.0005\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 1.7879 - val_loss: 6.9967\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.4648 - val_loss: 6.2400\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3138 - val_loss: 6.2728\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 0.3037 - val_loss: 6.4665\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2788 - val_loss: 6.1514\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.2750 - val_loss: 6.1997\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3194 - val_loss: 6.4310\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.1174 - val_loss: 7.0522\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.4092 - val_loss: 6.3323\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.2870 - val_loss: 6.2198\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.2596 - val_loss: 6.2462\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 0.2622 - val_loss: 7.0496\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 0.4034 - val_loss: 7.5645\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 0.4111 - val_loss: 6.3518\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 0.3670 - val_loss: 6.3301\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.8278 - val_loss: 6.7214\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 0.3024 - val_loss: 6.2689\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3232 - val_loss: 6.5974\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.1342 - val_loss: 8.6663\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4640 - val_loss: 6.3396\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2520 - val_loss: 6.3940\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2478 - val_loss: 6.5513\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2623 - val_loss: 6.1653\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2896 - val_loss: 6.0626\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3930 - val_loss: 6.5707\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.3423 - val_loss: 6.6105\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3953 - val_loss: 6.7056\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.4433 - val_loss: 6.8638\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dafe42bd60>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=200, batch_size=5, validation_data=(X_test, y_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 5.Evaluate Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 10ms/step - loss: 6.8638\n",
      "MSE: 6.863780498504639\n"
     ]
    }
   ],
   "source": [
    "mse = model.evaluate(X_test, y_test) \n",
    "print(\"MSE:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98614b40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8e53e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a898fcf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "264e7c60",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "47a51548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from time import time\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "476f046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store evaluation metrics\n",
    "folds = []\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b8e5b54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of folds for cross-validation\n",
    "num_folds = 5\n",
    "kf = KFold(n_splits=num_folds)\n",
    "\n",
    "# Initialize lists to store loss values\n",
    "training_loss = []\n",
    "validation_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1c1e42f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 6ms/step\n",
      "Fold: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 5ms/step\n",
      "Fold: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 8ms/step\n",
      "Fold: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 8ms/step\n",
      "Fold: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the folds\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "    print(f\"Fold: {fold+1}\")\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    # Create the model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(3))\n",
    "\n",
    "\n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam) \n",
    "\n",
    "    # Start the timer\n",
    "    start_time = time()\n",
    "\n",
    "    # Train the model and record the loss history\n",
    "    history = model.fit(X_train_fold, y_train_fold, epochs=200, batch_size=5, validation_data=(X_val_fold, y_val_fold), verbose=0)\n",
    "    \n",
    "    # Store the loss values\n",
    "    training_loss.append(history.history['loss'])\n",
    "    validation_loss.append(history.history['val_loss'])\n",
    "\n",
    "    # Evaluate the model on validation data\n",
    "    y_val_pred = model.predict(X_val_fold)\n",
    "\n",
    "    # Stop the timer and calculate time taken\n",
    "    end_time = time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    mse = mean_squared_error(y_val_fold, y_val_pred)\n",
    "    mae = mean_absolute_error(y_val_fold, y_val_pred)\n",
    "    rmse = sqrt(mse)\n",
    "\n",
    "    # Append the results to the lists\n",
    "    folds.append(fold+1)\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e3af43b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "Fold: [1, 2, 3, 4, 5]\n",
      "Mean Squared Error (MSE): [175.39414124595422, 65.85095766857864, 93.97369054567281, 69.45093331038923, 291.3691661680872]\n",
      "Mean Absolute Error (MAE): [9.659460725675304, 6.061700924062972, 6.579702484522273, 6.13989723744443, 11.201268114774225]\n",
      "Root Mean Squared Error (RMSE): [13.243645315620402, 8.114860298771546, 9.69400281337244, 8.333722656195683, 17.06953913168388]\n",
      "Time taken: [1135.1087067127228, 1120.5661046504974, 1144.8133668899536, 1122.2948577404022, 1113.81245303154]\n"
     ]
    }
   ],
   "source": [
    "# Print the evaluation results\n",
    "print(\"Evaluation Results:\")\n",
    "print(\"Fold:\", folds)\n",
    "print(\"Mean Squared Error (MSE):\", mse_scores)\n",
    "print(\"Mean Absolute Error (MAE):\", mae_scores)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse_scores)\n",
    "print(\"Time taken:\", time_taken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d4c0ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8178f115",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f596e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f1a0fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77970d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6817641a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e380f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
