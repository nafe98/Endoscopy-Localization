{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_1_Scattered_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101.870132</td>\n",
       "      <td>134.252574</td>\n",
       "      <td>72.801390</td>\n",
       "      <td>108.446568</td>\n",
       "      <td>130.203502</td>\n",
       "      <td>150.760073</td>\n",
       "      <td>103.508252</td>\n",
       "      <td>125.193887</td>\n",
       "      <td>89.453295</td>\n",
       "      <td>97.318384</td>\n",
       "      <td>...</td>\n",
       "      <td>81.685404</td>\n",
       "      <td>84.830110</td>\n",
       "      <td>86.513881</td>\n",
       "      <td>81.048996</td>\n",
       "      <td>114.964811</td>\n",
       "      <td>120.010616</td>\n",
       "      <td>103.909997</td>\n",
       "      <td>133.568532</td>\n",
       "      <td>57.626093</td>\n",
       "      <td>109.708209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101.656974</td>\n",
       "      <td>133.421922</td>\n",
       "      <td>76.037698</td>\n",
       "      <td>115.578180</td>\n",
       "      <td>124.237134</td>\n",
       "      <td>144.797812</td>\n",
       "      <td>106.645699</td>\n",
       "      <td>137.372609</td>\n",
       "      <td>92.314999</td>\n",
       "      <td>112.314087</td>\n",
       "      <td>...</td>\n",
       "      <td>81.526583</td>\n",
       "      <td>92.908051</td>\n",
       "      <td>94.438277</td>\n",
       "      <td>89.628271</td>\n",
       "      <td>114.498751</td>\n",
       "      <td>106.887589</td>\n",
       "      <td>99.505693</td>\n",
       "      <td>128.544662</td>\n",
       "      <td>67.730350</td>\n",
       "      <td>113.436964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102.889598</td>\n",
       "      <td>140.640194</td>\n",
       "      <td>71.553137</td>\n",
       "      <td>106.871465</td>\n",
       "      <td>123.654012</td>\n",
       "      <td>148.446127</td>\n",
       "      <td>103.789337</td>\n",
       "      <td>135.667714</td>\n",
       "      <td>99.182335</td>\n",
       "      <td>106.232463</td>\n",
       "      <td>...</td>\n",
       "      <td>75.930487</td>\n",
       "      <td>82.432658</td>\n",
       "      <td>87.572150</td>\n",
       "      <td>90.919428</td>\n",
       "      <td>116.186110</td>\n",
       "      <td>121.150696</td>\n",
       "      <td>96.193748</td>\n",
       "      <td>134.116483</td>\n",
       "      <td>68.863500</td>\n",
       "      <td>116.446807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.508164</td>\n",
       "      <td>130.788193</td>\n",
       "      <td>73.179060</td>\n",
       "      <td>122.566756</td>\n",
       "      <td>128.558120</td>\n",
       "      <td>144.121205</td>\n",
       "      <td>102.460744</td>\n",
       "      <td>129.928887</td>\n",
       "      <td>86.763744</td>\n",
       "      <td>106.168512</td>\n",
       "      <td>...</td>\n",
       "      <td>79.984057</td>\n",
       "      <td>99.957787</td>\n",
       "      <td>93.313344</td>\n",
       "      <td>84.668294</td>\n",
       "      <td>111.953201</td>\n",
       "      <td>119.676628</td>\n",
       "      <td>106.414441</td>\n",
       "      <td>137.948662</td>\n",
       "      <td>69.634344</td>\n",
       "      <td>114.024685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>104.073940</td>\n",
       "      <td>127.217426</td>\n",
       "      <td>69.730286</td>\n",
       "      <td>115.690253</td>\n",
       "      <td>120.920018</td>\n",
       "      <td>148.666096</td>\n",
       "      <td>116.786233</td>\n",
       "      <td>139.061346</td>\n",
       "      <td>83.559242</td>\n",
       "      <td>103.091764</td>\n",
       "      <td>...</td>\n",
       "      <td>75.279364</td>\n",
       "      <td>87.349475</td>\n",
       "      <td>97.655142</td>\n",
       "      <td>89.118820</td>\n",
       "      <td>126.637608</td>\n",
       "      <td>114.886056</td>\n",
       "      <td>101.361093</td>\n",
       "      <td>126.482809</td>\n",
       "      <td>66.133931</td>\n",
       "      <td>109.168340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>126.520010</td>\n",
       "      <td>110.917507</td>\n",
       "      <td>102.822108</td>\n",
       "      <td>62.853700</td>\n",
       "      <td>149.747652</td>\n",
       "      <td>127.099840</td>\n",
       "      <td>123.942335</td>\n",
       "      <td>108.196626</td>\n",
       "      <td>107.105731</td>\n",
       "      <td>96.441980</td>\n",
       "      <td>...</td>\n",
       "      <td>91.496394</td>\n",
       "      <td>121.729389</td>\n",
       "      <td>87.948166</td>\n",
       "      <td>77.602308</td>\n",
       "      <td>127.656991</td>\n",
       "      <td>114.668824</td>\n",
       "      <td>127.756278</td>\n",
       "      <td>109.362652</td>\n",
       "      <td>102.983525</td>\n",
       "      <td>78.077730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>122.656116</td>\n",
       "      <td>114.785269</td>\n",
       "      <td>98.718438</td>\n",
       "      <td>76.449249</td>\n",
       "      <td>152.660776</td>\n",
       "      <td>140.174139</td>\n",
       "      <td>136.835759</td>\n",
       "      <td>113.267986</td>\n",
       "      <td>104.631338</td>\n",
       "      <td>98.998328</td>\n",
       "      <td>...</td>\n",
       "      <td>92.880258</td>\n",
       "      <td>108.747017</td>\n",
       "      <td>88.541794</td>\n",
       "      <td>75.344392</td>\n",
       "      <td>125.557441</td>\n",
       "      <td>111.031434</td>\n",
       "      <td>134.494231</td>\n",
       "      <td>116.813742</td>\n",
       "      <td>112.599318</td>\n",
       "      <td>79.992646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>126.597748</td>\n",
       "      <td>119.491178</td>\n",
       "      <td>105.538634</td>\n",
       "      <td>75.394449</td>\n",
       "      <td>145.766322</td>\n",
       "      <td>143.595590</td>\n",
       "      <td>129.875574</td>\n",
       "      <td>120.944104</td>\n",
       "      <td>106.966013</td>\n",
       "      <td>96.617547</td>\n",
       "      <td>...</td>\n",
       "      <td>89.648431</td>\n",
       "      <td>106.485343</td>\n",
       "      <td>93.400271</td>\n",
       "      <td>71.177932</td>\n",
       "      <td>123.918015</td>\n",
       "      <td>105.789520</td>\n",
       "      <td>127.670906</td>\n",
       "      <td>109.512188</td>\n",
       "      <td>104.166149</td>\n",
       "      <td>83.022547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>139.564043</td>\n",
       "      <td>109.141995</td>\n",
       "      <td>106.936201</td>\n",
       "      <td>78.218026</td>\n",
       "      <td>144.561741</td>\n",
       "      <td>141.507291</td>\n",
       "      <td>125.361425</td>\n",
       "      <td>123.071554</td>\n",
       "      <td>105.897605</td>\n",
       "      <td>91.914775</td>\n",
       "      <td>...</td>\n",
       "      <td>86.126272</td>\n",
       "      <td>106.959002</td>\n",
       "      <td>88.494586</td>\n",
       "      <td>63.991014</td>\n",
       "      <td>129.409898</td>\n",
       "      <td>109.907911</td>\n",
       "      <td>126.391262</td>\n",
       "      <td>111.268189</td>\n",
       "      <td>100.508162</td>\n",
       "      <td>70.592735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>132.013398</td>\n",
       "      <td>115.430773</td>\n",
       "      <td>105.461899</td>\n",
       "      <td>71.804618</td>\n",
       "      <td>151.624598</td>\n",
       "      <td>143.982949</td>\n",
       "      <td>127.958184</td>\n",
       "      <td>113.784393</td>\n",
       "      <td>97.022346</td>\n",
       "      <td>99.972913</td>\n",
       "      <td>...</td>\n",
       "      <td>88.589209</td>\n",
       "      <td>107.322913</td>\n",
       "      <td>86.795897</td>\n",
       "      <td>75.659668</td>\n",
       "      <td>122.322131</td>\n",
       "      <td>117.782888</td>\n",
       "      <td>126.797409</td>\n",
       "      <td>117.722182</td>\n",
       "      <td>110.106607</td>\n",
       "      <td>76.549859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5   \\\n",
       "0     101.870132  134.252574   72.801390  108.446568  130.203502  150.760073   \n",
       "1     101.656974  133.421922   76.037698  115.578180  124.237134  144.797812   \n",
       "2     102.889598  140.640194   71.553137  106.871465  123.654012  148.446127   \n",
       "3     100.508164  130.788193   73.179060  122.566756  128.558120  144.121205   \n",
       "4     104.073940  127.217426   69.730286  115.690253  120.920018  148.666096   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  126.520010  110.917507  102.822108   62.853700  149.747652  127.099840   \n",
       "2439  122.656116  114.785269   98.718438   76.449249  152.660776  140.174139   \n",
       "2440  126.597748  119.491178  105.538634   75.394449  145.766322  143.595590   \n",
       "2441  139.564043  109.141995  106.936201   78.218026  144.561741  141.507291   \n",
       "2442  132.013398  115.430773  105.461899   71.804618  151.624598  143.982949   \n",
       "\n",
       "              6           7           8           9   ...         38  \\\n",
       "0     103.508252  125.193887   89.453295   97.318384  ...  81.685404   \n",
       "1     106.645699  137.372609   92.314999  112.314087  ...  81.526583   \n",
       "2     103.789337  135.667714   99.182335  106.232463  ...  75.930487   \n",
       "3     102.460744  129.928887   86.763744  106.168512  ...  79.984057   \n",
       "4     116.786233  139.061346   83.559242  103.091764  ...  75.279364   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  123.942335  108.196626  107.105731   96.441980  ...  91.496394   \n",
       "2439  136.835759  113.267986  104.631338   98.998328  ...  92.880258   \n",
       "2440  129.875574  120.944104  106.966013   96.617547  ...  89.648431   \n",
       "2441  125.361425  123.071554  105.897605   91.914775  ...  86.126272   \n",
       "2442  127.958184  113.784393   97.022346   99.972913  ...  88.589209   \n",
       "\n",
       "              39         40         41          42          43          44  \\\n",
       "0      84.830110  86.513881  81.048996  114.964811  120.010616  103.909997   \n",
       "1      92.908051  94.438277  89.628271  114.498751  106.887589   99.505693   \n",
       "2      82.432658  87.572150  90.919428  116.186110  121.150696   96.193748   \n",
       "3      99.957787  93.313344  84.668294  111.953201  119.676628  106.414441   \n",
       "4      87.349475  97.655142  89.118820  126.637608  114.886056  101.361093   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  121.729389  87.948166  77.602308  127.656991  114.668824  127.756278   \n",
       "2439  108.747017  88.541794  75.344392  125.557441  111.031434  134.494231   \n",
       "2440  106.485343  93.400271  71.177932  123.918015  105.789520  127.670906   \n",
       "2441  106.959002  88.494586  63.991014  129.409898  109.907911  126.391262   \n",
       "2442  107.322913  86.795897  75.659668  122.322131  117.782888  126.797409   \n",
       "\n",
       "              45          46          47  \n",
       "0     133.568532   57.626093  109.708209  \n",
       "1     128.544662   67.730350  113.436964  \n",
       "2     134.116483   68.863500  116.446807  \n",
       "3     137.948662   69.634344  114.024685  \n",
       "4     126.482809   66.133931  109.168340  \n",
       "...          ...         ...         ...  \n",
       "2438  109.362652  102.983525   78.077730  \n",
       "2439  116.813742  112.599318   79.992646  \n",
       "2440  109.512188  104.166149   83.022547  \n",
       "2441  111.268189  100.508162   70.592735  \n",
       "2442  117.722182  110.106607   76.549859  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101.870132</td>\n",
       "      <td>134.252574</td>\n",
       "      <td>72.801390</td>\n",
       "      <td>108.446568</td>\n",
       "      <td>130.203502</td>\n",
       "      <td>150.760073</td>\n",
       "      <td>103.508252</td>\n",
       "      <td>125.193887</td>\n",
       "      <td>89.453295</td>\n",
       "      <td>97.318384</td>\n",
       "      <td>...</td>\n",
       "      <td>81.685404</td>\n",
       "      <td>84.830110</td>\n",
       "      <td>86.513881</td>\n",
       "      <td>81.048996</td>\n",
       "      <td>114.964811</td>\n",
       "      <td>120.010616</td>\n",
       "      <td>103.909997</td>\n",
       "      <td>133.568532</td>\n",
       "      <td>57.626093</td>\n",
       "      <td>109.708209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101.656974</td>\n",
       "      <td>133.421922</td>\n",
       "      <td>76.037698</td>\n",
       "      <td>115.578180</td>\n",
       "      <td>124.237134</td>\n",
       "      <td>144.797812</td>\n",
       "      <td>106.645699</td>\n",
       "      <td>137.372609</td>\n",
       "      <td>92.314999</td>\n",
       "      <td>112.314087</td>\n",
       "      <td>...</td>\n",
       "      <td>81.526583</td>\n",
       "      <td>92.908051</td>\n",
       "      <td>94.438277</td>\n",
       "      <td>89.628271</td>\n",
       "      <td>114.498751</td>\n",
       "      <td>106.887589</td>\n",
       "      <td>99.505693</td>\n",
       "      <td>128.544662</td>\n",
       "      <td>67.730350</td>\n",
       "      <td>113.436964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102.889598</td>\n",
       "      <td>140.640194</td>\n",
       "      <td>71.553137</td>\n",
       "      <td>106.871465</td>\n",
       "      <td>123.654012</td>\n",
       "      <td>148.446127</td>\n",
       "      <td>103.789337</td>\n",
       "      <td>135.667714</td>\n",
       "      <td>99.182335</td>\n",
       "      <td>106.232463</td>\n",
       "      <td>...</td>\n",
       "      <td>75.930487</td>\n",
       "      <td>82.432658</td>\n",
       "      <td>87.572150</td>\n",
       "      <td>90.919428</td>\n",
       "      <td>116.186110</td>\n",
       "      <td>121.150696</td>\n",
       "      <td>96.193748</td>\n",
       "      <td>134.116483</td>\n",
       "      <td>68.863500</td>\n",
       "      <td>116.446807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.508164</td>\n",
       "      <td>130.788193</td>\n",
       "      <td>73.179060</td>\n",
       "      <td>122.566756</td>\n",
       "      <td>128.558120</td>\n",
       "      <td>144.121205</td>\n",
       "      <td>102.460744</td>\n",
       "      <td>129.928887</td>\n",
       "      <td>86.763744</td>\n",
       "      <td>106.168512</td>\n",
       "      <td>...</td>\n",
       "      <td>79.984057</td>\n",
       "      <td>99.957787</td>\n",
       "      <td>93.313344</td>\n",
       "      <td>84.668294</td>\n",
       "      <td>111.953201</td>\n",
       "      <td>119.676628</td>\n",
       "      <td>106.414441</td>\n",
       "      <td>137.948662</td>\n",
       "      <td>69.634344</td>\n",
       "      <td>114.024685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>104.073940</td>\n",
       "      <td>127.217426</td>\n",
       "      <td>69.730286</td>\n",
       "      <td>115.690253</td>\n",
       "      <td>120.920018</td>\n",
       "      <td>148.666096</td>\n",
       "      <td>116.786233</td>\n",
       "      <td>139.061346</td>\n",
       "      <td>83.559242</td>\n",
       "      <td>103.091764</td>\n",
       "      <td>...</td>\n",
       "      <td>75.279364</td>\n",
       "      <td>87.349475</td>\n",
       "      <td>97.655142</td>\n",
       "      <td>89.118820</td>\n",
       "      <td>126.637608</td>\n",
       "      <td>114.886056</td>\n",
       "      <td>101.361093</td>\n",
       "      <td>126.482809</td>\n",
       "      <td>66.133931</td>\n",
       "      <td>109.168340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>126.520010</td>\n",
       "      <td>110.917507</td>\n",
       "      <td>102.822108</td>\n",
       "      <td>62.853700</td>\n",
       "      <td>149.747652</td>\n",
       "      <td>127.099840</td>\n",
       "      <td>123.942335</td>\n",
       "      <td>108.196626</td>\n",
       "      <td>107.105731</td>\n",
       "      <td>96.441980</td>\n",
       "      <td>...</td>\n",
       "      <td>91.496394</td>\n",
       "      <td>121.729389</td>\n",
       "      <td>87.948166</td>\n",
       "      <td>77.602308</td>\n",
       "      <td>127.656991</td>\n",
       "      <td>114.668824</td>\n",
       "      <td>127.756278</td>\n",
       "      <td>109.362652</td>\n",
       "      <td>102.983525</td>\n",
       "      <td>78.077730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>122.656116</td>\n",
       "      <td>114.785269</td>\n",
       "      <td>98.718438</td>\n",
       "      <td>76.449249</td>\n",
       "      <td>152.660776</td>\n",
       "      <td>140.174139</td>\n",
       "      <td>136.835759</td>\n",
       "      <td>113.267986</td>\n",
       "      <td>104.631338</td>\n",
       "      <td>98.998328</td>\n",
       "      <td>...</td>\n",
       "      <td>92.880258</td>\n",
       "      <td>108.747017</td>\n",
       "      <td>88.541794</td>\n",
       "      <td>75.344392</td>\n",
       "      <td>125.557441</td>\n",
       "      <td>111.031434</td>\n",
       "      <td>134.494231</td>\n",
       "      <td>116.813742</td>\n",
       "      <td>112.599318</td>\n",
       "      <td>79.992646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>126.597748</td>\n",
       "      <td>119.491178</td>\n",
       "      <td>105.538634</td>\n",
       "      <td>75.394449</td>\n",
       "      <td>145.766322</td>\n",
       "      <td>143.595590</td>\n",
       "      <td>129.875574</td>\n",
       "      <td>120.944104</td>\n",
       "      <td>106.966013</td>\n",
       "      <td>96.617547</td>\n",
       "      <td>...</td>\n",
       "      <td>89.648431</td>\n",
       "      <td>106.485343</td>\n",
       "      <td>93.400271</td>\n",
       "      <td>71.177932</td>\n",
       "      <td>123.918015</td>\n",
       "      <td>105.789520</td>\n",
       "      <td>127.670906</td>\n",
       "      <td>109.512188</td>\n",
       "      <td>104.166149</td>\n",
       "      <td>83.022547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>139.564043</td>\n",
       "      <td>109.141995</td>\n",
       "      <td>106.936201</td>\n",
       "      <td>78.218026</td>\n",
       "      <td>144.561741</td>\n",
       "      <td>141.507291</td>\n",
       "      <td>125.361425</td>\n",
       "      <td>123.071554</td>\n",
       "      <td>105.897605</td>\n",
       "      <td>91.914775</td>\n",
       "      <td>...</td>\n",
       "      <td>86.126272</td>\n",
       "      <td>106.959002</td>\n",
       "      <td>88.494586</td>\n",
       "      <td>63.991014</td>\n",
       "      <td>129.409898</td>\n",
       "      <td>109.907911</td>\n",
       "      <td>126.391262</td>\n",
       "      <td>111.268189</td>\n",
       "      <td>100.508162</td>\n",
       "      <td>70.592735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>132.013398</td>\n",
       "      <td>115.430773</td>\n",
       "      <td>105.461899</td>\n",
       "      <td>71.804618</td>\n",
       "      <td>151.624598</td>\n",
       "      <td>143.982949</td>\n",
       "      <td>127.958184</td>\n",
       "      <td>113.784393</td>\n",
       "      <td>97.022346</td>\n",
       "      <td>99.972913</td>\n",
       "      <td>...</td>\n",
       "      <td>88.589209</td>\n",
       "      <td>107.322913</td>\n",
       "      <td>86.795897</td>\n",
       "      <td>75.659668</td>\n",
       "      <td>122.322131</td>\n",
       "      <td>117.782888</td>\n",
       "      <td>126.797409</td>\n",
       "      <td>117.722182</td>\n",
       "      <td>110.106607</td>\n",
       "      <td>76.549859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     101.870132  134.252574   72.801390  108.446568  130.203502  150.760073   \n",
       "1     101.656974  133.421922   76.037698  115.578180  124.237134  144.797812   \n",
       "2     102.889598  140.640194   71.553137  106.871465  123.654012  148.446127   \n",
       "3     100.508164  130.788193   73.179060  122.566756  128.558120  144.121205   \n",
       "4     104.073940  127.217426   69.730286  115.690253  120.920018  148.666096   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  126.520010  110.917507  102.822108   62.853700  149.747652  127.099840   \n",
       "2439  122.656116  114.785269   98.718438   76.449249  152.660776  140.174139   \n",
       "2440  126.597748  119.491178  105.538634   75.394449  145.766322  143.595590   \n",
       "2441  139.564043  109.141995  106.936201   78.218026  144.561741  141.507291   \n",
       "2442  132.013398  115.430773  105.461899   71.804618  151.624598  143.982949   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10  ...   sensor39  \\\n",
       "0     103.508252  125.193887   89.453295   97.318384  ...  81.685404   \n",
       "1     106.645699  137.372609   92.314999  112.314087  ...  81.526583   \n",
       "2     103.789337  135.667714   99.182335  106.232463  ...  75.930487   \n",
       "3     102.460744  129.928887   86.763744  106.168512  ...  79.984057   \n",
       "4     116.786233  139.061346   83.559242  103.091764  ...  75.279364   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  123.942335  108.196626  107.105731   96.441980  ...  91.496394   \n",
       "2439  136.835759  113.267986  104.631338   98.998328  ...  92.880258   \n",
       "2440  129.875574  120.944104  106.966013   96.617547  ...  89.648431   \n",
       "2441  125.361425  123.071554  105.897605   91.914775  ...  86.126272   \n",
       "2442  127.958184  113.784393   97.022346   99.972913  ...  88.589209   \n",
       "\n",
       "        sensor40   sensor41   sensor42    sensor43    sensor44    sensor45  \\\n",
       "0      84.830110  86.513881  81.048996  114.964811  120.010616  103.909997   \n",
       "1      92.908051  94.438277  89.628271  114.498751  106.887589   99.505693   \n",
       "2      82.432658  87.572150  90.919428  116.186110  121.150696   96.193748   \n",
       "3      99.957787  93.313344  84.668294  111.953201  119.676628  106.414441   \n",
       "4      87.349475  97.655142  89.118820  126.637608  114.886056  101.361093   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  121.729389  87.948166  77.602308  127.656991  114.668824  127.756278   \n",
       "2439  108.747017  88.541794  75.344392  125.557441  111.031434  134.494231   \n",
       "2440  106.485343  93.400271  71.177932  123.918015  105.789520  127.670906   \n",
       "2441  106.959002  88.494586  63.991014  129.409898  109.907911  126.391262   \n",
       "2442  107.322913  86.795897  75.659668  122.322131  117.782888  126.797409   \n",
       "\n",
       "        sensor46    sensor47    sensor48  \n",
       "0     133.568532   57.626093  109.708209  \n",
       "1     128.544662   67.730350  113.436964  \n",
       "2     134.116483   68.863500  116.446807  \n",
       "3     137.948662   69.634344  114.024685  \n",
       "4     126.482809   66.133931  109.168340  \n",
       "...          ...         ...         ...  \n",
       "2438  109.362652  102.983525   78.077730  \n",
       "2439  116.813742  112.599318   79.992646  \n",
       "2440  109.512188  104.166149   83.022547  \n",
       "2441  111.268189  100.508162   70.592735  \n",
       "2442  117.722182  110.106607   76.549859  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 11s 18ms/step - loss: 1360.9033 - val_loss: 1278.8341\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1203.3009 - val_loss: 1179.2206\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1118.2698 - val_loss: 1106.1171\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1055.3602 - val_loss: 1051.5631\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1008.8918 - val_loss: 1011.5726\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 975.2419 - val_loss: 983.0858\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 951.7518 - val_loss: 963.6063\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 935.9247 - val_loss: 950.7937\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 926.1072 - val_loss: 943.2438\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.4671 - val_loss: 939.0498\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 917.5291 - val_loss: 936.9661\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 916.1790 - val_loss: 935.9773\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.6241 - val_loss: 935.6556\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.4165 - val_loss: 935.4841\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.3974 - val_loss: 935.4511\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.3317 - val_loss: 935.5718\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 915.3651 - val_loss: 935.4864\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 915.3461 - val_loss: 935.5613\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.3157 - val_loss: 935.5247\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.3381 - val_loss: 935.5821\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.3896 - val_loss: 935.4792\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 915.3524 - val_loss: 935.4303\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 915.3126 - val_loss: 935.4608\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 915.3679 - val_loss: 935.4213\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 915.3752 - val_loss: 935.5173\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.3665 - val_loss: 935.5557\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.2916 - val_loss: 935.4714\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.3477 - val_loss: 935.5391\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 910.8765 - val_loss: 898.4508\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 867.0718 - val_loss: 870.0001\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 845.5699 - val_loss: 842.4408\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 807.8734 - val_loss: 814.7068\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 775.7698 - val_loss: 783.4918\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 751.8984 - val_loss: 756.9154\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 722.3415 - val_loss: 725.2106\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 680.1224 - val_loss: 677.0784\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 627.5306 - val_loss: 613.2335\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 568.1158 - val_loss: 558.2420\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 517.3275 - val_loss: 507.9642\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 478.8230 - val_loss: 469.1030\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 435.9799 - val_loss: 428.3594\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 398.2036 - val_loss: 391.4615\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 363.6380 - val_loss: 358.9330\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 331.8011 - val_loss: 324.9485\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 300.4023 - val_loss: 304.9680\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 274.9386 - val_loss: 269.0001\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 249.5933 - val_loss: 245.3177\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 226.5893 - val_loss: 223.7612\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 203.6895 - val_loss: 203.7264\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 182.7049 - val_loss: 180.2298\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 165.8595 - val_loss: 163.8697\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 147.6922 - val_loss: 147.2367\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 133.0021 - val_loss: 130.0757\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 118.6025 - val_loss: 118.2593\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 106.2760 - val_loss: 106.4415\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 94.7020 - val_loss: 101.4711\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 85.4204 - val_loss: 85.4326\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 75.4469 - val_loss: 76.9015\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 67.4451 - val_loss: 67.4626\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 59.4448 - val_loss: 59.3802\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 53.0457 - val_loss: 52.7455\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 48.0219 - val_loss: 48.3082\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 42.1914 - val_loss: 42.9534\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 37.8030 - val_loss: 38.5094\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 34.1581 - val_loss: 34.8748\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 30.5026 - val_loss: 34.0788\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 28.0080 - val_loss: 30.1634\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 26.7377 - val_loss: 28.5768\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 23.2351 - val_loss: 25.3221\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 21.3438 - val_loss: 21.9985\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 19.5894 - val_loss: 23.8471\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 17.8605 - val_loss: 19.9914\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 16.8013 - val_loss: 23.3234\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 16.1956 - val_loss: 17.5431\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 15.3495 - val_loss: 20.2122\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 14.2215 - val_loss: 14.9917\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 13.0095 - val_loss: 14.1000\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 12.5727 - val_loss: 13.5584\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.3257 - val_loss: 17.7065\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.0181 - val_loss: 12.4804\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 11.1331 - val_loss: 11.8194\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.3583 - val_loss: 11.3043\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.2885 - val_loss: 11.3441\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.0841 - val_loss: 10.9751\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.3473 - val_loss: 14.1409\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.9610 - val_loss: 11.0377\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.3257 - val_loss: 11.3174\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.3502 - val_loss: 11.5543\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.4790 - val_loss: 10.5793\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.0291 - val_loss: 11.1465\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.8913 - val_loss: 14.5756\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.2504 - val_loss: 10.2967\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.6163 - val_loss: 10.4599\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.8634 - val_loss: 11.1400\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.3814 - val_loss: 9.7601\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.9121 - val_loss: 8.7901\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.1446 - val_loss: 9.5333\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.4062 - val_loss: 9.6548\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.7089 - val_loss: 8.3615\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.9400 - val_loss: 9.4805\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.9357 - val_loss: 8.4608\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.2595 - val_loss: 8.4894\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.4814 - val_loss: 8.4342\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 6.2163 - val_loss: 7.8559\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.4737 - val_loss: 8.0714\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.7136 - val_loss: 8.3589\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.8642 - val_loss: 8.9291\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 5.7373 - val_loss: 8.3608\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 5.4510 - val_loss: 7.7172\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3031 - val_loss: 9.2809\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3386 - val_loss: 7.6402\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.5987 - val_loss: 9.2314\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.0909 - val_loss: 8.2209\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 5.0940 - val_loss: 7.2455\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.0577 - val_loss: 7.9992\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.9143 - val_loss: 8.5316\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.2340 - val_loss: 8.1618\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.0124 - val_loss: 9.6828\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.7888 - val_loss: 6.8521\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.6750 - val_loss: 7.2577\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.6314 - val_loss: 7.6694\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.6675 - val_loss: 7.2309\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.2077 - val_loss: 7.9636\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.0550 - val_loss: 7.5105\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.6636 - val_loss: 8.5407\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.2305 - val_loss: 7.0440\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.4168 - val_loss: 6.5623\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.9396 - val_loss: 6.4180\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.9724 - val_loss: 7.1269\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.0386 - val_loss: 8.4803\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.8154 - val_loss: 7.1320\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3.9059 - val_loss: 8.0245\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.5131 - val_loss: 6.9869\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.1584 - val_loss: 6.6082\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3.7255 - val_loss: 7.0161\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.8452 - val_loss: 6.3504\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.4597 - val_loss: 7.4793\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.7350 - val_loss: 7.4951\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.4283 - val_loss: 6.9439\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.9804 - val_loss: 7.2266\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.6922 - val_loss: 6.0960\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3.4536 - val_loss: 7.2974\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3.6922 - val_loss: 7.6724\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 3.5436 - val_loss: 6.8666\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.9129 - val_loss: 6.0098\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.5701 - val_loss: 7.3186\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.5400 - val_loss: 6.6567\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.7826 - val_loss: 6.9145\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.7231 - val_loss: 6.7376\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3698 - val_loss: 6.2354\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3401 - val_loss: 6.0070\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3016 - val_loss: 6.3711\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.8166 - val_loss: 6.4094\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.1114 - val_loss: 8.1374\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0093 - val_loss: 6.5899\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0364 - val_loss: 8.6195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0520 - val_loss: 6.4487\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.9613 - val_loss: 22.3873\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.3541 - val_loss: 6.3191\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2905 - val_loss: 5.8951\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.9209 - val_loss: 5.7209\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.8543 - val_loss: 6.5181\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.7591 - val_loss: 5.7814\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.7556 - val_loss: 5.6939\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.5729 - val_loss: 5.7369\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7888 - val_loss: 6.2342\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 2.6721 - val_loss: 6.9494\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8726 - val_loss: 6.8185\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.1694 - val_loss: 6.5757\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.1442 - val_loss: 8.2813\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.6039 - val_loss: 5.8375\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 2.6232 - val_loss: 6.2754\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 2.7707 - val_loss: 6.4549\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.5189 - val_loss: 7.7629\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3.0029 - val_loss: 7.6941\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 3.1244 - val_loss: 5.6052\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.5408 - val_loss: 6.3197\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.3952 - val_loss: 6.1745\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.3448 - val_loss: 6.2725\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3.0299 - val_loss: 6.2271\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.8874 - val_loss: 6.8458\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3906 - val_loss: 6.1058\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.3713 - val_loss: 6.3513\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2615 - val_loss: 6.4909\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.6251 - val_loss: 5.9253\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.6878 - val_loss: 6.0667\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1532 - val_loss: 6.4980\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.1758 - val_loss: 6.0815\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.6334 - val_loss: 6.1418\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5439 - val_loss: 7.0363\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2181 - val_loss: 6.6767\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1858 - val_loss: 6.1762\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.3704 - val_loss: 5.7152\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0092 - val_loss: 5.8307\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 1.8320 - val_loss: 5.8692\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 1.9085 - val_loss: 5.7139\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.2832 - val_loss: 6.0536\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1943 - val_loss: 6.4593\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.2480 - val_loss: 5.7884\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7875 - val_loss: 6.6427\n",
      "16/16 [==============================] - 1s 8ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 6.64264936443286\n",
      "Mean Absolute Error (MAE): 1.7658807219224724\n",
      "Root Mean Squared Error (RMSE): 2.5773337704753843\n",
      "Time taken: 1189.9958555698395\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 10s 17ms/step - loss: 1405.7262 - val_loss: 1281.3319\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1247.8558 - val_loss: 1178.8722\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1159.7374 - val_loss: 1103.6056\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1091.3315 - val_loss: 1044.8322\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1039.0751 - val_loss: 1000.4990\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 999.0027 - val_loss: 967.9079\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 970.2971 - val_loss: 945.6028\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 950.4951 - val_loss: 930.7682\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 937.4393 - val_loss: 921.7124\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 929.3868 - val_loss: 916.5703\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 924.8256 - val_loss: 914.1673\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 922.5662 - val_loss: 913.2441\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.5069 - val_loss: 912.8992\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.0995 - val_loss: 912.9136\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.9297 - val_loss: 912.9138\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.9710 - val_loss: 913.0267\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.0540 - val_loss: 912.9192\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.8913 - val_loss: 913.0942\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.8850 - val_loss: 913.1671\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 920.8771 - val_loss: 913.1423\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.9042 - val_loss: 913.1401\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.3059 - val_loss: 912.7714\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.0093 - val_loss: 912.8495\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.9472 - val_loss: 913.0565\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 920.8716 - val_loss: 912.9657\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 920.6062 - val_loss: 913.0093\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.9356 - val_loss: 912.9310\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 920.9079 - val_loss: 912.9334\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 911.6854 - val_loss: 880.3937\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 869.8416 - val_loss: 847.4404\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 843.4861 - val_loss: 823.1150\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 819.0933 - val_loss: 796.5891\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 788.2314 - val_loss: 753.5820\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 736.9119 - val_loss: 702.1664\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 683.3624 - val_loss: 651.0131\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 632.6785 - val_loss: 600.4285\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 579.6371 - val_loss: 545.9356\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 529.9240 - val_loss: 509.4404\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 486.2593 - val_loss: 454.1154\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 441.8327 - val_loss: 411.2598\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 401.1968 - val_loss: 377.4047\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 363.8218 - val_loss: 339.4374\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 326.7859 - val_loss: 304.2921\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 294.4132 - val_loss: 280.6997\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 268.1803 - val_loss: 251.0155\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 239.7988 - val_loss: 222.3558\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 214.8961 - val_loss: 204.7130\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 190.4574 - val_loss: 177.3639\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 170.4453 - val_loss: 162.7314\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 150.7577 - val_loss: 142.4953\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 134.2051 - val_loss: 127.6079\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 119.4027 - val_loss: 118.9649\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 105.0739 - val_loss: 98.1763\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 90.1287 - val_loss: 83.7491\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 79.7851 - val_loss: 77.2274\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 69.1938 - val_loss: 66.5998\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 61.1610 - val_loss: 56.4195\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 53.0097 - val_loss: 56.1634\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 47.7206 - val_loss: 46.8954\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 41.5865 - val_loss: 38.9978\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 36.8999 - val_loss: 33.7005\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 31.3487 - val_loss: 32.2422\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 29.0040 - val_loss: 29.0910\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 25.0569 - val_loss: 25.6095\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 23.1089 - val_loss: 31.6227\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 20.4993 - val_loss: 22.5043\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 18.9543 - val_loss: 23.0719\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 17.4567 - val_loss: 17.0089\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 15.0790 - val_loss: 15.8722\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 14.2700 - val_loss: 15.9534\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 13.2817 - val_loss: 13.8329\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.6793 - val_loss: 13.7929\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.0705 - val_loss: 13.5153\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.7979 - val_loss: 10.7446\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.1135 - val_loss: 10.9197\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.6562 - val_loss: 10.3276\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.9749 - val_loss: 13.6416\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 14ms/step - loss: 9.0783 - val_loss: 9.8820\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.9752 - val_loss: 9.9504\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.4579 - val_loss: 11.6070\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.2963 - val_loss: 9.5562\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.3431 - val_loss: 10.5538\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.7971 - val_loss: 8.2655\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.3436 - val_loss: 8.6793\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.2271 - val_loss: 9.1299\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.8607 - val_loss: 7.6060\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.3338 - val_loss: 8.5910\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.4152 - val_loss: 7.7018\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.3751 - val_loss: 7.0123\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 5.7875 - val_loss: 6.2012\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 6.7883 - val_loss: 8.9734\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 5.4332 - val_loss: 10.2247\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.1306 - val_loss: 12.0573\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.8999 - val_loss: 7.1334\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.9066 - val_loss: 7.6972\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 6.0102 - val_loss: 7.7859\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 4.7532 - val_loss: 6.4428\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 4.8249 - val_loss: 8.1930\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 4.8932 - val_loss: 6.6643\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 5.9246 - val_loss: 7.8222\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.9992 - val_loss: 6.1256\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.4081 - val_loss: 6.9310\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.5271 - val_loss: 8.0833\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.6481 - val_loss: 9.2558\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.3121 - val_loss: 6.8934\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.3742 - val_loss: 13.2750\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.4313 - val_loss: 6.3564\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.3094 - val_loss: 8.8791\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.2787 - val_loss: 5.8547\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.8541 - val_loss: 6.6607\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.1455 - val_loss: 21.4832\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.3981 - val_loss: 6.9180\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.6592 - val_loss: 5.6650\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5031 - val_loss: 6.2055\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.4809 - val_loss: 6.5500\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3.8554 - val_loss: 5.7201\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5589 - val_loss: 6.1701\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 5.4388 - val_loss: 6.6727\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.7274 - val_loss: 6.1106\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2666 - val_loss: 6.9053\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2990 - val_loss: 6.1527\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3073 - val_loss: 6.0352\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2319 - val_loss: 6.0773\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.3613 - val_loss: 6.3596\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.2155 - val_loss: 10.1564\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.7552 - val_loss: 5.6096\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.8702 - val_loss: 5.6289\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.9185 - val_loss: 5.1889\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.9056 - val_loss: 5.3750\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.1763 - val_loss: 5.3896\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.6683 - val_loss: 6.2996\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2106 - val_loss: 5.9145\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0187 - val_loss: 5.8078\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 3.1137 - val_loss: 6.3205\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7613 - val_loss: 5.7311\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8912 - val_loss: 5.5626\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4965 - val_loss: 5.7253\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7275 - val_loss: 9.0767\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.6116 - val_loss: 7.1039\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.6494 - val_loss: 5.0117\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2745 - val_loss: 5.2828\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.4934 - val_loss: 5.7026\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.2526 - val_loss: 5.3766\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.5116 - val_loss: 5.3071\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2453 - val_loss: 5.0251\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2467 - val_loss: 5.3033\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2384 - val_loss: 6.0348\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.8424 - val_loss: 5.5907\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1830 - val_loss: 5.3321\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0304 - val_loss: 5.0991\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8223 - val_loss: 5.0952\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.1415 - val_loss: 5.1545\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0736 - val_loss: 5.9623\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1171 - val_loss: 4.8345\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0243 - val_loss: 5.4615\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 2.3798 - val_loss: 7.0092\n",
      "Epoch 157/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2847 - val_loss: 5.7111\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.8885 - val_loss: 4.8930\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 3.9588 - val_loss: 5.6876\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.7581 - val_loss: 5.0189\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6116 - val_loss: 5.0429\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.6273 - val_loss: 5.7638\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.6252 - val_loss: 5.8988\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 2.0494 - val_loss: 5.7377\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3330 - val_loss: 6.2982\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0106 - val_loss: 5.5506\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.6537 - val_loss: 5.9889\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9498 - val_loss: 5.1101\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 1.6888 - val_loss: 5.3120\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.7099 - val_loss: 5.2209\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9786 - val_loss: 5.3104\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5232 - val_loss: 5.2350\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.8394 - val_loss: 6.3906\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5864 - val_loss: 5.1425\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.3645 - val_loss: 5.0967\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 3.1412 - val_loss: 19.5322\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.0994 - val_loss: 5.0964\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2776 - val_loss: 5.0910\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1783 - val_loss: 5.1663\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2259 - val_loss: 5.9107\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.4537 - val_loss: 5.0425\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2687 - val_loss: 5.3966\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6462 - val_loss: 5.5068\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.7097 - val_loss: 6.2828\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.4853 - val_loss: 5.1273\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.6227 - val_loss: 5.8266\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.3441 - val_loss: 5.5992\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2003 - val_loss: 5.1607\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2414 - val_loss: 5.9026\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.8995 - val_loss: 5.4228\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1699 - val_loss: 5.0691\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1993 - val_loss: 5.5378\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5635 - val_loss: 5.9183\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2355 - val_loss: 5.3154\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.3027 - val_loss: 5.2344\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.3656 - val_loss: 5.4951\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2735 - val_loss: 4.9634\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.7647 - val_loss: 8.5143\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6952 - val_loss: 4.9645\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1048 - val_loss: 5.0768\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 5.076816931929069\n",
      "Mean Absolute Error (MAE): 1.5371223182772382\n",
      "Root Mean Squared Error (RMSE): 2.253179294226065\n",
      "Time taken: 1182.866319179535\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 10s 18ms/step - loss: 1394.6967 - val_loss: 1265.4718\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1232.9670 - val_loss: 1169.9790\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1143.6299 - val_loss: 1098.2629\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1076.0316 - val_loss: 1043.5297\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1024.6915 - val_loss: 1004.3517\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 987.0963 - val_loss: 976.3574\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 960.3038 - val_loss: 957.4138\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 942.0119 - val_loss: 945.0700\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 930.2567 - val_loss: 938.0898\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 923.1196 - val_loss: 934.6709\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 919.2514 - val_loss: 933.3909\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 917.3520 - val_loss: 933.0137\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 916.4520 - val_loss: 933.0806\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 916.1937 - val_loss: 933.1764\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 916.0088 - val_loss: 933.5258\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 915.9755 - val_loss: 933.6083\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 916.0038 - val_loss: 933.7571\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 916.0062 - val_loss: 933.6885\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 916.0031 - val_loss: 933.5817\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 916.0083 - val_loss: 933.8198\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 916.0096 - val_loss: 933.7640\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 916.0037 - val_loss: 933.9765\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.9750 - val_loss: 933.8017\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 916.0569 - val_loss: 933.9308\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 907.4651 - val_loss: 894.4730\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 857.4393 - val_loss: 856.8289\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 829.0286 - val_loss: 837.0595\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 801.2492 - val_loss: 810.6154\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 758.0883 - val_loss: 745.1776\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 687.9430 - val_loss: 679.6074\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 634.1272 - val_loss: 621.8723\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 590.7391 - val_loss: 574.2682\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 544.7929 - val_loss: 538.9277\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 499.0457 - val_loss: 479.8291\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 456.6013 - val_loss: 435.6089\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 419.7130 - val_loss: 399.8824\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 380.1726 - val_loss: 363.0466\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 349.1444 - val_loss: 342.2694\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 316.6789 - val_loss: 304.8636\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 286.0775 - val_loss: 275.2871\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 259.3705 - val_loss: 248.1459\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 236.3615 - val_loss: 223.8649\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 211.3313 - val_loss: 196.9245\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 189.6553 - val_loss: 180.8900\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 171.1472 - val_loss: 161.6539\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 152.9588 - val_loss: 139.3528\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 136.5746 - val_loss: 126.2684\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 121.7219 - val_loss: 114.5101\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 107.3453 - val_loss: 101.5865\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 96.2780 - val_loss: 92.0111\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 85.6989 - val_loss: 78.2434\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 75.0881 - val_loss: 67.0809\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 66.7069 - val_loss: 59.7404\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 58.8700 - val_loss: 54.6412\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 52.4976 - val_loss: 47.3805\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 47.0039 - val_loss: 44.5922\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 43.3351 - val_loss: 39.5579\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 39.4737 - val_loss: 33.4191\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 35.1934 - val_loss: 36.9014\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 31.8802 - val_loss: 28.8855\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 28.4354 - val_loss: 26.3154\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 26.0976 - val_loss: 26.7743\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 24.2380 - val_loss: 23.7623\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 22.0306 - val_loss: 19.4389\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 21.0115 - val_loss: 21.0512\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 19.8796 - val_loss: 20.4463\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 17.8720 - val_loss: 25.6362\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 17.4460 - val_loss: 16.5011\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 16.0355 - val_loss: 15.1707\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 15.7348 - val_loss: 15.5947\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 14.5618 - val_loss: 14.8038\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 13.3369 - val_loss: 12.5291\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 13.2045 - val_loss: 13.1523\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 12.2786 - val_loss: 13.2559\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.3962 - val_loss: 11.0879\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 11.1457 - val_loss: 12.9531\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 11.1792 - val_loss: 17.6285\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 14ms/step - loss: 11.2456 - val_loss: 14.2114\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 10.5757 - val_loss: 14.1846\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 9.7188 - val_loss: 12.1694\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 9.9062 - val_loss: 10.8813\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 8.7749 - val_loss: 11.4794\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 10.2429 - val_loss: 10.9922\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 8.9417 - val_loss: 9.7704\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.6242 - val_loss: 9.8411\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.2768 - val_loss: 10.0370\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.1892 - val_loss: 8.7483\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 7.9240 - val_loss: 8.4483\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.7615 - val_loss: 8.6976\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.5576 - val_loss: 14.8251\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.9352 - val_loss: 10.1776\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.5052 - val_loss: 9.1061\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.1942 - val_loss: 11.6429\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.0142 - val_loss: 8.4430\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.1268 - val_loss: 7.8306\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.3065 - val_loss: 9.5817\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 6.0979 - val_loss: 9.9407\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.0576 - val_loss: 7.4576\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 6.1964 - val_loss: 7.2480\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 5.8716 - val_loss: 7.7798\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.6543 - val_loss: 17.1763\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.3678 - val_loss: 9.3721\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.0580 - val_loss: 7.5266\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.0321 - val_loss: 6.3862\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.2844 - val_loss: 7.9420\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.9954 - val_loss: 7.7426\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.3827 - val_loss: 7.4893\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.2534 - val_loss: 8.5931\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.4919 - val_loss: 8.8024\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.2563 - val_loss: 5.9833\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.9191 - val_loss: 9.9639\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.0241 - val_loss: 7.8657\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.5064 - val_loss: 6.6510\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 4.6641 - val_loss: 7.6117\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.1828 - val_loss: 8.5349\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.5882 - val_loss: 5.9626\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.5648 - val_loss: 6.8980\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 4.3178 - val_loss: 6.3839\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.7839 - val_loss: 9.5222\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.2282 - val_loss: 7.2540\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.5926 - val_loss: 5.7340\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.0191 - val_loss: 6.1974\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.9767 - val_loss: 5.9181\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.1258 - val_loss: 7.6668\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.1886 - val_loss: 5.4624\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.1052 - val_loss: 7.0592\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.0447 - val_loss: 5.3905\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.8264 - val_loss: 6.8760\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.8863 - val_loss: 6.3517\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.6531 - val_loss: 5.9321\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 3.5831 - val_loss: 5.3796\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3.6692 - val_loss: 6.0506\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5553 - val_loss: 6.0218\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.1685 - val_loss: 14.2903\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.1354 - val_loss: 5.5916\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3.2301 - val_loss: 6.0122\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.4621 - val_loss: 5.5086\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3.6392 - val_loss: 5.3068\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3.5256 - val_loss: 5.9292\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3.3970 - val_loss: 6.7766\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 3.1161 - val_loss: 6.2831\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.8017 - val_loss: 7.3412\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3.0995 - val_loss: 8.6231\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3.4351 - val_loss: 5.4525\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0219 - val_loss: 6.6158\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.2071 - val_loss: 6.2430\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.2159 - val_loss: 5.5791\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.3090 - val_loss: 6.3521\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.5009 - val_loss: 5.4151\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.6332 - val_loss: 5.5897\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0126 - val_loss: 5.9573\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.1054 - val_loss: 6.1086\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3.1987 - val_loss: 5.4073\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.6724 - val_loss: 5.2983\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.6990 - val_loss: 5.1940\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8392 - val_loss: 5.2527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.7039 - val_loss: 5.7823\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3.1262 - val_loss: 5.5241\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.9036 - val_loss: 5.8302\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.6394 - val_loss: 5.6018\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.8419 - val_loss: 5.9537\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.7421 - val_loss: 5.0942\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8508 - val_loss: 5.4462\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.4458 - val_loss: 6.0323\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3765 - val_loss: 6.5118\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5438 - val_loss: 5.2333\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.7617 - val_loss: 7.4264\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.5893 - val_loss: 5.1253\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1404 - val_loss: 5.4519\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.3151 - val_loss: 5.6652\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.5314 - val_loss: 5.4238\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2576 - val_loss: 5.4575\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2864 - val_loss: 5.2142\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1192 - val_loss: 4.9314\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2320 - val_loss: 5.4279\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.2211 - val_loss: 8.9299\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.5708 - val_loss: 5.1488\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9125 - val_loss: 4.8777\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8915 - val_loss: 4.9477\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9546 - val_loss: 5.5637\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1814 - val_loss: 5.8105\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.6042 - val_loss: 5.7036\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0989 - val_loss: 5.2237\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9707 - val_loss: 4.6853\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9554 - val_loss: 5.3186\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.3561 - val_loss: 5.0185\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1121 - val_loss: 5.0792\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.1466 - val_loss: 5.6971\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.9557 - val_loss: 5.1285\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.8687 - val_loss: 5.7709\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.9123 - val_loss: 5.1166\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.7447 - val_loss: 6.0884\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 1.8947 - val_loss: 5.5896\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.0989 - val_loss: 4.9914\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.9331 - val_loss: 5.5428\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.1258 - val_loss: 5.0374\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 1.7782 - val_loss: 6.6301\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 2.0139 - val_loss: 6.2613\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7933 - val_loss: 5.0669\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7982 - val_loss: 8.5649\n",
      "16/16 [==============================] - 1s 7ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 8.564926733132788\n",
      "Mean Absolute Error (MAE): 1.9244629683650987\n",
      "Root Mean Squared Error (RMSE): 2.926589607911022\n",
      "Time taken: 1185.3394286632538\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 10s 17ms/step - loss: 1369.6901 - val_loss: 1251.8552\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1207.3361 - val_loss: 1148.0406\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1121.9176 - val_loss: 1072.2738\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1058.7178 - val_loss: 1017.5303\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1013.0994 - val_loss: 977.5308\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 980.2725 - val_loss: 949.1196\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 957.6004 - val_loss: 930.0197\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 942.7183 - val_loss: 917.4779\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 933.5112 - val_loss: 910.0172\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 928.2861 - val_loss: 906.0204\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 925.6483 - val_loss: 903.8520\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 924.4321 - val_loss: 902.8503\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 923.8789 - val_loss: 902.4452\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 923.7034 - val_loss: 902.2773\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 923.6730 - val_loss: 902.2400\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 923.6530 - val_loss: 902.1492\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 923.6182 - val_loss: 902.1962\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 923.6439 - val_loss: 902.0925\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 923.6771 - val_loss: 902.1453\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 923.6215 - val_loss: 902.1129\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 923.6196 - val_loss: 902.0356\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 923.6526 - val_loss: 902.2435\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 923.6633 - val_loss: 902.0895\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 922.0968 - val_loss: 886.2125\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 882.7316 - val_loss: 844.9758\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 838.7672 - val_loss: 804.7934\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 813.3325 - val_loss: 782.0900\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 792.3290 - val_loss: 762.8916\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 775.5768 - val_loss: 749.4374\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 759.0478 - val_loss: 743.3680\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 755.2884 - val_loss: 753.4375\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 754.0122 - val_loss: 718.3141\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 710.1344 - val_loss: 676.1696\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 680.2451 - val_loss: 645.9791\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 628.1243 - val_loss: 622.4667\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 567.7861 - val_loss: 522.2784\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 508.8215 - val_loss: 474.5608\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 471.6265 - val_loss: 445.5310\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 420.6339 - val_loss: 386.5547\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 376.2664 - val_loss: 350.3069\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 340.6657 - val_loss: 323.9415\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 318.0113 - val_loss: 280.8954\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 281.2682 - val_loss: 260.3128\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 251.2063 - val_loss: 231.7587\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 230.8031 - val_loss: 206.6656\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 203.3768 - val_loss: 201.7880\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 183.4953 - val_loss: 167.7457\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 180.2167 - val_loss: 161.1249\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 148.5018 - val_loss: 135.1835\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 130.9082 - val_loss: 113.4318\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 113.9016 - val_loss: 101.8216\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 104.0306 - val_loss: 119.5960\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 94.9095 - val_loss: 83.0683\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 80.9031 - val_loss: 74.2734\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 69.7768 - val_loss: 66.9450\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 63.7115 - val_loss: 60.1483\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 55.7505 - val_loss: 48.7285\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 52.3699 - val_loss: 54.1184\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 44.7978 - val_loss: 37.6100\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 39.0842 - val_loss: 34.2713\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 34.6877 - val_loss: 35.4436\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 31.1830 - val_loss: 29.5315\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 28.6176 - val_loss: 25.2689\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 25.8766 - val_loss: 27.1320\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 24.4238 - val_loss: 21.2903\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 22.8724 - val_loss: 19.3421\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 21.5295 - val_loss: 22.3313\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 21.5539 - val_loss: 19.7856\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 18.7722 - val_loss: 19.1099\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 18.0188 - val_loss: 21.2479\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 18.0871 - val_loss: 17.9386\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 16.6682 - val_loss: 18.1444\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 15.2328 - val_loss: 25.8098\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 15.6513 - val_loss: 13.5981\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 15.2227 - val_loss: 12.8787\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 14.8307 - val_loss: 14.0340\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.4617 - val_loss: 13.2565\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 12.8744 - val_loss: 16.5953\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.1211 - val_loss: 14.4832\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.2207 - val_loss: 13.6218\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.9307 - val_loss: 10.9755\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 11.4801 - val_loss: 16.8336\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 13.3427 - val_loss: 12.0604\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.3208 - val_loss: 13.0595\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.0145 - val_loss: 10.2322\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.3270 - val_loss: 14.2195\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.2861 - val_loss: 10.1788\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.9486 - val_loss: 10.9834\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.4623 - val_loss: 14.7512\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.7057 - val_loss: 12.0069\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.7797 - val_loss: 11.9360\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.7879 - val_loss: 16.6903\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.5300 - val_loss: 9.9362\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.8748 - val_loss: 11.5448\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.6630 - val_loss: 10.1040\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.6613 - val_loss: 10.3869\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.3186 - val_loss: 9.2858\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.6969 - val_loss: 9.8404\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.1932 - val_loss: 10.7829\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.7934 - val_loss: 9.4247\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.9962 - val_loss: 9.2751\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 12.2467 - val_loss: 236.0356\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 15.8448 - val_loss: 10.0369\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 7.2493 - val_loss: 9.4273\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.9730 - val_loss: 8.2638\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.5740 - val_loss: 8.6946\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 6.2457 - val_loss: 9.0156\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 7.4047 - val_loss: 8.2096\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 6.1504 - val_loss: 10.4467\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.1684 - val_loss: 9.0983\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.1915 - val_loss: 8.3342\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.9585 - val_loss: 7.8284\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.1190 - val_loss: 10.8931\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.8718 - val_loss: 10.3299\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.7249 - val_loss: 11.3680\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.7897 - val_loss: 8.2282\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.0730 - val_loss: 7.1249\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.9167 - val_loss: 10.7719\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.1761 - val_loss: 7.7689\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 6.8954 - val_loss: 8.0416\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 6.3499 - val_loss: 13.8821\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 6.1026 - val_loss: 7.1270\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.5766 - val_loss: 9.2834\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 5.7171 - val_loss: 7.1772\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 7.0290 - val_loss: 8.1555\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.2880 - val_loss: 7.0742\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.6737 - val_loss: 6.8920\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.0613 - val_loss: 7.7192\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.0365 - val_loss: 7.3247\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 5.4794 - val_loss: 10.4241\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.9638 - val_loss: 10.0933\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.6870 - val_loss: 7.6543\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.6450 - val_loss: 8.1091\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.7992 - val_loss: 12.6507\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.4192 - val_loss: 7.4222\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 4.6935 - val_loss: 6.3410\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.0937 - val_loss: 8.8550\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.1176 - val_loss: 7.7027\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.9884 - val_loss: 8.7477\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.3034 - val_loss: 6.5574\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.0019 - val_loss: 9.5907\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.5364 - val_loss: 6.2151\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.6383 - val_loss: 6.5339\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 5.3483 - val_loss: 6.8326\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.9689 - val_loss: 6.7533\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 6.1725 - val_loss: 5.8189\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.1908 - val_loss: 10.6926\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.0877 - val_loss: 5.9581\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.1795 - val_loss: 6.0033\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.8170 - val_loss: 5.9584\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.7151 - val_loss: 6.3089\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.8407 - val_loss: 7.3101\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.6717 - val_loss: 5.9211\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.6457 - val_loss: 5.5008\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5573 - val_loss: 5.6444\n",
      "Epoch 156/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 3.6697 - val_loss: 6.5035\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.5419 - val_loss: 6.4775\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.7143 - val_loss: 6.3874\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.2237 - val_loss: 5.8046\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5730 - val_loss: 5.6894\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3462 - val_loss: 6.7943\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.4958 - val_loss: 6.3866\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3.4157 - val_loss: 6.2086\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.9750 - val_loss: 6.6821\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3.5351 - val_loss: 7.0817\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3777 - val_loss: 6.2372\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2112 - val_loss: 6.0564\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.4123 - val_loss: 6.8655\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3595 - val_loss: 6.3630\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.0970 - val_loss: 6.0564\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.1606 - val_loss: 5.8303\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0741 - val_loss: 5.9313\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0762 - val_loss: 6.3147\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0762 - val_loss: 6.4813\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2616 - val_loss: 7.0822\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.1441 - val_loss: 6.3115\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.9881 - val_loss: 6.7837\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3.1532 - val_loss: 6.0352\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3057 - val_loss: 7.2688\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.9883 - val_loss: 6.5055\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.6014 - val_loss: 5.6539\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7364 - val_loss: 6.0038\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.7920 - val_loss: 5.2791\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8975 - val_loss: 6.4868\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 4.1364 - val_loss: 6.4140\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8429 - val_loss: 7.7177\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.6440 - val_loss: 5.8549\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.8624 - val_loss: 5.4422\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2656 - val_loss: 8.2094\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 2.9385 - val_loss: 5.6167\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4637 - val_loss: 9.5582\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 5.4961 - val_loss: 6.3216\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5728 - val_loss: 5.8949\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5161 - val_loss: 5.5924\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2701 - val_loss: 6.6796\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4727 - val_loss: 5.4372\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3732 - val_loss: 6.1389\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4630 - val_loss: 6.3170\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.1415 - val_loss: 12.8920\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3.9039 - val_loss: 5.7259\n",
      "16/16 [==============================] - 1s 6ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 5.725791314030414\n",
      "Mean Absolute Error (MAE): 1.6174864595115146\n",
      "Root Mean Squared Error (RMSE): 2.392862577339203\n",
      "Time taken: 1166.1967957019806\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 10s 17ms/step - loss: 1345.6915 - val_loss: 1268.9114\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1194.4360 - val_loss: 1172.7117\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1113.9901 - val_loss: 1100.5015\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1053.8647 - val_loss: 1044.8770\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1008.8580 - val_loss: 1003.6869\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 976.5400 - val_loss: 973.5679\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 954.0627 - val_loss: 952.5437\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 939.2695 - val_loss: 938.2985\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 930.0021 - val_loss: 929.1339\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 924.7785 - val_loss: 923.6951\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 922.0280 - val_loss: 920.6276\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.7992 - val_loss: 918.8476\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 920.2465 - val_loss: 918.1210\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 920.0201 - val_loss: 917.5317\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 919.9991 - val_loss: 917.4017\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 919.9700 - val_loss: 917.1920\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 919.9779 - val_loss: 917.0452\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 919.9314 - val_loss: 917.0363\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 919.9927 - val_loss: 917.2021\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 919.9790 - val_loss: 917.0900\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 912.4026 - val_loss: 887.8875\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 875.1365 - val_loss: 855.4520\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 848.2067 - val_loss: 834.7896\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 823.5919 - val_loss: 803.6595\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 789.3151 - val_loss: 766.6343\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 750.6154 - val_loss: 725.1189\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 707.2336 - val_loss: 681.1566\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 665.3345 - val_loss: 655.5104\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 623.9213 - val_loss: 599.5320\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 578.9030 - val_loss: 562.0706\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 533.7463 - val_loss: 515.2691\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 490.9327 - val_loss: 472.2177\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 450.9826 - val_loss: 435.4067\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 415.2150 - val_loss: 399.1693\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 378.6874 - val_loss: 362.5431\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 350.2761 - val_loss: 331.8307\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 315.8394 - val_loss: 301.8145\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 288.1700 - val_loss: 275.3622\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 263.1043 - val_loss: 255.1644\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 237.9156 - val_loss: 233.8828\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 218.0835 - val_loss: 214.1170\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 196.7353 - val_loss: 198.9066\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 175.8220 - val_loss: 169.8140\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 156.6800 - val_loss: 152.4643\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 140.8069 - val_loss: 139.1637\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 125.1875 - val_loss: 121.2497\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 110.1744 - val_loss: 105.8487\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 99.5529 - val_loss: 110.4339\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 86.7157 - val_loss: 86.0832\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 78.1645 - val_loss: 75.8515\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 69.0094 - val_loss: 71.3027\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 61.7828 - val_loss: 60.4886\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 55.5850 - val_loss: 54.7549\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 49.7152 - val_loss: 47.6676\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 43.1567 - val_loss: 44.7572\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 38.7090 - val_loss: 38.7009\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 35.1077 - val_loss: 34.3212\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 31.6315 - val_loss: 29.9069\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 29.1304 - val_loss: 30.8536\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 25.7298 - val_loss: 25.5751\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 23.2361 - val_loss: 28.3199\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 21.4982 - val_loss: 24.5897\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 21.3285 - val_loss: 20.9134\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 18.1240 - val_loss: 19.6527\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 17.3661 - val_loss: 20.1390\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 16.3610 - val_loss: 19.1760\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 15.2524 - val_loss: 17.6715\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 14.6180 - val_loss: 17.1879\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 14.8171 - val_loss: 14.1095\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 13.2534 - val_loss: 17.7502\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 12.4857 - val_loss: 12.7038\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.1566 - val_loss: 12.3657\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.3972 - val_loss: 13.5205\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.5429 - val_loss: 11.2809\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.1309 - val_loss: 13.1177\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.1708 - val_loss: 10.0643\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 11.8490 - val_loss: 12.6056\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 9.3793 - val_loss: 11.0978\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.5730 - val_loss: 10.2164\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.6143 - val_loss: 10.4237\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.0146 - val_loss: 13.3366\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.3598 - val_loss: 12.6561\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.5500 - val_loss: 9.7385\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.4646 - val_loss: 10.0973\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.6960 - val_loss: 10.6129\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 7.1281 - val_loss: 11.2695\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.6914 - val_loss: 9.3891\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.1655 - val_loss: 10.8049\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.3414 - val_loss: 9.3634\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 6.9896 - val_loss: 8.5531\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.5135 - val_loss: 8.8759\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.4347 - val_loss: 9.1518\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.5693 - val_loss: 8.9774\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.6881 - val_loss: 9.0690\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.9955 - val_loss: 10.0361\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.8506 - val_loss: 7.6605\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.7834 - val_loss: 8.6416\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.5699 - val_loss: 7.7628\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.2409 - val_loss: 7.8412\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 5.4424 - val_loss: 8.7417\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.1433 - val_loss: 10.1560\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.0261 - val_loss: 7.5521\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.5670 - val_loss: 8.7459\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.8990 - val_loss: 7.2583\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.5993 - val_loss: 6.7927\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 4.9384 - val_loss: 6.4273\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.8978 - val_loss: 7.2978\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 4.8928 - val_loss: 9.5377\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.5115 - val_loss: 8.9005\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.8143 - val_loss: 8.0897\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.5866 - val_loss: 6.6018\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.2725 - val_loss: 7.6157\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.4622 - val_loss: 6.7653\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.2438 - val_loss: 8.2565\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.3980 - val_loss: 7.6414\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.8823 - val_loss: 8.0343\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.1496 - val_loss: 7.0029\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.9662 - val_loss: 8.0288\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.0140 - val_loss: 10.3935\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.0450 - val_loss: 6.8770\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.9785 - val_loss: 6.6725\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.1639 - val_loss: 8.0005\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.9659 - val_loss: 7.5799\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.8627 - val_loss: 6.6203\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.9331 - val_loss: 8.9352\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.3197 - val_loss: 6.9063\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.3819 - val_loss: 7.0161\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.6053 - val_loss: 11.3854\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3643 - val_loss: 7.8982\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 3.5034 - val_loss: 6.8083\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.4449 - val_loss: 6.6883\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.7857 - val_loss: 7.2599\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2406 - val_loss: 6.3558\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.4704 - val_loss: 6.9964\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.4998 - val_loss: 6.8310\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5607 - val_loss: 8.3110\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.1836 - val_loss: 7.0485\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.7995 - val_loss: 6.3447\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3306 - val_loss: 6.9726\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.1118 - val_loss: 7.5441\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.1996 - val_loss: 5.8363\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.1128 - val_loss: 6.7628\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.1988 - val_loss: 6.0505\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.1729 - val_loss: 6.0536\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3651 - val_loss: 6.1082\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8497 - val_loss: 7.2729\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.4999 - val_loss: 7.0032\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.1278 - val_loss: 5.8864\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.7173 - val_loss: 6.5042\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.8097 - val_loss: 6.1446\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7380 - val_loss: 7.0038\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.8059 - val_loss: 5.6371\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.6534 - val_loss: 5.9589\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4493 - val_loss: 7.6164\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5390 - val_loss: 5.8594\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5118 - val_loss: 6.6128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5867 - val_loss: 6.2380\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5553 - val_loss: 7.1507\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8355 - val_loss: 6.0058\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.1926 - val_loss: 7.7580\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4874 - val_loss: 5.9422\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.6054 - val_loss: 9.4377\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.4780 - val_loss: 6.1354\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2949 - val_loss: 7.6304\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2679 - val_loss: 6.1926\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2305 - val_loss: 7.3977\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 3.4983 - val_loss: 8.1328\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0019 - val_loss: 6.0476\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0368 - val_loss: 6.1179\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.1236 - val_loss: 6.3369\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.1389 - val_loss: 6.4769\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4395 - val_loss: 7.6051\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3890 - val_loss: 7.5314\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.3623 - val_loss: 6.4965\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2219 - val_loss: 7.1330\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3014 - val_loss: 9.5186\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.6164 - val_loss: 6.9996\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.3864 - val_loss: 6.0121\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.0814 - val_loss: 6.5013\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.3393 - val_loss: 7.1043\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5706 - val_loss: 7.1866\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2568 - val_loss: 6.1353\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7290 - val_loss: 5.8451\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0132 - val_loss: 5.9830\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9512 - val_loss: 6.8395\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0487 - val_loss: 6.6404\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1564 - val_loss: 6.7711\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9347 - val_loss: 6.2687\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9767 - val_loss: 5.9311\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.8135 - val_loss: 5.9053\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.2184 - val_loss: 5.9139\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6505 - val_loss: 6.1755\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5070 - val_loss: 6.0250\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.4775 - val_loss: 5.9625\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.4360 - val_loss: 5.9251\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.4833 - val_loss: 5.9699\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7168 - val_loss: 7.2083\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8471 - val_loss: 6.9578\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.5661 - val_loss: 6.3095\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7544 - val_loss: 6.1336\n",
      "16/16 [==============================] - 1s 8ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 6.133661202393345\n",
      "Mean Absolute Error (MAE): 1.6603169088035397\n",
      "Root Mean Squared Error (RMSE): 2.4766229431210043\n",
      "Time taken: 1172.641904592514\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(3))\n",
    "\n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_train, y_train, epochs=200, batch_size=5, validation_data=(X_test, y_test))\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 48, 512)           1052672   \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 48, 256)           787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,037,635\n",
      "Trainable params: 2,037,635\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_9992\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE   Time taken\n",
      "0        1  6.642649  1.765881  2.577334  1189.995856\n",
      "1        2  5.076817  1.537122  2.253179  1182.866319\n",
      "2        3  8.564927  1.924463  2.926590  1185.339429\n",
      "3        4  5.725791  1.617486  2.392863  1166.196796\n",
      "4        5  6.133661  1.660317  2.476623  1172.641905\n",
      "5  Average  6.428769  1.701054  2.525318  1179.408061\n",
      "Results saved to 'DL_Result_PL_model_1_Scattered_iReg_f.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('DL_Result_PL_model_1_Scattered_iReg_f.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'DL_Result_PL_model_1_Scattered_iReg_f.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
